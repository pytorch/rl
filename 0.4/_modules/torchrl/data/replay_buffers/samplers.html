


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchrl.data.replay_buffers.samplers &mdash; torchrl 0.4 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  <!-- Google Analytics -->
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117752657-2');
    </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  0.4
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
      <li>torchrl.data.replay_buffers.samplers</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">_modules/torchrl/data/replay_buffers/samplers</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../../../../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../../../../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../../../../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchrl.data.replay_buffers.samplers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the MIT license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">textwrap</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">copy</span><span class="p">,</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">multiprocessing.context</span> <span class="kn">import</span> <span class="n">get_spawning_popen</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">tensordict</span> <span class="kn">import</span> <span class="n">MemoryMappedTensor</span><span class="p">,</span> <span class="n">TensorDict</span>
<span class="kn">from</span> <span class="nn">tensordict.utils</span> <span class="kn">import</span> <span class="n">NestedKey</span>

<span class="kn">from</span> <span class="nn">torchrl._extension</span> <span class="kn">import</span> <span class="n">EXTENSION_WARNING</span>

<span class="kn">from</span> <span class="nn">torchrl._utils</span> <span class="kn">import</span> <span class="n">_replace_last</span><span class="p">,</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">torchrl.data.replay_buffers.storages</span> <span class="kn">import</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">StorageEnsemble</span><span class="p">,</span> <span class="n">TensorStorage</span>
<span class="kn">from</span> <span class="nn">torchrl.data.replay_buffers.utils</span> <span class="kn">import</span> <span class="n">_is_int</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">torchrl._torchrl</span> <span class="kn">import</span> <span class="p">(</span>
        <span class="n">MinSegmentTreeFp32</span><span class="p">,</span>
        <span class="n">MinSegmentTreeFp64</span><span class="p">,</span>
        <span class="n">SumSegmentTreeFp32</span><span class="p">,</span>
        <span class="n">SumSegmentTreeFp64</span><span class="p">,</span>
    <span class="p">)</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">EXTENSION_WARNING</span><span class="p">)</span>

<span class="n">_EMPTY_STORAGE_ERROR</span> <span class="o">=</span> <span class="s2">&quot;Cannot sample from an empty storage.&quot;</span>


<div class="viewcode-block" id="Sampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.Sampler.html#torchrl.data.replay_buffers.Sampler">[docs]</a><span class="k">class</span> <span class="nc">Sampler</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A generic sampler base class for composable Replay Buffers.&quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">update_priority</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">priority</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">mark_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">default_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">1.0</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="o">...</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ran_out</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># by default, samplers never run out</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">()&quot;</span></div>


<div class="viewcode-block" id="RandomSampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.RandomSampler.html#torchrl.data.replay_buffers.RandomSampler">[docs]</a><span class="k">class</span> <span class="nc">RandomSampler</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A uniformly random sampler for composable replay buffers.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size (int, optional): if provided, the batch size to be used by</span>
<span class="sd">            the replay buffer when calling :meth:`~.ReplayBuffer.sample`.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">_EMPTY_STORAGE_ERROR</span><span class="p">)</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">_rand_given_ndim</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span></div>


<div class="viewcode-block" id="SamplerWithoutReplacement"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.SamplerWithoutReplacement.html#torchrl.data.replay_buffers.SamplerWithoutReplacement">[docs]</a><span class="k">class</span> <span class="nc">SamplerWithoutReplacement</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A data-consuming sampler that ensures that the same sample is not present in consecutive batches.</span>

<span class="sd">    Args:</span>
<span class="sd">        drop_last (bool, optional): if ``True``, the last incomplete sample (if any) will be dropped.</span>
<span class="sd">            If ``False``, this last sample will be kept and (unlike with torch dataloaders)</span>
<span class="sd">            completed with other samples from a fresh indices permutation.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        shuffle (bool, optional): if ``False``, the items are not randomly</span>
<span class="sd">            permuted. This enables to iterate over the replay buffer in the</span>
<span class="sd">            order the data was collected. Defaults to ``True``.</span>

<span class="sd">    *Caution*: If the size of the storage changes in between two calls, the samples will be re-shuffled</span>
<span class="sd">    (as we can&#39;t generally keep track of which samples have been sampled before and which haven&#39;t).</span>

<span class="sd">    Similarly, it is expected that the storage content remains the same in between two calls,</span>
<span class="sd">    but this is not enforced.</span>

<span class="sd">    When the sampler reaches the end of the list of available indices, a new sample order</span>
<span class="sd">    will be generated and the resulting indices will be completed with this new draw, which</span>
<span class="sd">    can lead to duplicated indices, unless the :obj:`drop_last` argument is set to ``True``.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="n">drop_last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="n">shuffle</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="n">path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sampler_metadata.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="n">file</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sampler_metadata.json&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">metadata</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_sample_list</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">len_storage</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">storage</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="o">.</span><span class="n">device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">device</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
            <span class="n">_sample_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">len_storage</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_sample_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">len_storage</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="o">=</span> <span class="n">_sample_list</span>

    <span class="k">def</span> <span class="nf">_single_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">len_storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">[:</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">[</span><span class="n">batch_size</span><span class="p">:]</span>

        <span class="c1"># check if we have enough elements for one more batch, assuming same batch size</span>
        <span class="c1"># will be used each time sample is called</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batch_size</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ran_out</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_sample_list</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">len_storage</span><span class="o">=</span><span class="n">len_storage</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ran_out</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="n">index</span>

    <span class="k">def</span> <span class="nf">_storage_len</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="n">len_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage_len</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">len_storage</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">_EMPTY_STORAGE_ERROR</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">len_storage</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;An empty storage was passed&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">!=</span> <span class="n">len_storage</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_sample_list</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">len_storage</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">len_storage</span> <span class="o">&lt;</span> <span class="n">batch_size</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The batch size (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">) is greater than the storage capacity (</span><span class="si">{</span><span class="n">len_storage</span><span class="si">}</span><span class="s2">). &quot;</span>
                <span class="s2">&quot;This makes it impossible to return a sample without repeating indices. &quot;</span>
                <span class="s2">&quot;Consider changing the sampler class or turn the &#39;drop_last&#39; argument to False.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">=</span> <span class="n">len_storage</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_single_sample</span><span class="p">(</span><span class="n">len_storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># we &#39;always&#39; return the indices. The &#39;drop_last&#39; just instructs the</span>
        <span class="c1"># sampler to turn to `ran_out = True` whenever the next sample</span>
        <span class="c1"># will be too short. This will be read by the replay buffer</span>
        <span class="c1"># as a signal for an early break of the __iter__().</span>
        <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="p">{}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ran_out</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span>

    <span class="nd">@ran_out</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">ran_out</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">OrderedDict</span><span class="p">(</span>
            <span class="n">len_storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span><span class="p">,</span>
            <span class="n">_sample_list</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">,</span>
            <span class="n">drop_last</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span><span class="p">,</span>
            <span class="n">_ran_out</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;len_storage&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_sample_list&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;drop_last&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ran_out</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_ran_out&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">perc</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">perc</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="n">perc</span><span class="si">:</span><span class="s2"> 4.4f</span><span class="si">}</span><span class="s2">% sampled)&quot;</span></div>


<div class="viewcode-block" id="PrioritizedSampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.PrioritizedSampler.html#torchrl.data.replay_buffers.PrioritizedSampler">[docs]</a><span class="k">class</span> <span class="nc">PrioritizedSampler</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Prioritized sampler for replay buffer.</span>

<span class="sd">    Presented in &quot;Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2015. Prioritized experience replay.&quot; (https://arxiv.org/abs/1511.05952)</span>

<span class="sd">    Args:</span>
<span class="sd">        max_capacity (int): maximum capacity of the buffer.</span>
<span class="sd">        alpha (float): exponent α determines how much prioritization is used,</span>
<span class="sd">            with α = 0 corresponding to the uniform case.</span>
<span class="sd">        beta (float): importance sampling negative exponent.</span>
<span class="sd">        eps (float, optional): delta added to the priorities to ensure that the buffer</span>
<span class="sd">            does not contain null priorities. Defaults to 1e-8.</span>
<span class="sd">        reduction (str, optional): the reduction method for multidimensional</span>
<span class="sd">            tensordicts (ie stored trajectory). Can be one of &quot;max&quot;, &quot;min&quot;,</span>
<span class="sd">            &quot;median&quot; or &quot;mean&quot;.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers import ReplayBuffer, LazyTensorStorage, PrioritizedSampler</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; rb = ReplayBuffer(storage=LazyTensorStorage(10), sampler=PrioritizedSampler(max_capacity=10, alpha=1.0, beta=1.0))</span>
<span class="sd">        &gt;&gt;&gt; priority = torch.tensor([0, 1000])</span>
<span class="sd">        &gt;&gt;&gt; data_0 = TensorDict({&quot;reward&quot;: 0, &quot;obs&quot;: [0], &quot;action&quot;: [0], &quot;priority&quot;: priority[0]}, [])</span>
<span class="sd">        &gt;&gt;&gt; data_1 = TensorDict({&quot;reward&quot;: 1, &quot;obs&quot;: [1], &quot;action&quot;: [2], &quot;priority&quot;: priority[1]}, [])</span>
<span class="sd">        &gt;&gt;&gt; rb.add(data_0)</span>
<span class="sd">        &gt;&gt;&gt; rb.add(data_1)</span>
<span class="sd">        &gt;&gt;&gt; rb.update_priority(torch.tensor([0, 1]), priority=priority)</span>
<span class="sd">        &gt;&gt;&gt; sample, info = rb.sample(10, return_info=True)</span>
<span class="sd">        &gt;&gt;&gt; print(sample)</span>
<span class="sd">        TensorDict(</span>
<span class="sd">                fields={</span>
<span class="sd">                    action: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),</span>
<span class="sd">                    obs: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.int64, is_shared=False),</span>
<span class="sd">                    priority: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False),</span>
<span class="sd">                    reward: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.int64, is_shared=False)},</span>
<span class="sd">                batch_size=torch.Size([10]),</span>
<span class="sd">                device=cpu,</span>
<span class="sd">                is_shared=False)</span>
<span class="sd">        &gt;&gt;&gt; print(info)</span>
<span class="sd">        {&#39;_weight&#39;: array([1.e-11, 1.e-11, 1.e-11, 1.e-11, 1.e-11, 1.e-11, 1.e-11, 1.e-11,</span>
<span class="sd">               1.e-11, 1.e-11], dtype=float32), &#39;index&#39;: array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}</span>

<span class="sd">    .. note:: Using a :class:`~torchrl.data.replay_buffers.TensorDictReplayBuffer` can smoothen the</span>
<span class="sd">        process of updating the priorities:</span>

<span class="sd">            &gt;&gt;&gt; from torchrl.data.replay_buffers import TensorDictReplayBuffer as TDRB, LazyTensorStorage, PrioritizedSampler</span>
<span class="sd">            &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">            &gt;&gt;&gt; rb = TDRB(</span>
<span class="sd">            ...     storage=LazyTensorStorage(10),</span>
<span class="sd">            ...     sampler=PrioritizedSampler(max_capacity=10, alpha=1.0, beta=1.0),</span>
<span class="sd">            ...     priority_key=&quot;priority&quot;,  # This kwarg isn&#39;t present in regular RBs</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; priority = torch.tensor([0, 1000])</span>
<span class="sd">            &gt;&gt;&gt; data_0 = TensorDict({&quot;reward&quot;: 0, &quot;obs&quot;: [0], &quot;action&quot;: [0], &quot;priority&quot;: priority[0]}, [])</span>
<span class="sd">            &gt;&gt;&gt; data_1 = TensorDict({&quot;reward&quot;: 1, &quot;obs&quot;: [1], &quot;action&quot;: [2], &quot;priority&quot;: priority[1]}, [])</span>
<span class="sd">            &gt;&gt;&gt; data = torch.stack([data_0, data_1])</span>
<span class="sd">            &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">            &gt;&gt;&gt; rb.update_priority(data)  # Reads the &quot;priority&quot; key as indicated in the constructor</span>
<span class="sd">            &gt;&gt;&gt; sample, info = rb.sample(10, return_info=True)</span>
<span class="sd">            &gt;&gt;&gt; print(sample[&#39;index&#39;])  # The index is packed with the tensordict</span>
<span class="sd">            tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">max_capacity</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;alpha must be strictly greater than 0, got alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">beta</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;beta must be greater or equal to 0, got beta=</span><span class="si">{</span><span class="n">beta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span> <span class="o">=</span> <span class="n">max_capacity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(alpha=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="si">}</span><span class="s2">, beta=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="si">}</span><span class="s2">, eps=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="si">}</span><span class="s2">, reduction=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">max_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">get_spawning_popen</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Samplers of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> cannot be shared between processes.&quot;</span>
            <span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatType</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span> <span class="o">=</span> <span class="n">SumSegmentTreeFp32</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span> <span class="o">=</span> <span class="n">MinSegmentTreeFp32</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">DoubleTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span> <span class="o">=</span> <span class="n">SumSegmentTreeFp64</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span> <span class="o">=</span> <span class="n">MinSegmentTreeFp64</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;dtype </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> not supported by PrioritizedSampler&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">()</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">default_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">_EMPTY_STORAGE_ERROR</span><span class="p">)</span>
        <span class="n">p_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">))</span>
        <span class="n">p_min</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">p_sum</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;negative p_sum&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p_min</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;negative p_min&quot;</span><span class="p">)</span>
        <span class="c1"># For some undefined reason, only np.random works here.</span>
        <span class="c1"># All PT attempts fail, even when subsequently transformed into numpy</span>
        <span class="n">mass</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">p_sum</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="c1"># mass = torch.zeros(batch_size, dtype=torch.double).uniform_(0.0, p_sum)</span>
        <span class="c1"># mass = torch.rand(batch_size).mul_(p_sum)</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="o">.</span><span class="n">scan_lower_bound</span><span class="p">(</span><span class="n">mass</span><span class="p">)</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">index</span><span class="o">.</span><span class="n">clamp_max_</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>

        <span class="c1"># Importance sampling weight formula:</span>
        <span class="c1">#   w_i = (p_i / sum(p) * N) ^ (-beta)</span>
        <span class="c1">#   weight_i = w_i / max(w)</span>
        <span class="c1">#   weight_i = (p_i / sum(p) * N) ^ (-beta) /</span>
        <span class="c1">#       ((min(p) / sum(p) * N) ^ (-beta))</span>
        <span class="c1">#   weight_i = ((p_i / sum(p) * N) / (min(p) / sum(p) * N)) ^ (-beta)</span>
        <span class="c1">#   weight_i = (p_i / min(p)) ^ (-beta)</span>
        <span class="c1"># weight = np.power(weight / (p_min + self._eps), -self._beta)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">weight</span> <span class="o">/</span> <span class="n">p_min</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">//</span> <span class="n">shape</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="o">*</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;_weight&quot;</span><span class="p">:</span> <span class="n">weight</span><span class="p">}</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">_add_or_extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_priority</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span>
            <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">priority</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">priority</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;priority should be a scalar or an iterable of the same &quot;</span>
                <span class="s2">&quot;length as index&quot;</span>
            <span class="p">)</span>
        <span class="c1"># make sure everything is cast to cpu</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
        <span class="c1"># MaxValueWriter will set -1 for items in the data that we don&#39;t want</span>
        <span class="c1"># to update. We therefore have to keep only the non-negative indices.</span>
        <span class="n">valid_index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">valid_index</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">valid_index</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="n">valid_index</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">priority</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="p">[</span><span class="n">valid_index</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">priority</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">priority</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># some writers don&#39;t systematically write data and can return None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_add_or_extend</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># some writers don&#39;t systematically write data and can return None</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_add_or_extend</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>

<div class="viewcode-block" id="PrioritizedSampler.update_priority"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.PrioritizedSampler.html#torchrl.data.replay_buffers.PrioritizedSampler.update_priority">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">update_priority</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">priority</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Updates the priority of the data pointed by the index.</span>

<span class="sd">        Args:</span>
<span class="sd">            index (int or torch.Tensor): indexes of the priorities to be</span>
<span class="sd">                updated.</span>
<span class="sd">            priority (Number or torch.Tensor): new priorities of the</span>
<span class="sd">                indexed elements.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
        <span class="c1"># we need to reshape priority if it has more than one element or if it has</span>
        <span class="c1"># a different shape than index</span>
        <span class="k">if</span> <span class="n">priority</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">priority</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;priority should be a number or an iterable of the same &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;length as index. Got priority of shape </span><span class="si">{</span><span class="n">priority</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> and index &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span> <span class="kn">from</span> <span class="nn">err</span>
        <span class="k">elif</span> <span class="n">priority</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># MaxValueWriter will set -1 for items in the data that we don&#39;t want</span>
        <span class="c1"># to update. We therefore have to keep only the non-negative indices.</span>
        <span class="k">if</span> <span class="n">_is_int</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported index shape: </span><span class="si">{</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">valid_index</span> <span class="o">=</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">valid_index</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="k">return</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">valid_index</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="n">valid_index</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">priority</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
                    <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="p">[</span><span class="n">valid_index</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">priority</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">priority</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">priority</span></div>

    <span class="k">def</span> <span class="nf">mark_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_priority</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_priority</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;_alpha&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">,</span>
            <span class="s2">&quot;_beta&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="p">,</span>
            <span class="s2">&quot;_eps&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="p">,</span>
            <span class="s2">&quot;_max_priority&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span><span class="p">,</span>
            <span class="s2">&quot;_sum_tree&quot;</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">),</span>
            <span class="s2">&quot;_min_tree&quot;</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="p">),</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_alpha&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_beta&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_eps&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_max_priority&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_sum_tree&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_min_tree&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="n">path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">mm_st</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">from_filename</span><span class="p">(</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sumtree.memmap&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">mm_mt</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">from_filename</span><span class="p">(</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;mintree.memmap&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
            <span class="n">mm_st</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sumtree.memmap&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">mm_mt</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;mintree.memmap&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">mm_st</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)])</span>
        <span class="p">)</span>
        <span class="n">mm_mt</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">)])</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sampler_metadata.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;_alpha&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="p">,</span>
                    <span class="s2">&quot;_beta&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="p">,</span>
                    <span class="s2">&quot;_eps&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="p">,</span>
                    <span class="s2">&quot;_max_priority&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span><span class="p">,</span>
                    <span class="s2">&quot;_max_capacity&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="n">file</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sampler_metadata.json&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;_alpha&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_beta</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;_beta&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_eps</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;_eps&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_priority</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;_max_priority&quot;</span><span class="p">]</span>
        <span class="n">_max_capacity</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;_max_capacity&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">_max_capacity</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;max capacity of loaded metadata (</span><span class="si">{</span><span class="n">_max_capacity</span><span class="si">}</span><span class="s2">) differs from self._max_capacity (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>
        <span class="n">mm_st</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">from_filename</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
            <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sumtree.memmap&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">mm_mt</span> <span class="o">=</span> <span class="n">MemoryMappedTensor</span><span class="o">.</span><span class="n">from_filename</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_max_capacity</span><span class="p">,),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
            <span class="n">filename</span><span class="o">=</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;mintree.memmap&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">elt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mm_st</span><span class="o">.</span><span class="n">tolist</span><span class="p">()):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">elt</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">elt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mm_mt</span><span class="o">.</span><span class="n">tolist</span><span class="p">()):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_tree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">elt</span></div>


<div class="viewcode-block" id="SliceSampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.SliceSampler.html#torchrl.data.replay_buffers.SliceSampler">[docs]</a><span class="k">class</span> <span class="nc">SliceSampler</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Samples slices of data along the first dimension, given start and stop signals.</span>

<span class="sd">    This class samples sub-trajectories with replacement. For a version without</span>
<span class="sd">    replacement, see :class:`~torchrl.data.replay_buffers.samplers.SliceSamplerWithoutReplacement`.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        num_slices (int): the number of slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``num_slices`` argument. Exclusive</span>
<span class="sd">            with ``slice_len``.</span>
<span class="sd">        slice_len (int): the length of the slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``slice_len`` argument and divisible</span>
<span class="sd">            by it. Exclusive with ``num_slices``.</span>
<span class="sd">        end_key (NestedKey, optional): the key indicating the end of a</span>
<span class="sd">            trajectory (or episode). Defaults to ``(&quot;next&quot;, &quot;done&quot;)``.</span>
<span class="sd">        traj_key (NestedKey, optional): the key indicating the trajectories.</span>
<span class="sd">            Defaults to ``&quot;episode&quot;`` (commonly used across datasets in TorchRL).</span>
<span class="sd">        ends (torch.Tensor, optional): a 1d boolean tensor containing the end of run signals.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">            If provided, it is assumed that the storage is at capacity and that</span>
<span class="sd">            if the last element of the ``ends`` tensor is ``False``,</span>
<span class="sd">            the same trajectory spans across end and beginning.</span>
<span class="sd">        trajectories (torch.Tensor, optional): a 1d integer tensor containing the run ids.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">            If provided, it is assumed that the storage is at capacity and that</span>
<span class="sd">            if the last element of the trajectory tensor is identical to the first,</span>
<span class="sd">            the same trajectory spans across end and beginning.</span>
<span class="sd">        cache_values (bool, optional): to be used with static datasets.</span>
<span class="sd">            Will cache the start and end signal of the trajectory. This can be safely used even</span>
<span class="sd">            if the trajectory indices change during calls to :class:`~torchrl.data.ReplayBuffer.extend`</span>
<span class="sd">            as this operation will erase the cache.</span>

<span class="sd">            .. warning:: ``cache_values=True`` will not work if the sampler is used with a</span>
<span class="sd">                storage that is extended by another buffer. For instance:</span>

<span class="sd">                    &gt;&gt;&gt; buffer0 = ReplayBuffer(storage=storage,</span>
<span class="sd">                    ...     sampler=SliceSampler(num_slices=8, cache_values=True),</span>
<span class="sd">                    ...     writer=ImmutableWriter())</span>
<span class="sd">                    &gt;&gt;&gt; buffer1 = ReplayBuffer(storage=storage,</span>
<span class="sd">                    ...     sampler=other_sampler)</span>
<span class="sd">                    &gt;&gt;&gt; # Wrong! Does not erase the buffer from the sampler of buffer0</span>
<span class="sd">                    &gt;&gt;&gt; buffer1.extend(data)</span>

<span class="sd">            .. warning:: ``cache_values=True`` will not work as expected if the buffer is</span>
<span class="sd">                shared between processes and one process is responsible for writing</span>
<span class="sd">                and one process for sampling, as erasing the cache can only be done locally.</span>

<span class="sd">        truncated_key (NestedKey, optional): If not ``None``, this argument</span>
<span class="sd">            indicates where a truncated signal should be written in the output</span>
<span class="sd">            data. This is used to indicate to value estimators where the provided</span>
<span class="sd">            trajectory breaks. Defaults to ``(&quot;next&quot;, &quot;truncated&quot;)``.</span>
<span class="sd">            This feature only works with :class:`~torchrl.data.replay_buffers.TensorDictReplayBuffer`</span>
<span class="sd">            instances (otherwise the truncated key is returned in the info dictionary</span>
<span class="sd">            returned by the :meth:`~torchrl.data.replay_buffers.ReplayBuffer.sample` method).</span>
<span class="sd">        strict_length (bool, optional): if ``False``, trajectories of length</span>
<span class="sd">            shorter than `slice_len` (or `batch_size // num_slices`) will be</span>
<span class="sd">            allowed to appear in the batch. If ``True``, trajectories shorted</span>
<span class="sd">            than required will be filtered out.</span>
<span class="sd">            Be mindful that this can result in effective `batch_size`  shorter</span>
<span class="sd">            than the one asked for! Trajectories can be split using</span>
<span class="sd">            :func:`~torchrl.collectors.split_trajectories`. Defaults to ``True``.</span>
<span class="sd">        compile (bool or dict of kwargs, optional): if ``True``, the bottleneck of</span>
<span class="sd">            the :meth:`~sample` method will be compiled with :func:`~torch.compile`.</span>
<span class="sd">            Keyword arguments can also be passed to torch.compile with this arg.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        span (bool, int, Tuple[bool | int, bool | int], optional): if provided, the sampled</span>
<span class="sd">            trajectory will span across the left and/or the right. This means that possibly</span>
<span class="sd">            fewer elements will be provided than what was required. A boolean value means</span>
<span class="sd">            that at least one element will be sampled per trajectory. An integer `i` means</span>
<span class="sd">            that at least `slice_len - i` samples will be gathered for each sampled trajectory.</span>
<span class="sd">            Using tuples allows a fine grained control over the span on the left (beginning</span>
<span class="sd">            of the stored trajectory) and on the right (end of the stored trajectory).</span>

<span class="sd">    .. note:: To recover the trajectory splits in the storage,</span>
<span class="sd">        :class:`~torchrl.data.replay_buffers.samplers.SliceSampler` will first</span>
<span class="sd">        attempt to find the ``traj_key`` entry in the storage. If it cannot be</span>
<span class="sd">        found, the ``end_key`` will be used to reconstruct the episodes.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers import LazyMemmapStorage, TensorDictReplayBuffer</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers.samplers import SliceSampler</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt; rb = TensorDictReplayBuffer(</span>
<span class="sd">        ...     storage=LazyMemmapStorage(1_000_000),</span>
<span class="sd">        ...     sampler=SliceSampler(cache_values=True, num_slices=10),</span>
<span class="sd">        ...     batch_size=320,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; episode = torch.zeros(1000, dtype=torch.int)</span>
<span class="sd">        &gt;&gt;&gt; episode[:300] = 1</span>
<span class="sd">        &gt;&gt;&gt; episode[300:550] = 2</span>
<span class="sd">        &gt;&gt;&gt; episode[550:700] = 3</span>
<span class="sd">        &gt;&gt;&gt; episode[700:] = 4</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         &quot;episode&quot;: episode,</span>
<span class="sd">        ...         &quot;obs&quot;: torch.randn((3, 4, 5)).expand(1000, 3, 4, 5),</span>
<span class="sd">        ...         &quot;act&quot;: torch.randn((20,)).expand(1000, 20),</span>
<span class="sd">        ...         &quot;other&quot;: torch.randn((20, 50)).expand(1000, 20, 50),</span>
<span class="sd">        ...     }, [1000]</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">        &gt;&gt;&gt; sample = rb.sample()</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;sample:&quot;, sample)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;episodes&quot;, sample.get(&quot;episode&quot;).unique())</span>
<span class="sd">        episodes tensor([1, 2, 3, 4], dtype=torch.int32)</span>

<span class="sd">    :class:`~torchrl.data.replay_buffers.SliceSampler` is default-compatible with</span>
<span class="sd">    most of TorchRL&#39;s datasets:</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.datasets import RobosetExperienceReplay</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data import SliceSampler</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt; num_slices = 10</span>
<span class="sd">        &gt;&gt;&gt; dataid = list(RobosetExperienceReplay.available_datasets)[0]</span>
<span class="sd">        &gt;&gt;&gt; data = RobosetExperienceReplay(dataid, batch_size=320, sampler=SliceSampler(num_slices=num_slices))</span>
<span class="sd">        &gt;&gt;&gt; for batch in data:</span>
<span class="sd">        ...     batch = batch.reshape(num_slices, -1)</span>
<span class="sd">        ...     break</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;check that each batch only has one episode:&quot;, batch[&quot;episode&quot;].unique(dim=1))</span>
<span class="sd">        check that each batch only has one episode: tensor([[19],</span>
<span class="sd">                [14],</span>
<span class="sd">                [ 8],</span>
<span class="sd">                [10],</span>
<span class="sd">                [13],</span>
<span class="sd">                [ 4],</span>
<span class="sd">                [ 2],</span>
<span class="sd">                [ 3],</span>
<span class="sd">                [22],</span>
<span class="sd">                [ 8]])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">slice_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">end_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">traj_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ends</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trajectories</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_values</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncated_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;truncated&quot;</span><span class="p">),</span>
        <span class="n">strict_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="nb">compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">span</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">bool</span> <span class="o">|</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span> <span class="o">=</span> <span class="n">num_slices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span> <span class="o">=</span> <span class="n">slice_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_key</span> <span class="o">=</span> <span class="n">end_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span> <span class="o">=</span> <span class="n">traj_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span> <span class="o">=</span> <span class="n">truncated_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span> <span class="o">=</span> <span class="n">cache_values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_traj</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span> <span class="o">=</span> <span class="n">strict_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">span</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="n">span</span> <span class="o">=</span> <span class="p">(</span><span class="n">span</span><span class="p">,</span> <span class="n">span</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">span</span> <span class="o">=</span> <span class="n">span</span>

        <span class="k">if</span> <span class="n">trajectories</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">traj_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">end_key</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;`trajectories` and `end_key` or `traj_key` are exclusive arguments.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">ends</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;trajectories and ends are exclusive arguments.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">cache_values</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;To be used, trajectories requires `cache_values` to be set to `True`.&quot;</span>
                <span class="p">)</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_start_stop_traj</span><span class="p">(</span>
                <span class="n">trajectory</span><span class="o">=</span><span class="n">trajectories</span><span class="p">,</span>
                <span class="n">at_capacity</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="s2">&quot;stop-and-length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span>

        <span class="k">elif</span> <span class="n">ends</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">traj_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">end_key</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;`ends` and `end_key` or `traj_key` are exclusive arguments.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">trajectories</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;trajectories and ends are exclusive arguments.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">cache_values</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;To be used, ends requires `cache_values` to be set to `True`.&quot;</span>
                <span class="p">)</span>
            <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_start_stop_traj</span><span class="p">(</span><span class="n">end</span><span class="o">=</span><span class="n">ends</span><span class="p">,</span> <span class="n">at_capacity</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="s2">&quot;stop-and-length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">end_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">end_key</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;done&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">traj_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">traj_key</span> <span class="o">=</span> <span class="s2">&quot;episode&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">end_key</span> <span class="o">=</span> <span class="n">end_key</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span> <span class="o">=</span> <span class="n">traj_key</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">((</span><span class="n">num_slices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">^</span> <span class="p">(</span><span class="n">slice_len</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;Either num_slices or slice_len must be not None, and not both. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Got num_slices=</span><span class="si">{</span><span class="n">num_slices</span><span class="si">}</span><span class="s2"> and slice_len=</span><span class="si">{</span><span class="n">slice_len</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compile</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="nb">compile</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">compile</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">compile</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">kwargs</span> <span class="o">=</span> <span class="nb">compile</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_index</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">get_spawning_popen</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;It seems you are sharing a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> across processes with&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;cache_values=True. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;While this isn&#39;t forbidden and could perfectly work if your dataset &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;is unaltered on both processes, remember that calling extend/add on&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;one process will NOT erase the cache on another process&#39;s sampler, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;which will cause synchronization issues.&quot;</span>
            <span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_cache&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(num_slices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;slice_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;end_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">end_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;traj_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;truncated_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;strict_length=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_find_start_stop_traj</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">trajectory</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">at_capacity</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">trajectory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># slower</span>
            <span class="c1"># _, stop_idx = torch.unique_consecutive(trajectory, return_counts=True)</span>
            <span class="c1"># stop_idx = stop_idx.cumsum(0) - 1</span>

            <span class="c1"># even slower</span>
            <span class="c1"># t = trajectory.unsqueeze(0)</span>
            <span class="c1"># w = torch.tensor([1, -1], dtype=torch.int).view(1, 1, 2)</span>
            <span class="c1"># stop_idx = torch.conv1d(t, w).nonzero()</span>

            <span class="c1"># faster</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">trajectory</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">trajectory</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">end</span><span class="p">,</span> <span class="n">trajectory</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">!=</span> <span class="n">trajectory</span><span class="p">[:</span><span class="mi">1</span><span class="p">]],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">length</span> <span class="o">=</span> <span class="n">trajectory</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># TODO: check that storage is at capacity here, if not we need to assume that the last element of end is True</span>

            <span class="c1"># We presume that not done at the end means that the traj spans across end and beginning of storage</span>
            <span class="n">length</span> <span class="o">=</span> <span class="n">end</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">at_capacity</span><span class="p">:</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_fill</span><span class="p">(</span>
                <span class="n">end</span><span class="p">,</span>
                <span class="n">index</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">end</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">end</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="c1"># we must have at least one end by traj to delimitate trajectories</span>
            <span class="c1"># so if no end can be found we set it manually</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">end</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">end</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mask</span><span class="p">])</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="n">end</span><span class="o">.</span><span class="n">ndim</span>
        <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Expected the end-of-trajectory signal to be at least 1-dimensional.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_end_to_start_stop</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="n">length</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">end</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_end_to_start_stop</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
        <span class="c1"># Using transpose ensures the start and stop are sorted the same way</span>
        <span class="n">stop_idx</span> <span class="o">=</span> <span class="n">end</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span>
        <span class="n">stop_idx</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="c1"># First build the start indices as the stop + 1, we&#39;ll shift it later</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">start_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">start_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">%=</span> <span class="n">end</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1"># shift start: to do this, we check when the non-first dim indices are identical</span>
        <span class="c1"># and get a mask like [False, True, True, False, True, ...] where False means</span>
        <span class="c1"># that there&#39;s a switch from one dim to another (ie, a switch from one element of the batch</span>
        <span class="c1"># to another). We roll this one step along the time dimension and these two</span>
        <span class="c1"># masks provide us with the indices of the permutation matrix we need</span>
        <span class="c1"># to apply to start_idx.</span>
        <span class="k">if</span> <span class="n">start_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">start_idx_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">start_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">==</span> <span class="n">start_idx</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:])</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">m1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">start_idx_mask</span><span class="p">[:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">start_idx_mask</span><span class="p">])</span>
            <span class="n">m2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">start_idx_mask</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">start_idx_mask</span><span class="p">[:</span><span class="mi">1</span><span class="p">])])</span>
            <span class="n">start_idx_replace</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">start_idx</span><span class="p">)</span>
            <span class="n">start_idx_replace</span><span class="p">[</span><span class="n">m1</span><span class="p">]</span> <span class="o">=</span> <span class="n">start_idx</span><span class="p">[</span><span class="n">m2</span><span class="p">]</span>
            <span class="n">start_idx_replace</span><span class="p">[</span><span class="o">~</span><span class="n">m1</span><span class="p">]</span> <span class="o">=</span> <span class="n">start_idx</span><span class="p">[</span><span class="o">~</span><span class="n">m2</span><span class="p">]</span>
            <span class="n">start_idx</span> <span class="o">=</span> <span class="n">start_idx_replace</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># In this case we have only one start and stop has already been set</span>
            <span class="k">pass</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">start_idx</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">lengths</span><span class="p">[</span><span class="n">lengths</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">lengths</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">length</span>
        <span class="k">return</span> <span class="n">start_idx</span><span class="p">,</span> <span class="n">stop_idx</span><span class="p">,</span> <span class="n">lengths</span>

    <span class="k">def</span> <span class="nf">_start_to_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">st</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">length</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

        <span class="n">arange</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">ndims</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">st</span><span class="o">.</span><span class="n">ndim</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">ndims</span><span class="p">:</span>
            <span class="n">arange</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">arange</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">arange</span><span class="p">)]</span> <span class="o">*</span> <span class="n">ndims</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">arange</span> <span class="o">=</span> <span class="n">arange</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">st</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">arange</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="c1"># we do this to make sure that we&#39;re not broadcasting the start</span>
            <span class="c1"># wrong as a tensor with shape [N] can&#39;t be expanded to [N, 1]</span>
            <span class="c1"># without getting an error</span>
            <span class="n">st</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">arange</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">arange</span> <span class="o">+</span> <span class="n">st</span>

    <span class="k">def</span> <span class="nf">_tensor_slices_from_startend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">storage_length</span><span class="p">):</span>
        <span class="c1"># start is a 2d tensor resulting from nonzero()</span>
        <span class="c1"># seq_length is a 1d tensor indicating the desired length of each sequence</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_start_to_end</span><span class="p">(</span><span class="n">_start</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">)</span> <span class="k">for</span> <span class="n">_start</span> <span class="ow">in</span> <span class="n">start</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># when padding is needed</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_start_to_end</span><span class="p">(</span><span class="n">_start</span><span class="p">,</span> <span class="n">_seq_len</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">_start</span><span class="p">,</span> <span class="n">_seq_len</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="n">result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="n">storage_length</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">_get_stop_and_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">,</span> <span class="n">fallback</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span> <span class="ow">and</span> <span class="s2">&quot;stop-and-length&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;stop-and-length&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_traj</span><span class="p">:</span>
            <span class="c1"># We first try with the traj_key</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">TensorStorage</span><span class="p">):</span>
                    <span class="n">trajectory</span> <span class="o">=</span> <span class="n">storage</span><span class="p">[:]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_used_traj_key</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">trajectory</span> <span class="o">=</span> <span class="n">storage</span><span class="p">[:]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                        <span class="k">raise</span>
                    <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;Could not get a tensordict out of the storage, which is required for SliceSampler to compute the trajectories.&quot;</span>
                        <span class="p">)</span>
                <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_start_stop_traj</span><span class="p">(</span>
                    <span class="n">trajectory</span><span class="o">=</span><span class="n">trajectory</span><span class="p">,</span> <span class="n">at_capacity</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">_is_full</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="s2">&quot;stop-and-length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span>
                <span class="k">return</span> <span class="n">vals</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">fallback</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_traj</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stop_and_length</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">fallback</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">raise</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">done</span> <span class="o">=</span> <span class="n">storage</span><span class="p">[:]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">end_key</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                    <span class="k">raise</span>
                <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Could not get a tensordict out of the storage, which is required for SliceSampler to compute the trajectories.&quot;</span>
                    <span class="p">)</span>
                <span class="n">vals</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_find_start_stop_traj</span><span class="p">(</span>
                    <span class="n">end</span><span class="o">=</span><span class="n">done</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">)],</span> <span class="n">at_capacity</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">_is_full</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_values</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span><span class="p">[</span><span class="s2">&quot;stop-and-length&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vals</span>
                <span class="k">return</span> <span class="n">vals</span>
            <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">fallback</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_fetch_traj</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stop_and_length</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">fallback</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">raise</span>

    <span class="k">def</span> <span class="nf">_adjusted_batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The batch-size must be divisible by the number of slices, got &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;batch_size=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2"> and num_slices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span>
            <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;The batch-size must be divisible by the slice length, got &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;batch_size=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2"> and slice_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span>
            <span class="n">num_slices</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span>
        <span class="k">return</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_slices</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="c1"># pick up as many trajs as we need</span>
        <span class="n">start_idx</span><span class="p">,</span> <span class="n">stop_idx</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stop_and_length</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
        <span class="c1"># we have to make sure that the number of dims of the storage</span>
        <span class="c1"># is the same as the stop/start signals since we will</span>
        <span class="c1"># use these to sample the storage</span>
        <span class="k">if</span> <span class="n">start_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected the end-of-trajectory signal to be &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">storage</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">-dimensional. Got a </span><span class="si">{</span><span class="n">start_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> tensor &quot;</span>
                <span class="s2">&quot;instead.&quot;</span>
            <span class="p">)</span>
        <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjusted_batch_size</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">storage_length</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_slices</span><span class="p">(</span>
            <span class="n">lengths</span><span class="p">,</span>
            <span class="n">start_idx</span><span class="p">,</span>
            <span class="n">stop_idx</span><span class="p">,</span>
            <span class="n">seq_length</span><span class="p">,</span>
            <span class="n">num_slices</span><span class="p">,</span>
            <span class="n">storage_length</span><span class="o">=</span><span class="n">storage_length</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sample_slices</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">start_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">stop_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">storage_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">traj_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="c1"># start_idx and stop_idx are 2d tensors organized like a non-zero</span>

        <span class="k">def</span> <span class="nf">get_traj_idx</span><span class="p">(</span><span class="n">maxval</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">maxval</span><span class="p">,</span> <span class="p">(</span><span class="n">num_slices</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">lengths</span> <span class="o">&lt;</span> <span class="n">seq_length</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="p">:</span>
                <span class="n">idx</span> <span class="o">=</span> <span class="n">lengths</span> <span class="o">&gt;=</span> <span class="n">seq_length</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">idx</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Did not find a single trajectory with sufficient length (length range: </span><span class="si">{</span><span class="n">lengths</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">}</span><span class="s2"> - </span><span class="si">{</span><span class="n">lengths</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2"> / required=</span><span class="si">{</span><span class="n">seq_length</span><span class="si">}</span><span class="s2">)).&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="n">seq_length</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">lengths</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">):</span>
                    <span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">lengths_idx</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">start_idx</span> <span class="o">=</span> <span class="n">start_idx</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">stop_idx</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">traj_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">get_traj_idx</span><span class="p">(</span><span class="n">lengths_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Here we must filter out the indices that correspond to trajectories</span>
                    <span class="c1"># we don&#39;t want to keep. That could potentially lead to an empty sample.</span>
                    <span class="c1"># The difficulty with this adjustment is that traj_idx points to a full</span>
                    <span class="c1"># sequences of lengths, but we filter out part of it so we must</span>
                    <span class="c1"># convert traj_idx to a boolean mask, index this mask with the</span>
                    <span class="c1"># valid indices and then recover the nonzero.</span>
                    <span class="n">idx_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
                    <span class="n">idx_mask</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">idx_mask</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">traj_idx</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;None of the provided indices pointed to a trajectory of &quot;</span>
                            <span class="s2">&quot;sufficient length. Consider using strict_length=False for the &quot;</span>
                            <span class="s2">&quot;sampler instead.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">num_slices</span> <span class="o">=</span> <span class="n">traj_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="k">del</span> <span class="n">idx</span>
                <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths_idx</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">traj_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">get_traj_idx</span><span class="p">(</span><span class="n">lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">num_slices</span> <span class="o">=</span> <span class="n">traj_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="c1"># make seq_length a tensor with values clamped by lengths</span>
                <span class="n">seq_length</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="n">seq_length</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">traj_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">get_traj_idx</span><span class="p">(</span><span class="n">lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">num_slices</span> <span class="o">=</span> <span class="n">traj_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_index</span><span class="p">(</span>
            <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">,</span>
            <span class="n">start_idx</span><span class="o">=</span><span class="n">start_idx</span><span class="p">,</span>
            <span class="n">stop_idx</span><span class="o">=</span><span class="n">stop_idx</span><span class="p">,</span>
            <span class="n">num_slices</span><span class="o">=</span><span class="n">num_slices</span><span class="p">,</span>
            <span class="n">seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
            <span class="n">storage_length</span><span class="o">=</span><span class="n">storage_length</span><span class="p">,</span>
            <span class="n">traj_idx</span><span class="o">=</span><span class="n">traj_idx</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_index</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">start_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">stop_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">storage_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">traj_idx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="c1"># end_point is the last possible index for start</span>
        <span class="n">last_indexable_start</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">end_point</span> <span class="o">=</span> <span class="n">last_indexable_start</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">end_point</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">span_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">span_left</span> <span class="o">&gt;=</span> <span class="n">seq_length</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The right and left span must be strictly lower than the sequence length&quot;</span>
                <span class="p">)</span>
            <span class="n">end_point</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">span_left</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">start_point</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">start_point</span> <span class="o">=</span> <span class="o">-</span><span class="n">seq_length</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">span_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">span_right</span> <span class="o">&gt;=</span> <span class="n">seq_length</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The right and left span must be strictly lower than the sequence length&quot;</span>
                <span class="p">)</span>
            <span class="n">start_point</span> <span class="o">=</span> <span class="o">-</span><span class="n">span_right</span>

        <span class="n">relative_starts</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">end_point</span> <span class="o">-</span> <span class="n">start_point</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">floor</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">start_idx</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">+</span> <span class="n">start_point</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">out_of_traj</span> <span class="o">=</span> <span class="n">relative_starts</span> <span class="o">&lt;</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="n">out_of_traj</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="c1"># a negative start means sampling fewer elements</span>
                <span class="n">seq_length</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                    <span class="o">~</span><span class="n">out_of_traj</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">+</span> <span class="n">relative_starts</span>
                <span class="p">)</span>
                <span class="n">relative_starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="o">~</span><span class="n">out_of_traj</span><span class="p">,</span> <span class="n">relative_starts</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">span</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">out_of_traj</span> <span class="o">=</span> <span class="n">relative_starts</span> <span class="o">+</span> <span class="n">seq_length</span> <span class="o">&gt;</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">out_of_traj</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="c1"># a negative start means sampling fewer elements</span>
                <span class="c1"># print(&#39;seq_length before&#39;, seq_length)</span>
                <span class="c1"># print(&#39;relative_starts&#39;, relative_starts)</span>
                <span class="n">seq_length</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
                    <span class="n">seq_length</span><span class="p">,</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">relative_starts</span>
                <span class="p">)</span>
                <span class="c1"># print(&#39;seq_length after&#39;, seq_length)</span>

        <span class="n">starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="p">(</span><span class="n">start_idx</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">relative_starts</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">start_idx</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">:],</span>
            <span class="p">],</span>
            <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_slices_from_startend</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">starts</span><span class="p">,</span> <span class="n">storage_length</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">truncated_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span>
            <span class="n">done_key</span> <span class="o">=</span> <span class="n">_replace_last</span><span class="p">(</span><span class="n">truncated_key</span><span class="p">,</span> <span class="s2">&quot;done&quot;</span><span class="p">)</span>
            <span class="n">terminated_key</span> <span class="o">=</span> <span class="n">_replace_last</span><span class="p">(</span><span class="n">truncated_key</span><span class="p">,</span> <span class="s2">&quot;terminated&quot;</span><span class="p">)</span>

            <span class="n">truncated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">index</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">truncated</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">truncated</span><span class="p">[</span><span class="n">seq_length</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># a traj is terminated if the stop index along col 0 (time)</span>
            <span class="c1"># equates start + traj length - 1</span>
            <span class="n">traj_terminated</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">stop_idx</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">start_idx</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">seq_length</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">terminated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">truncated</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">traj_terminated</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                    <span class="n">terminated</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="n">traj_terminated</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">terminated</span><span class="p">[(</span><span class="n">seq_length</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)[</span><span class="n">traj_terminated</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">truncated</span> <span class="o">=</span> <span class="n">truncated</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">terminated</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="o">|</span> <span class="n">truncated</span>
            <span class="k">return</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">{</span>
                <span class="n">truncated_key</span><span class="p">:</span> <span class="n">truncated</span><span class="p">,</span>
                <span class="n">done_key</span><span class="p">:</span> <span class="n">done</span><span class="p">,</span>
                <span class="n">terminated_key</span><span class="p">:</span> <span class="n">terminated</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">index</span><span class="p">,</span> <span class="p">{}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_used_traj_key</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__used_traj_key&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span><span class="p">)</span>

    <span class="nd">@_used_traj_key</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">_used_traj_key</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;__used_traj_key&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">_used_end_key</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;__used_end_key&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_key</span><span class="p">)</span>

    <span class="nd">@_used_end_key</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">_used_end_key</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;__used_end_key&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op - cache does not need to be saved</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="o">...</span></div>


<div class="viewcode-block" id="SliceSamplerWithoutReplacement"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.SliceSamplerWithoutReplacement.html#torchrl.data.replay_buffers.SliceSamplerWithoutReplacement">[docs]</a><span class="k">class</span> <span class="nc">SliceSamplerWithoutReplacement</span><span class="p">(</span><span class="n">SliceSampler</span><span class="p">,</span> <span class="n">SamplerWithoutReplacement</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Samples slices of data along the first dimension, given start and stop signals, without replacement.</span>

<span class="sd">    This class is to be used with static replay buffers or in between two</span>
<span class="sd">    replay buffer extensions. Extending the replay buffer will reset the</span>
<span class="sd">    the sampler, and continuous sampling without replacement is currently not</span>
<span class="sd">    allowed.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        drop_last (bool, optional): if ``True``, the last incomplete sample (if any) will be dropped.</span>
<span class="sd">            If ``False``, this last sample will be kept.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        num_slices (int): the number of slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``num_slices`` argument. Exclusive</span>
<span class="sd">            with ``slice_len``.</span>
<span class="sd">        slice_len (int): the length of the slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``slice_len`` argument and divisible</span>
<span class="sd">            by it. Exclusive with ``num_slices``.</span>
<span class="sd">        end_key (NestedKey, optional): the key indicating the end of a</span>
<span class="sd">            trajectory (or episode). Defaults to ``(&quot;next&quot;, &quot;done&quot;)``.</span>
<span class="sd">        traj_key (NestedKey, optional): the key indicating the trajectories.</span>
<span class="sd">            Defaults to ``&quot;episode&quot;`` (commonly used across datasets in TorchRL).</span>
<span class="sd">        ends (torch.Tensor, optional): a 1d boolean tensor containing the end of run signals.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">        trajectories (torch.Tensor, optional): a 1d integer tensor containing the run ids.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">        truncated_key (NestedKey, optional): If not ``None``, this argument</span>
<span class="sd">            indicates where a truncated signal should be written in the output</span>
<span class="sd">            data. This is used to indicate to value estimators where the provided</span>
<span class="sd">            trajectory breaks. Defaults to ``(&quot;next&quot;, &quot;truncated&quot;)``.</span>
<span class="sd">            This feature only works with :class:`~torchrl.data.replay_buffers.TensorDictReplayBuffer`</span>
<span class="sd">            instances (otherwise the truncated key is returned in the info dictionary</span>
<span class="sd">            returned by the :meth:`~torchrl.data.replay_buffers.ReplayBuffer.sample` method).</span>
<span class="sd">        strict_length (bool, optional): if ``False``, trajectories of length</span>
<span class="sd">            shorter than `slice_len` (or `batch_size // num_slices`) will be</span>
<span class="sd">            allowed to appear in the batch. If ``True``, trajectories shorted</span>
<span class="sd">            than required will be filtered out.</span>
<span class="sd">            Be mindful that this can result in effective `batch_size`  shorter</span>
<span class="sd">            than the one asked for! Trajectories can be split using</span>
<span class="sd">            :func:`~torchrl.collectors.split_trajectories`. Defaults to ``True``.</span>
<span class="sd">        shuffle (bool, optional): if ``False``, the order of the trajectories</span>
<span class="sd">            is not shuffled. Defaults to ``True``.</span>
<span class="sd">        compile (bool or dict of kwargs, optional): if ``True``, the bottleneck of</span>
<span class="sd">            the :meth:`~sample` method will be compiled with :func:`~torch.compile`.</span>
<span class="sd">            Keyword arguments can also be passed to torch.compile with this arg.</span>
<span class="sd">            Defaults to ``False``.</span>

<span class="sd">    .. note:: To recover the trajectory splits in the storage,</span>
<span class="sd">        :class:`~torchrl.data.replay_buffers.samplers.SliceSamplerWithoutReplacement` will first</span>
<span class="sd">        attempt to find the ``traj_key`` entry in the storage. If it cannot be</span>
<span class="sd">        found, the ``end_key`` will be used to reconstruct the episodes.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers import LazyMemmapStorage, TensorDictReplayBuffer</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers.samplers import SliceSamplerWithoutReplacement</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; rb = TensorDictReplayBuffer(</span>
<span class="sd">        ...     storage=LazyMemmapStorage(1000),</span>
<span class="sd">        ...     # asking for 10 slices for a total of 320 elements, ie, 10 trajectories of 32 transitions each</span>
<span class="sd">        ...     sampler=SliceSamplerWithoutReplacement(num_slices=10),</span>
<span class="sd">        ...     batch_size=320,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; episode = torch.zeros(1000, dtype=torch.int)</span>
<span class="sd">        &gt;&gt;&gt; episode[:300] = 1</span>
<span class="sd">        &gt;&gt;&gt; episode[300:550] = 2</span>
<span class="sd">        &gt;&gt;&gt; episode[550:700] = 3</span>
<span class="sd">        &gt;&gt;&gt; episode[700:] = 4</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         &quot;episode&quot;: episode,</span>
<span class="sd">        ...         &quot;obs&quot;: torch.randn((3, 4, 5)).expand(1000, 3, 4, 5),</span>
<span class="sd">        ...         &quot;act&quot;: torch.randn((20,)).expand(1000, 20),</span>
<span class="sd">        ...         &quot;other&quot;: torch.randn((20, 50)).expand(1000, 20, 50),</span>
<span class="sd">        ...     }, [1000]</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">        &gt;&gt;&gt; sample = rb.sample()</span>
<span class="sd">        &gt;&gt;&gt; # since we want trajectories of 32 transitions but there are only 4 episodes to</span>
<span class="sd">        &gt;&gt;&gt; # sample from, we only get 4 x 32 = 128 transitions in this batch</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;sample:&quot;, sample)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;trajectories in sample&quot;, sample.get(&quot;episode&quot;).unique())</span>

<span class="sd">    :class:`~torchrl.data.replay_buffers.SliceSamplerWithoutReplacement` is default-compatible with</span>
<span class="sd">    most of TorchRL&#39;s datasets, and allows users to consume datasets in a dataloader-like fashion:</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.datasets import RobosetExperienceReplay</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data import SliceSamplerWithoutReplacement</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt; num_slices = 10</span>
<span class="sd">        &gt;&gt;&gt; dataid = list(RobosetExperienceReplay.available_datasets)[0]</span>
<span class="sd">        &gt;&gt;&gt; data = RobosetExperienceReplay(dataid, batch_size=320,</span>
<span class="sd">        ...     sampler=SliceSamplerWithoutReplacement(num_slices=num_slices))</span>
<span class="sd">        &gt;&gt;&gt; # the last sample is kept, since drop_last=False by default</span>
<span class="sd">        &gt;&gt;&gt; for i, batch in enumerate(data):</span>
<span class="sd">        ...     print(batch.get(&quot;episode&quot;).unique())</span>
<span class="sd">        tensor([ 5,  6,  8, 11, 12, 14, 16, 17, 19, 24])</span>
<span class="sd">        tensor([ 1,  2,  7,  9, 10, 13, 15, 18, 21, 22])</span>
<span class="sd">        tensor([ 0,  3,  4, 20, 23])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">slice_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">end_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">traj_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ends</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trajectories</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">truncated_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;truncated&quot;</span><span class="p">),</span>
        <span class="n">strict_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="nb">compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">SliceSampler</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_slices</span><span class="o">=</span><span class="n">num_slices</span><span class="p">,</span>
            <span class="n">slice_len</span><span class="o">=</span><span class="n">slice_len</span><span class="p">,</span>
            <span class="n">end_key</span><span class="o">=</span><span class="n">end_key</span><span class="p">,</span>
            <span class="n">traj_key</span><span class="o">=</span><span class="n">traj_key</span><span class="p">,</span>
            <span class="n">cache_values</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">truncated_key</span><span class="o">=</span><span class="n">truncated_key</span><span class="p">,</span>
            <span class="n">strict_length</span><span class="o">=</span><span class="n">strict_length</span><span class="p">,</span>
            <span class="n">ends</span><span class="o">=</span><span class="n">ends</span><span class="p">,</span>
            <span class="n">trajectories</span><span class="o">=</span><span class="n">trajectories</span><span class="p">,</span>
            <span class="nb">compile</span><span class="o">=</span><span class="nb">compile</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">SamplerWithoutReplacement</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">perc</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample_list</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">len_storage</span> <span class="o">*</span> <span class="mi">100</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;num_slices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;slice_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;end_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">end_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;traj_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;truncated_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;strict_length=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="si">}</span><span class="s2">,&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">perc</span><span class="si">}</span><span class="s2">% sampled)&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cache</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">SamplerWithoutReplacement</span><span class="o">.</span><span class="n">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_storage_len</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage_len_buffer</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="n">start_idx</span><span class="p">,</span> <span class="n">stop_idx</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stop_and_length</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
        <span class="c1"># we have to make sure that the number of dims of the storage</span>
        <span class="c1"># is the same as the stop/start signals since we will</span>
        <span class="c1"># use these to sample the storage</span>
        <span class="k">if</span> <span class="n">start_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected the end-of-trajectory signal to be &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">storage</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">-dimensional. Got a </span><span class="si">{</span><span class="n">start_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> tensor &quot;</span>
                <span class="s2">&quot;instead.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage_len_buffer</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">start_idx</span><span class="p">)</span>
        <span class="c1"># first get indices of the trajectories we want to retrieve</span>
        <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjusted_batch_size</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">indices</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">SamplerWithoutReplacement</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">,</span> <span class="n">num_slices</span><span class="p">)</span>
        <span class="n">storage_length</span> <span class="o">=</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># traj_idx will either be a single tensor or a tuple that can be reorganized</span>
        <span class="c1"># like a non-zero through stacking.</span>
        <span class="k">def</span> <span class="nf">tuple_to_tensor</span><span class="p">(</span><span class="n">traj_idx</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">traj_idx</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">storage</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                    <span class="n">storage</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">)[</span><span class="n">traj_idx</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">traj_idx</span>

        <span class="n">idx</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample_slices</span><span class="p">(</span>
            <span class="n">lengths</span><span class="p">,</span>
            <span class="n">start_idx</span><span class="p">,</span>
            <span class="n">stop_idx</span><span class="p">,</span>
            <span class="n">seq_length</span><span class="p">,</span>
            <span class="n">num_slices</span><span class="p">,</span>
            <span class="n">storage_length</span><span class="p">,</span>
            <span class="n">traj_idx</span><span class="o">=</span><span class="n">tuple_to_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">idx</span><span class="p">,</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="n">SamplerWithoutReplacement</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">SamplerWithoutReplacement</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span></div>


<div class="viewcode-block" id="PrioritizedSliceSampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.PrioritizedSliceSampler.html#torchrl.data.replay_buffers.PrioritizedSliceSampler">[docs]</a><span class="k">class</span> <span class="nc">PrioritizedSliceSampler</span><span class="p">(</span><span class="n">SliceSampler</span><span class="p">,</span> <span class="n">PrioritizedSampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Samples slices of data along the first dimension, given start and stop signals, using prioritized sampling.</span>

<span class="sd">    This class samples sub-trajectories with replacement following a priority weighting presented in &quot;Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2015.</span>
<span class="sd">        Prioritized experience replay.&quot;</span>
<span class="sd">        (https://arxiv.org/abs/1511.05952)</span>

<span class="sd">    For more info see :class:`~torchrl.data.replay_buffers.samplers.SliceSampler` and :class:`~torchrl.data.replay_buffers.samplers.PrioritizedSampler`.</span>

<span class="sd">    Args:</span>
<span class="sd">        alpha (float): exponent α determines how much prioritization is used,</span>
<span class="sd">            with α = 0 corresponding to the uniform case.</span>
<span class="sd">        beta (float): importance sampling negative exponent.</span>
<span class="sd">        eps (float, optional): delta added to the priorities to ensure that the buffer</span>
<span class="sd">            does not contain null priorities. Defaults to 1e-8.</span>
<span class="sd">        reduction (str, optional): the reduction method for multidimensional</span>
<span class="sd">            tensordicts (i.e., stored trajectory). Can be one of &quot;max&quot;, &quot;min&quot;,</span>
<span class="sd">            &quot;median&quot; or &quot;mean&quot;.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        num_slices (int): the number of slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``num_slices`` argument. Exclusive</span>
<span class="sd">            with ``slice_len``.</span>
<span class="sd">        slice_len (int): the length of the slices to be sampled. The batch-size</span>
<span class="sd">            must be greater or equal to the ``slice_len`` argument and divisible</span>
<span class="sd">            by it. Exclusive with ``num_slices``.</span>
<span class="sd">        end_key (NestedKey, optional): the key indicating the end of a</span>
<span class="sd">            trajectory (or episode). Defaults to ``(&quot;next&quot;, &quot;done&quot;)``.</span>
<span class="sd">        traj_key (NestedKey, optional): the key indicating the trajectories.</span>
<span class="sd">            Defaults to ``&quot;episode&quot;`` (commonly used across datasets in TorchRL).</span>
<span class="sd">        ends (torch.Tensor, optional): a 1d boolean tensor containing the end of run signals.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">        trajectories (torch.Tensor, optional): a 1d integer tensor containing the run ids.</span>
<span class="sd">            To be used whenever the ``end_key`` or ``traj_key`` is expensive to get,</span>
<span class="sd">            or when this signal is readily available. Must be used with ``cache_values=True``</span>
<span class="sd">            and cannot be used in conjunction with ``end_key`` or ``traj_key``.</span>
<span class="sd">        cache_values (bool, optional): to be used with static datasets.</span>
<span class="sd">            Will cache the start and end signal of the trajectory. This can be safely used even</span>
<span class="sd">            if the trajectory indices change during calls to :class:`~torchrl.data.ReplayBuffer.extend`</span>
<span class="sd">            as this operation will erase the cache.</span>

<span class="sd">            .. warning:: ``cache_values=True`` will not work if the sampler is used with a</span>
<span class="sd">                storage that is extended by another buffer. For instance:</span>

<span class="sd">                    &gt;&gt;&gt; buffer0 = ReplayBuffer(storage=storage,</span>
<span class="sd">                    ...     sampler=SliceSampler(num_slices=8, cache_values=True),</span>
<span class="sd">                    ...     writer=ImmutableWriter())</span>
<span class="sd">                    &gt;&gt;&gt; buffer1 = ReplayBuffer(storage=storage,</span>
<span class="sd">                    ...     sampler=other_sampler)</span>
<span class="sd">                    &gt;&gt;&gt; # Wrong! Does not erase the buffer from the sampler of buffer0</span>
<span class="sd">                    &gt;&gt;&gt; buffer1.extend(data)</span>

<span class="sd">            .. warning:: ``cache_values=True`` will not work as expected if the buffer is</span>
<span class="sd">                shared between processes and one process is responsible for writing</span>
<span class="sd">                and one process for sampling, as erasing the cache can only be done locally.</span>

<span class="sd">        truncated_key (NestedKey, optional): If not ``None``, this argument</span>
<span class="sd">            indicates where a truncated signal should be written in the output</span>
<span class="sd">            data. This is used to indicate to value estimators where the provided</span>
<span class="sd">            trajectory breaks. Defaults to ``(&quot;next&quot;, &quot;truncated&quot;)``.</span>
<span class="sd">            This feature only works with :class:`~torchrl.data.replay_buffers.TensorDictReplayBuffer`</span>
<span class="sd">            instances (otherwise the truncated key is returned in the info dictionary</span>
<span class="sd">            returned by the :meth:`~torchrl.data.replay_buffers.ReplayBuffer.sample` method).</span>
<span class="sd">        strict_length (bool, optional): if ``False``, trajectories of length</span>
<span class="sd">            shorter than `slice_len` (or `batch_size // num_slices`) will be</span>
<span class="sd">            allowed to appear in the batch. If ``True``, trajectories shorted</span>
<span class="sd">            than required will be filtered out.</span>
<span class="sd">            Be mindful that this can result in effective `batch_size`  shorter</span>
<span class="sd">            than the one asked for! Trajectories can be split using</span>
<span class="sd">            :func:`~torchrl.collectors.split_trajectories`. Defaults to ``True``.</span>
<span class="sd">        compile (bool or dict of kwargs, optional): if ``True``, the bottleneck of</span>
<span class="sd">            the :meth:`~sample` method will be compiled with :func:`~torch.compile`.</span>
<span class="sd">            Keyword arguments can also be passed to torch.compile with this arg.</span>
<span class="sd">            Defaults to ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.replay_buffers import TensorDictReplayBuffer, LazyMemmapStorage, PrioritizedSliceSampler</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; sampler = PrioritizedSliceSampler(max_capacity=9, num_slices=3, alpha=0.7, beta=0.9)</span>
<span class="sd">        &gt;&gt;&gt; rb = TensorDictReplayBuffer(storage=LazyMemmapStorage(9), sampler=sampler, batch_size=6)</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         &quot;observation&quot;: torch.randn(9,16),</span>
<span class="sd">        ...         &quot;action&quot;: torch.randn(9, 1),</span>
<span class="sd">        ...         &quot;episode&quot;: torch.tensor([0,0,0,1,1,1,2,2,2], dtype=torch.long),</span>
<span class="sd">        ...         &quot;steps&quot;: torch.tensor([0,1,2,0,1,2,0,1,2], dtype=torch.long),</span>
<span class="sd">        ...         (&quot;next&quot;, &quot;observation&quot;): torch.randn(9,16),</span>
<span class="sd">        ...         (&quot;next&quot;, &quot;reward&quot;): torch.randn(9,1),</span>
<span class="sd">        ...         (&quot;next&quot;, &quot;done&quot;): torch.tensor([0,0,1,0,0,1,0,0,1], dtype=torch.bool).unsqueeze(1),</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     batch_size=[9],</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">        &gt;&gt;&gt; sample, info = rb.sample(return_info=True)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;episode&quot;, sample[&quot;episode&quot;].tolist())</span>
<span class="sd">        episode [2, 2, 2, 2, 1, 1]</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;steps&quot;, sample[&quot;steps&quot;].tolist())</span>
<span class="sd">        steps [1, 2, 0, 1, 1, 2]</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;weight&quot;, info[&quot;_weight&quot;].tolist())</span>
<span class="sd">        weight [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; priority = torch.tensor([0,3,3,0,0,0,1,1,1])</span>
<span class="sd">        &gt;&gt;&gt; rb.update_priority(torch.arange(0,9,1), priority=priority)</span>
<span class="sd">        &gt;&gt;&gt; sample, info = rb.sample(return_info=True)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;episode&quot;, sample[&quot;episode&quot;].tolist())</span>
<span class="sd">        episode [2, 2, 2, 2, 2, 2]</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;steps&quot;, sample[&quot;steps&quot;].tolist())</span>
<span class="sd">        steps [1, 2, 0, 1, 0, 1]</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;weight&quot;, info[&quot;_weight&quot;].tolist())</span>
<span class="sd">        weight [9.120110917137936e-06, 9.120110917137936e-06, 9.120110917137936e-06, 9.120110917137936e-06, 9.120110917137936e-06, 9.120110917137936e-06]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">max_capacity</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">num_slices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">slice_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">end_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">traj_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ends</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trajectories</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_values</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">truncated_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;truncated&quot;</span><span class="p">),</span>
        <span class="n">strict_length</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="nb">compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="nb">dict</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">SliceSampler</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">num_slices</span><span class="o">=</span><span class="n">num_slices</span><span class="p">,</span>
            <span class="n">slice_len</span><span class="o">=</span><span class="n">slice_len</span><span class="p">,</span>
            <span class="n">end_key</span><span class="o">=</span><span class="n">end_key</span><span class="p">,</span>
            <span class="n">traj_key</span><span class="o">=</span><span class="n">traj_key</span><span class="p">,</span>
            <span class="n">cache_values</span><span class="o">=</span><span class="n">cache_values</span><span class="p">,</span>
            <span class="n">truncated_key</span><span class="o">=</span><span class="n">truncated_key</span><span class="p">,</span>
            <span class="n">strict_length</span><span class="o">=</span><span class="n">strict_length</span><span class="p">,</span>
            <span class="n">ends</span><span class="o">=</span><span class="n">ends</span><span class="p">,</span>
            <span class="n">trajectories</span><span class="o">=</span><span class="n">trajectories</span><span class="p">,</span>
            <span class="nb">compile</span><span class="o">=</span><span class="nb">compile</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">max_capacity</span><span class="o">=</span><span class="n">max_capacity</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
            <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;num_slices=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_slices</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;slice_len=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">slice_len</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;end_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">end_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;traj_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">traj_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;truncated_key=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;strict_length=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="si">}</span><span class="s2">,&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;alpha=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_alpha</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;beta=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_beta</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;eps=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_eps</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">SliceSampler</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">state</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PrioritizedSampler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">SliceSampler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="c1"># Sample `batch_size` indices representing the start of a slice.</span>
        <span class="c1"># The sampling is based on a weight vector.</span>
        <span class="n">start_idx</span><span class="p">,</span> <span class="n">stop_idx</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_stop_and_length</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span>
        <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_slices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_adjusted_batch_size</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="n">num_trajs</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_trajs</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">lengths</span> <span class="o">&lt;</span> <span class="n">seq_length</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">strict_length</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Some stored trajectories have a length shorter than the slice that was asked for. &quot;</span>
                    <span class="s2">&quot;Create the sampler with `strict_length=False` to allow shorter trajectories to appear &quot;</span>
                    <span class="s2">&quot;in you batch.&quot;</span>
                <span class="p">)</span>
            <span class="c1"># make seq_length a tensor with values clamped by lengths</span>
            <span class="n">seq_length</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="n">seq_length</span><span class="p">)</span>

        <span class="c1"># build a list of index that we don&#39;t want to sample: all the steps at a `seq_length` distance of</span>
        <span class="c1"># the end the trajectory, with the end of trajectory (`stop_idx`) included</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;seq_length as a list is not supported for now. seq_length=</span><span class="si">{</span><span class="n">seq_length</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>

        <span class="n">subtractive_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">stop_idx</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">stop_idx</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">subtractive_idx</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
        <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="n">preceding_stop_idx</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">preceding_stop_idx</span><span class="p">,</span>
                <span class="n">stop_idx</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">seq_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
            <span class="p">],</span>
            <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># convert the 2d index into a flat one to accomodate the _sum_tree</span>
            <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ravel_multi_index</span><span class="p">(</span>
                    <span class="nb">tuple</span><span class="p">(</span><span class="n">preceding_stop_idx</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()),</span> <span class="n">storage</span><span class="o">.</span><span class="n">shape</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">preceding_stop_idx</span> <span class="o">=</span> <span class="n">preceding_stop_idx</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># force to not sample index at the end of a trajectory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sum_tree</span><span class="p">[</span><span class="n">preceding_stop_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="c1"># and no need to update self._min_tree</span>

        <span class="n">starts</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="o">=</span><span class="n">storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">seq_length</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">starts</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">starts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">starts</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># starts = torch.as_tensor(starts, device=lengths.device)</span>
        <span class="n">info</span><span class="p">[</span><span class="s2">&quot;_weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s2">&quot;_weight&quot;</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># extends starting indices of each slice with sequence_length to get indices of all steps</span>
        <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tensor_slices_from_startend</span><span class="p">(</span>
            <span class="n">seq_length</span><span class="p">,</span> <span class="n">starts</span><span class="p">,</span> <span class="n">storage_length</span><span class="o">=</span><span class="n">storage</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># repeat the weight of each slice to match the number of steps</span>
        <span class="n">info</span><span class="p">[</span><span class="s2">&quot;_weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="n">info</span><span class="p">[</span><span class="s2">&quot;_weight&quot;</span><span class="p">],</span> <span class="n">seq_length</span><span class="p">)</span>

        <span class="c1"># sanity check</span>
        <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Number of indices is expected to match the batch size (</span><span class="si">{</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">).&quot;</span>
            <span class="p">)</span>

        <span class="c1"># if self.truncated_key is not None:</span>
        <span class="c1">#     truncated_key = self.truncated_key</span>
        <span class="c1">#     done_key = _replace_last(truncated_key, &quot;done&quot;)</span>
        <span class="c1">#     terminated_key = _replace_last(truncated_key, &quot;terminated&quot;)</span>
        <span class="c1">#</span>
        <span class="c1">#     truncated = torch.zeros(</span>
        <span class="c1">#         (index.shape[0], 1), dtype=torch.bool, device=index.device</span>
        <span class="c1">#     )</span>
        <span class="c1">#     if isinstance(seq_length, int):</span>
        <span class="c1">#         truncated.view(num_slices, -1)[:, -1] = 1</span>
        <span class="c1">#     else:</span>
        <span class="c1">#         truncated[seq_length.cumsum(0) - 1] = 1</span>
        <span class="c1">#     # a traj is terminated if the stop index along col 0 (time)</span>
        <span class="c1">#     # equates start + traj length - 1</span>
        <span class="c1">#     traj_terminated = (</span>
        <span class="c1">#         stop_idx[traj_idx, 0] == start_idx[traj_idx, 0] + seq_length - 1</span>
        <span class="c1">#     )</span>
        <span class="c1">#     terminated = torch.zeros_like(truncated)</span>
        <span class="c1">#     if traj_terminated.any():</span>
        <span class="c1">#         if isinstance(seq_length, int):</span>
        <span class="c1">#             truncated.view(num_slices, -1)[traj_terminated] = 1</span>
        <span class="c1">#         else:</span>
        <span class="c1">#             truncated[(seq_length.cumsum(0) - 1)[traj_terminated]] = 1</span>
        <span class="c1">#     truncated = truncated &amp; ~terminated</span>
        <span class="c1">#     done = terminated | truncated</span>
        <span class="c1">#     return index.to(torch.long).unbind(-1), {</span>
        <span class="c1">#         truncated_key: truncated,</span>
        <span class="c1">#         done_key: done,</span>
        <span class="c1">#         terminated_key: terminated,</span>
        <span class="c1">#     }</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># TODO: fix this part</span>
            <span class="c1"># following logics borrowed from SliceSampler</span>
            <span class="n">truncated_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">truncated_key</span>
            <span class="n">done_key</span> <span class="o">=</span> <span class="n">_replace_last</span><span class="p">(</span><span class="n">truncated_key</span><span class="p">,</span> <span class="s2">&quot;done&quot;</span><span class="p">)</span>
            <span class="n">terminated_key</span> <span class="o">=</span> <span class="n">_replace_last</span><span class="p">(</span><span class="n">truncated_key</span><span class="p">,</span> <span class="s2">&quot;terminated&quot;</span><span class="p">)</span>

            <span class="n">truncated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">index</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">truncated</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">truncated</span><span class="p">[</span><span class="n">seq_length</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">traj_terminated</span> <span class="o">=</span> <span class="n">stop_idx</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="p">(</span>
                <span class="n">start_idx</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">seq_length</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">terminated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">truncated</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">traj_terminated</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                    <span class="n">terminated</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_slices</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="n">traj_terminated</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">terminated</span><span class="p">[(</span><span class="n">seq_length</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)[</span><span class="n">traj_terminated</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">truncated</span> <span class="o">=</span> <span class="n">truncated</span> <span class="o">&amp;</span> <span class="o">~</span><span class="n">terminated</span>
            <span class="n">done</span> <span class="o">=</span> <span class="n">terminated</span> <span class="o">|</span> <span class="n">truncated</span>

            <span class="n">info</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="n">truncated_key</span><span class="p">:</span> <span class="n">truncated</span><span class="p">,</span>
                    <span class="n">done_key</span><span class="p">:</span> <span class="n">done</span><span class="p">,</span>
                    <span class="n">terminated_key</span><span class="p">:</span> <span class="n">terminated</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">index</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># no op for SliceSampler</span>
        <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op for SliceSampler</span>
        <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="c1"># no op for SliceSampler</span>
        <span class="k">return</span> <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># no op for SliceSampler</span>
        <span class="k">return</span> <span class="n">PrioritizedSampler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></div>


<div class="viewcode-block" id="SamplerEnsemble"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.SamplerEnsemble.html#torchrl.data.replay_buffers.SamplerEnsemble">[docs]</a><span class="k">class</span> <span class="nc">SamplerEnsemble</span><span class="p">(</span><span class="n">Sampler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An ensemble of samplers.</span>

<span class="sd">    This class is designed to work with :class:`~torchrl.data.replay_buffers.replay_buffers.ReplayBufferEnsemble`.</span>
<span class="sd">    It contains the samplers as well as the sampling strategy hyperparameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        samplers (sequence of Sampler): the samplers to make the composite sampler.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        p (list or tensor of probabilities, optional): if provided, indicates the</span>
<span class="sd">            weights of each dataset during sampling.</span>
<span class="sd">        sample_from_all (bool, optional): if ``True``, each dataset will be sampled</span>
<span class="sd">            from. This is not compatible with the ``p`` argument. Defaults to ``False``.</span>
<span class="sd">        num_buffer_sampled (int, optional): the number of buffers to sample.</span>
<span class="sd">            if ``sample_from_all=True``, this has no effect, as it defaults to the</span>
<span class="sd">            number of buffers. If ``sample_from_all=False``, buffers will be</span>
<span class="sd">            sampled according to the probabilities ``p``.</span>

<span class="sd">    .. warning::</span>
<span class="sd">      The indices provided in the info dictionary are placed in a :class:`~tensordict.TensorDict` with</span>
<span class="sd">      keys ``index`` and ``buffer_ids`` that allow the upper :class:`~torchrl.data.ReplayBufferEnsemble`</span>
<span class="sd">      and :class:`~torchrl.data.StorageEnsemble` objects to retrieve the data.</span>
<span class="sd">      This format is different from with other samplers which usually return indices</span>
<span class="sd">      as regular tensors.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">samplers</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_from_all</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_buffer_sampled</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span> <span class="o">=</span> <span class="n">samplers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_all</span> <span class="o">=</span> <span class="n">sample_from_all</span>
        <span class="k">if</span> <span class="n">sample_from_all</span> <span class="ow">and</span> <span class="n">p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot pass both `p` argument and `sample_from_all=True`.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span> <span class="o">=</span> <span class="n">num_buffer_sampled</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">p</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_p</span>

    <span class="nd">@p</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">p</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">value</span> <span class="o">/</span> <span class="n">value</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_p</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_buffer_sampled</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_num_buffer_sampled&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_num_buffer_sampled&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="nd">@num_buffer_sampled</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">num_buffer_sampled</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;_num_buffer_sampled&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">StorageEnsemble</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span>
        <span class="n">sub_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_from_all</span><span class="p">:</span>
            <span class="n">samples</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="o">*</span><span class="p">[</span>
                    <span class="n">sampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">sub_batch_size</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">storage</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">_storages</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span>
            <span class="n">buffer_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">buffer_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span><span class="p">,)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">buffer_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">samples</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="o">*</span><span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">_storages</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sub_batch_size</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">buffer_ids</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">sample</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">sample</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="n">samples_stack</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">samples_stack</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>

        <span class="n">samples</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;index&quot;</span><span class="p">:</span> <span class="n">samples_stack</span><span class="p">,</span>
                <span class="s2">&quot;buffer_ids&quot;</span><span class="p">:</span> <span class="n">buffer_ids</span><span class="p">,</span>
            <span class="p">},</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">infos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">TensorDict</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="n">samples</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">info</span>
                <span class="k">else</span> <span class="n">TensorDict</span><span class="p">({},</span> <span class="p">[])</span>
                <span class="k">for</span> <span class="n">info</span> <span class="ow">in</span> <span class="n">infos</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span><span class="p">,</span> <span class="n">infos</span>

    <span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">Path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">):</span>
            <span class="n">sampler</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">Path</span><span class="p">):</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">):</span>
            <span class="n">sampler</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">):</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">sampler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sampler</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">):</span>
            <span class="n">sampler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="n">_INDEX_ERROR</span> <span class="o">=</span> <span class="s2">&quot;Expected an index of type torch.Tensor, range, np.ndarray, int, slice or ellipsis, got </span><span class="si">{}</span><span class="s2"> instead.&quot;</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">Ellipsis</span><span class="p">:</span>
                <span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Tuple of length greater than 1 are not accepted to index samplers of type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">result</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">slice</span><span class="p">)</span> <span class="ow">and</span> <span class="n">index</span> <span class="o">==</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">range</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot index a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> with tensor indices that have more than one dimension.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;A floating point index was recieved when an integer dtype was expected.&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">slice</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_INDEX_ERROR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">index</span><span class="p">)))</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_INDEX_ERROR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">index</span><span class="p">)))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="n">samplers</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">index</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># slice</span>
            <span class="n">samplers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_p</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">SamplerEnsemble</span><span class="p">(</span>
            <span class="o">*</span><span class="n">samplers</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
            <span class="n">sample_from_all</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_from_all</span><span class="p">,</span>
            <span class="n">num_buffer_sampled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_buffer_sampled</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">samplers</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;samplers=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_samplers</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="se">\n</span><span class="si">{</span><span class="n">samplers</span><span class="si">}</span><span class="s2">)&quot;</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>

        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>