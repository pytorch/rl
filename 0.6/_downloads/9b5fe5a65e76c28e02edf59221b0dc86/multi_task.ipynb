{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Task-specific policy in multi-task environments\nThis tutorial details how multi-task policies and batched environments can be used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At the end of this tutorial, you will be capable of writing policies that\ncan compute actions in diverse settings using a distinct set of weights.\nYou will also be able to execute diverse environments in parallel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import TensorDictModule, TensorDictSequential\nfrom torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import CatTensors, Compose, DoubleToFloat, ParallelEnv, TransformedEnv\nfrom torchrl.envs.libs.dm_control import DMControlEnv\nfrom torchrl.modules import MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We design two environments, one humanoid that must complete the stand task\nand another that must learn to walk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env1 = DMControlEnv(\"humanoid\", \"stand\")\nenv1_obs_keys = list(env1.observation_spec.keys())\nenv1 = TransformedEnv(\n    env1,\n    Compose(\n        CatTensors(env1_obs_keys, \"observation_stand\", del_keys=False),\n        CatTensors(env1_obs_keys, \"observation\"),\n        DoubleToFloat(\n            in_keys=[\"observation_stand\", \"observation\"],\n            in_keys_inv=[\"action\"],\n        ),\n    ),\n)\nenv2 = DMControlEnv(\"humanoid\", \"walk\")\nenv2_obs_keys = list(env2.observation_spec.keys())\nenv2 = TransformedEnv(\n    env2,\n    Compose(\n        CatTensors(env2_obs_keys, \"observation_walk\", del_keys=False),\n        CatTensors(env2_obs_keys, \"observation\"),\n        DoubleToFloat(\n            in_keys=[\"observation_walk\", \"observation\"],\n            in_keys_inv=[\"action\"],\n        ),\n    ),\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tdreset1 = env1.reset()\ntdreset2 = env2.reset()\n\n# With LazyStackedTensorDict, stacking is done in a lazy manner: the original tensordicts\n# can still be recovered by indexing the main tensordict\ntdreset = LazyStackedTensorDict.lazy_stack([tdreset1, tdreset2], 0)\nassert tdreset[0] is tdreset1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tdreset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy\n\nWe will design a policy where a backbone reads the \"observation\" key.\nThen specific sub-components will ready the \"observation_stand\" and\n\"observation_walk\" keys of the stacked tensordicts, if they are present,\nand pass them through the dedicated sub-network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "action_dim = env1.action_spec.shape[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy_common = TensorDictModule(\n    nn.Linear(67, 64), in_keys=[\"observation\"], out_keys=[\"hidden\"]\n)\npolicy_stand = TensorDictModule(\n    MLP(67 + 64, action_dim, depth=2),\n    in_keys=[\"observation_stand\", \"hidden\"],\n    out_keys=[\"action\"],\n)\npolicy_walk = TensorDictModule(\n    MLP(67 + 64, action_dim, depth=2),\n    in_keys=[\"observation_walk\", \"hidden\"],\n    out_keys=[\"action\"],\n)\nseq = TensorDictSequential(\n    policy_common, policy_stand, policy_walk, partial_tolerant=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check that our sequence outputs actions for a single env (stand).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seq(env1.reset())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check that our sequence outputs actions for a single env (walk).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seq(env2.reset())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This also works with the stack: now the stand and walk keys have\ndisappeared, because they're not shared by all tensordicts. But the\n``TensorDictSequential`` still performed the operations. Note that the\nbackbone was executed in a vectorized way - not in a loop - which is more efficient.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seq(tdreset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executing diverse tasks in parallel\n\nWe can parallelize the operations if the common keys-value pairs share the\nsame specs (in particular their shape and dtype must match: you can't do the\nfollowing if the observation shapes are different but are pointed to by the\nsame key).\n\nIf ParallelEnv receives a single env making function, it will assume that\na single task has to be performed. If a list of functions is provided, then\nit will assume that we are in a multi-task setting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def env1_maker():\n    return TransformedEnv(\n        DMControlEnv(\"humanoid\", \"stand\"),\n        Compose(\n            CatTensors(env1_obs_keys, \"observation_stand\", del_keys=False),\n            CatTensors(env1_obs_keys, \"observation\"),\n            DoubleToFloat(\n                in_keys=[\"observation_stand\", \"observation\"],\n                in_keys_inv=[\"action\"],\n            ),\n        ),\n    )\n\n\ndef env2_maker():\n    return TransformedEnv(\n        DMControlEnv(\"humanoid\", \"walk\"),\n        Compose(\n            CatTensors(env2_obs_keys, \"observation_walk\", del_keys=False),\n            CatTensors(env2_obs_keys, \"observation\"),\n            DoubleToFloat(\n                in_keys=[\"observation_walk\", \"observation\"],\n                in_keys_inv=[\"action\"],\n            ),\n        ),\n    )\n\n\nenv = ParallelEnv(2, [env1_maker, env2_maker])\nassert not env._single_task\n\ntdreset = env.reset()\nprint(tdreset)\nprint(tdreset[0])\nprint(tdreset[1])  # should be different"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's pass the output through our network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tdreset = seq(tdreset)\nprint(tdreset)\nprint(tdreset[0])\nprint(tdreset[1])  # should be different but all have an \"action\" key\n\n\nenv.step(tdreset)  # computes actions and execute steps in parallel\nprint(tdreset)\nprint(tdreset[0])\nprint(tdreset[1])  # next_observation has now been written"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rollout\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "td_rollout = env.rollout(100, policy=seq, return_contiguous=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "td_rollout[:, 0]  # tensordict of the first step: only the common keys are shown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "td_rollout[0]  # tensordict of the first env: the stand obs is present\n\nenv.close()\ndel env"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}