io:
  eval_interval: 6
  log_interval: 1
  eval_iters: 10
  logger: wandb
  project_name: torchrl_example_rlhf
  group_name: null
data:
  batch_size: 4 # if gradient_accumulation_steps > 1, this is the micro-batch size
  block_size: 550
  num_workers: 1
model:
  name_or_path: ./out
  out_dir: ./out_rlhf
  dropout: 0.1 # for pretraining 0 is good, for finetuning try 0.1+
reward_model:
  name_or_path: ./out_reward
train:
  grad_clip: 1.0
  max_epochs: 1000 # total number of training iterations
  always_save_checkpoint: True # if True, always save a checkpoint after each eval
  decay_lr: True
  optimizer:
    # keyword arguments for torch.optim.AdamW
    lr: 5.0e-5
    weight_decay: 0.0  # 01
    betas: [0.9, 0.999]
  scheduler:
    # keyword arguments for torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: 3000  # max_epochs * num_rollouts / ppo_batch_size
    eta_min: 5.0e-6
  ppo:
    episode_length: 50
    ppo_batch_size: 16
    ppo_num_epochs: 3
    num_rollouts_per_epoch: 32
sys:
  device: cuda # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
  ref_device: cuda:1  # device of reference model
  dtype: bfloat16 # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
  compile: False # use PyTorch 2.0 to compile the model to be faster
