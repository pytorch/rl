# Task and env
env:
  name: HalfCheetah-v3
  task: ""
  library: gym
  stacked_frames: 20
  inference_context: 5
  n_samples_stats: 2000
  frame_skip: 1
  num_train_envs: 1
  num_eval_envs: 10
  reward_scaling: 0.001 # for r2g
  noop: 1
  seed: 42
  target_return_mode: reduce
  eval_target_return: 6000
  collect_target_return: 12000


# logger
logger:
  backend: wandb
  exp_name: oDT-HalfCheetah-medium-v2
  model_name: oDT
  pretrain_log_interval: 500 # record interval in frames
  fintune_log_interval: 1
  eval_steps: 1000

# Buffer
replay_buffer:
  dataset: halfcheetah-medium-v2
  batch_size: 256
  prb: 0
  stacked_frames: 20
  buffer_prefetch: 64
  capacity: 1_000_000
  buffer_scratch_dir: "/tmp/"
  device: cpu
  prefetch: 3

# Optimization
optim:
  device: cpu
  lr: 1.0e-4
  weight_decay: 5.0e-4
  batch_size: 256
  lr_scheduler: ""
  pretrain_gradient_steps: 10000
  updates_per_episode: 300
  warmup_steps: 10000
  clip_grad: 0.25

# loss
loss: 
  alpha_init: 0.1
  target_entropy: auto

transformer:
  n_embd: 512
  n_layer: 4
  n_head: 4
  n_inner: 2048 # 4*512
  activation: relu
  n_positions: 1024
  resid_pdrop: 0.1
  attn_pdrop: 0.1
