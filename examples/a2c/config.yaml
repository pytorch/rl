# Environment
env_library: gym  # env_library used for the simulated environment.
env_name: HalfCheetah-v4  # name of the environment to be created. Default=Humanoid-v2
frame_skip: 2  # frame_skip for the environment.
batch_transform: True

# Logger
logger: wandb  # recorder type to be used. One of 'tensorboard', 'wandb' or 'csv'
record_video: False  # whether a video of the task should be rendered during logging.
exp_name: A2C  # experiment name. Used for logging directory.
record_interval: 100  # number of batch collections in between two collections of validation rollouts. Default=1000.

# Collector
frames_per_batch: 64  # Number of steps executed in the environment per collection.
total_frames: 2_000_000  # total number of frames collected for training. Does account for frame_skip.
num_workers: 2  # Number of workers used for data collection.
env_per_collector: 2  # Number of environments per collector. If the env_per_collector is in the range:

# Model
default_policy_scale: 1.0  # Default policy scale parameter
distribution: tanh_normal  # if True, uses a Tanh-Normal-Tanh distribution for the policy
lstm: False  # if True, uses an LSTM for the policy.
shared_mapping: False  # if True, the first layers of the actor-critic are shared.

# Objective
gamma: 0.99
entropy_coef: 0.01  # Entropy factor for the A2C loss
critic_coef: 0.25  # Critic factor for the A2C loss
critic_loss_function: l2  # loss function for the value network. Either one of l1, l2 or smooth_l1 (default).

# Trainer
optim_steps_per_batch: 1  # Number of optimization steps in between two collection of data.
optimizer: adam  # Optimizer to be used.
lr_scheduler: ""  # LR scheduler.
batch_size: 64  # batch size of the TensorDict retrieved from the replay buffer. Default=256.
log_interval: 1  # logging interval, in terms of optimization steps. Default=10000.
lr: 0.0007  # Learning rate used for the optimizer. Default=3e-4.
normalize_rewards_online: True  # Computes the running statistics of the rewards and normalizes them before they are passed to the loss module.
normalize_rewards_online_scale: 1.0  # Final scale of the normalized rewards.
normalize_rewards_online_decay: 0.0  # Decay of the reward moving averaging
sub_traj_len: 64  # length of the trajectories that sub-samples must have in online settings.
