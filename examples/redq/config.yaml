# Seed for collector and model
seed: 0

# Number of workers for the whole script, split among collector
num_workers: 2

env:
  name: HalfCheetah-v4
  task: ""
  library: gym
  normalize_rewards_online: 1
  normalize_rewards_online_scale: 5
  normalize_rewards_online_decay: 0.999
  frame_skip: 1
  from_pixels: 0
  batch_transform: 1
  norm_stats: 1
  init_env_steps: 10000
  reward_scaling:
  reward_loc:
  vecnorm: False
  categorical_action_encoding:
  noops:
  center_crop:
  catframes:
  image_size:
  grayscale: False

collector:
  async_collection: 1
  frames_per_batch: 1024
  total_frames: 1_000_000
  device: cpu
  env_per_collector: 1
  init_random_frames: 50_000
  multi_step: 1
  n_steps_return: 3
  max_frames_per_traj: -1
  exploration_mode: random

logger:
  record_video: 0
  record_interval: 10
  record_frames: 10000
  exp_name: cheetah
  backend: wandb
  kwargs:
    offline: False
  recorder_log_keys:

optim:
  optimizer: adam
  steps_per_batch: 1024
  lr: 3e-4
  init_random_frames: 25000
  lr_scheduler: ""
  value_network_update_interval: 200
  weight_decay: 0.0
  eps: 1e-4
  kwargs:
    betas: [0.0,0.9]
  clip_grad_norm: 100.0
  clip_norm:

buffer:
  batch_size: 256
  prb: 1
  sub_traj_len:
  size: 500_000
  scratch_dir:
  prefetch: 64

network:
  activation: elu
  tanh_loc: False
  default_policy_scale: 1.0
  actor_cells: 256
  actor_depth: 2
  qvalue_cells: 256
  qvalue_depth: 2
  scale_lb: 0.05

exploration:
  gSDE: False
  ou_exploration: 1
  annealing_frames: 1000000
  ou_sigma: 0.2
  ou_theta: 0.15
  noisy: False

loss:
  loss_function: smooth_l1
  type: double
  num_q_values: 10
  gamma: 0.99
  hard_update: False
  value_network_update_interval: 200
