# Environment
env:
  name: HalfCheetah-v3
  task: ""
  exp_name: "HalfCheetah-SAC"
  library: gym
  frame_skip: 1
  seed: 1

# Collection
collector:
  total_frames: 1000000
  init_random_frames: 10000
  frames_per_batch: 1000
  max_frames_per_traj: 1000
  init_env_steps: 1000
  async_collection: 1
  collector_device: cuda:1
  env_per_collector: 1
  num_workers: 8

# Replay Buffer
replay_buffer:
  size: 1000000
  prb: 0 # use prioritized experience replay

# Optimization
optimization:
  utd_ratio: 1.0
  gamma: 0.99
  loss_function: smooth_l1
  lr: 3e-4
  weight_decay: 2e-4
  lr_scheduler: ""
  batch_size: 256
  target_update_polyak: 0.995

# Algorithm
network:
  hidden_sizes: [256, 256]
  activation: relu
  default_policy_scale: 1.0
  scale_lb: 0.1
  device: "cuda:0"

# Logging
logger:
  backend: wandb
  mode: online
  eval_iter: 25000
