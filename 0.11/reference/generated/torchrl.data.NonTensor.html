


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>NonTensor &mdash; torchrl 0.11 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/pytorch.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-design.min.css" type="text/css" />
  <link rel="stylesheet" href="../../https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="OneHot" href="torchrl.data.OneHot.html" />
    <link rel="prev" title="MultiOneHot" href="torchrl.data.MultiOneHot.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/features">Features</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   
  <div>

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../../versions.html"><span style="font-size:110%">0.11 &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/export.html">Exporting TorchRL modules</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">

      <section data-toggle="wy-nav-shift" class="pytorch-content-wrap">
        <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
          <div class="pytorch-breadcrumbs-wrapper">
            















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../index.html">API Reference</a> &gt;</li>
        
          <li><a href="../data.html">torchrl.data package</a> &gt;</li>
        
          <li><a href="../data_specs.html">TensorSpec System</a> &gt;</li>
        
      <li>NonTensor</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../_sources/reference/generated/torchrl.data.NonTensor.rst.txt" rel="nofollow"><img src="../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
          </div>

          <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
            Shortcuts
          </div>
        </div>

        <div class="pytorch-content-left">
    
    
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" class="pytorch-article">
              
  <section id="nontensor">
<h1>NonTensor<a class="headerlink" href="#nontensor" title="Link to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="torchrl.data.NonTensor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrl.data.</span></span><span class="sig-name descname"><span class="pre">NonTensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.10)"><span class="pre">Size</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">(1,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.10)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.10)"><span class="pre">dtype</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batched</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_dims</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor" title="Link to this definition">¶</a></dt>
<dd><p>A spec for non-tensor data.</p>
<p>The <cite>NonTensor</cite> class is designed to handle specifications for data that do not conform to standard tensor
structures.
It maintains attributes such as shape, and device similar to the <cite>NonTensorData</cite> class.
The dtype is optional and should in practice be left to <cite>None</cite> in most cases.
Methods like <cite>rand</cite>, <cite>zero</cite>, and <cite>one</cite> will return a <cite>NonTensorData</cite> object with a <cite>None</cite> data value.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The default shape of <cite>NonTensor</cite> is <cite>(1,)</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.10)"><em>torch.Size</em></a><em>, </em><em>int</em><em>]</em><em>, </em><em>optional</em>) – The shape of the non-tensor data. Defaults to <cite>(1,)</cite>.</p></li>
<li><p><strong>device</strong> (<em>Optional</em><em>[</em><em>DEVICE_TYPING</em><em>]</em><em>, </em><em>optional</em>) – The device on which the data is stored. Defaults to <cite>None</cite>.</p></li>
<li><p><strong>dtype</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.10)"><em>torch.dtype</em></a><em> | </em><em>None</em><em>, </em><em>optional</em>) – The data type of the non-tensor data. Defaults to <cite>None</cite>.</p></li>
<li><p><strong>example_data</strong> (<em>Any</em><em>, </em><em>optional</em>) – An example of the data that this spec represents. This example is used as a
template when generating new data with the <cite>rand</cite>, <cite>zero</cite>, and <cite>one</cite> methods.</p></li>
<li><p><strong>batched</strong> (<em>bool</em><em>, </em><em>optional</em>) – Indicates whether the data is batched. If <cite>True</cite>, the <cite>rand</cite>, <cite>zero</cite>, and <cite>one</cite> methods
will generate data with an additional batch dimension, stacking copies of the <cite>example_data</cite> across this dimension.
Defaults to <cite>False</cite>.
Exclusive with <cite>feature_dims</cite>.</p></li>
<li><p><strong>feature_dims</strong> (<em>int</em><em>, </em><em>optional</em>) – The number of dimensions that are features.
The feature dimensions are the trailing dimensions that are not batch dimensions.
Every feature dimension is included in a single NonTensorData object, whereas these
are stacked across the batch dimension.
Exclusive with <cite>batched</cite>.
Defaults to <cite>None</cite> (all if batched=False, none if batched=True).</p></li>
<li><p><strong>**kwargs</strong> – Additional keyword arguments passed to the parent class.</p></li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">Choice</span></code> which allows to randomly choose among different specs when calling
<cite>rand</cite>.</p>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torchrl.data</span> <span class="kn">import</span> <span class="n">NonTensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spec</span> <span class="o">=</span> <span class="n">NonTensor</span><span class="p">(</span><span class="n">example_data</span><span class="o">=</span><span class="s2">&quot;a string&quot;</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spec</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
<span class="go">NonTensorData(data=a string, batch_size=torch.Size([3]), device=None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spec</span> <span class="o">=</span> <span class="n">NonTensor</span><span class="p">(</span><span class="n">example_data</span><span class="o">=</span><span class="s2">&quot;a string&quot;</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">spec</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
<span class="go">NonTensorStack(</span>
<span class="go">    [&#39;a string&#39;, &#39;a string&#39;, &#39;a string&#39;],</span>
<span class="go">    batch_size=torch.Size([3]),</span>
<span class="go">    device=None)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.assert_is_in">
<span class="sig-name descname"><span class="pre">assert_is_in</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.assert_is_in" title="Link to this definition">¶</a></dt>
<dd><p>Asserts whether a tensor belongs to the box, and raises an exception otherwise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>value</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – value to be checked.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.cardinality">
<span class="sig-name descname"><span class="pre">cardinality</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.cardinality"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.cardinality" title="Link to this definition">¶</a></dt>
<dd><p>The cardinality of the spec.</p>
<p>This refers to the number of possible outcomes in a spec. It is assumed that the cardinality of a composite
spec is the cartesian product of all possible outcomes.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.clear_device_">
<span class="sig-name descname"><span class="pre">clear_device_</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">T</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.clear_device_" title="Link to this definition">¶</a></dt>
<dd><p>A no-op for all leaf specs (which must have a device).</p>
<p>For <a class="reference internal" href="torchrl.data.Composite.html#torchrl.data.Composite" title="torchrl.data.Composite"><code class="xref py py-class docutils literal notranslate"><span class="pre">Composite</span></code></a> specs, this method will erase the device.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.clone">
<span class="sig-name descname"><span class="pre">clone</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrl.data.NonTensor" title="torchrl.data.tensor_specs.NonTensor"><span class="pre">NonTensor</span></a></span></span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.clone"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.clone" title="Link to this definition">¶</a></dt>
<dd><p>Creates a copy of the TensorSpec.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.contains">
<span class="sig-name descname"><span class="pre">contains</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.contains" title="Link to this definition">¶</a></dt>
<dd><p>If the value <code class="docutils literal notranslate"><span class="pre">val</span></code> could have been generated by the <code class="docutils literal notranslate"><span class="pre">TensorSpec</span></code>, returns <code class="docutils literal notranslate"><span class="pre">True</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>See <a class="reference internal" href="#torchrl.data.NonTensor.is_in" title="torchrl.data.NonTensor.is_in"><code class="xref py py-meth docutils literal notranslate"><span class="pre">is_in()</span></code></a> for more information.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#torchrl.data.NonTensor.cpu" title="Link to this definition">¶</a></dt>
<dd><p>Casts the TensorSpec to ‘cpu’ device.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#torchrl.data.NonTensor.cuda" title="Link to this definition">¶</a></dt>
<dd><p>Casts the TensorSpec to ‘cuda’ device.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.device">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">device</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.10)"><span class="pre">device</span></a></em><a class="headerlink" href="#torchrl.data.NonTensor.device" title="Link to this definition">¶</a></dt>
<dd><p>The device of the spec.</p>
<p>Only <a class="reference internal" href="torchrl.data.Composite.html#torchrl.data.Composite" title="torchrl.data.Composite"><code class="xref py py-class docutils literal notranslate"><span class="pre">Composite</span></code></a> specs can have a <code class="docutils literal notranslate"><span class="pre">None</span></code> device. All leaves must have a non-null device.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.encode">
<span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></span><a class="headerlink" href="#torchrl.data.NonTensor.encode" title="Link to this definition">¶</a></dt>
<dd><p>Encodes a value given the specified spec, and return the corresponding tensor.</p>
<p>This method is to be used in environments that return a value (eg, a numpy array) that can be
easily mapped to the TorchRL required domain.
If the value is already a tensor, the spec will not change its value and return it as-is.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>val</strong> (<em>np.ndarray</em><em> or </em><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – value to be encoded as tensor.</p>
</dd>
<dt class="field-even">Keyword Arguments<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ignore_device</strong> (<em>bool</em><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the spec device will
be ignored. This is used to group tensor casting within a call
to <code class="docutils literal notranslate"><span class="pre">TensorDict(...,</span> <span class="pre">device=&quot;cuda&quot;)</span></code> which is faster.</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor matching the required tensor specs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.enumerate">
<span class="sig-name descname"><span class="pre">enumerate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Any</span></span></span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.enumerate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.enumerate" title="Link to this definition">¶</a></dt>
<dd><p>Returns all the samples that can be obtained from the TensorSpec.</p>
<p>The samples will be stacked along the first dimension.</p>
<p>This method is only implemented for discrete specs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>use_mask</strong> (<em>bool</em><em>, </em><em>optional</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> and the spec has a mask,
samples that are masked are excluded. Default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.erase_memoize_cache">
<span class="sig-name descname"><span class="pre">erase_memoize_cache</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.erase_memoize_cache" title="Link to this definition">¶</a></dt>
<dd><p>Clears the memoized cache for cached encode execution.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="torchrl.data.TensorSpec.html#torchrl.data.TensorSpec.memoize_encode" title="torchrl.data.TensorSpec.memoize_encode"><code class="xref py py-meth docutils literal notranslate"><span class="pre">memoize_encode()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.expand">
<span class="sig-name descname"><span class="pre">expand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.expand"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.expand" title="Link to this definition">¶</a></dt>
<dd><p>Returns a new Spec with the expanded shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>*shape</strong> (<em>tuple</em><em> or </em><em>iterable</em><em> of </em><em>int</em>) – the new shape of the Spec.
Must be broadcastable with the current shape:
its length must be at least as long as the current shape length,
and its last values must be compliant too; ie they can only differ
from it if the current dimension is a singleton.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.flatten">
<span class="sig-name descname"><span class="pre">flatten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">T</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.flatten" title="Link to this definition">¶</a></dt>
<dd><p>Flattens a <code class="docutils literal notranslate"><span class="pre">TensorSpec</span></code>.</p>
<p>Check <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten" title="(in PyTorch v2.10)"><code class="xref py py-func docutils literal notranslate"><span class="pre">flatten()</span></code></a> for more information on this method.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.implements_for_spec">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">implements_for_spec</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">torch_function</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Callable</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.implements_for_spec" title="Link to this definition">¶</a></dt>
<dd><p>Register a torch function override for TensorSpec.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.index">
<span class="sig-name descname"><span class="pre">index</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">slice</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor_to_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.index" title="Link to this definition">¶</a></dt>
<dd><p>Indexes the input tensor.</p>
<p>This method is to be used with specs that encode one or more categorical variables (e.g.,
<a class="reference internal" href="torchrl.data.OneHot.html#torchrl.data.OneHot" title="torchrl.data.OneHot"><code class="xref py py-class docutils literal notranslate"><span class="pre">OneHot</span></code></a> or <a class="reference internal" href="torchrl.data.Categorical.html#torchrl.data.Categorical" title="torchrl.data.Categorical"><code class="xref py py-class docutils literal notranslate"><span class="pre">Categorical</span></code></a>), such that indexing of a tensor
with a sample can be done without caring about the actual representation of the index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<em>int</em><em>, </em><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a><em>, </em><em>slice</em><em> or </em><em>list</em>) – index of the tensor</p></li>
<li><p><strong>tensor_to_index</strong> – tensor to be indexed</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>indexed tensor</p>
</dd>
</dl>
<dl>
<dt>Exanples:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torchrl.data</span> <span class="kn">import</span> <span class="n">OneHot</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">one_hot</span> <span class="o">=</span> <span class="n">OneHot</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">categ</span> <span class="o">=</span> <span class="n">one_hot</span><span class="o">.</span><span class="n">to_categorical_spec</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">idx_one_hot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">100</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">idx_one_hot</span><span class="p">[</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">one_hot</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">idx_one_hot</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)))</span>
<span class="go">tensor(50)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">idx_categ</span> <span class="o">=</span> <span class="n">one_hot</span><span class="o">.</span><span class="n">to_categorical</span><span class="p">(</span><span class="n">idx_one_hot</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">categ</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">idx_categ</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)))</span>
<span class="go">tensor(50)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.is_in">
<span class="sig-name descname"><span class="pre">is_in</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.is_in"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.is_in" title="Link to this definition">¶</a></dt>
<dd><p>If the value <code class="docutils literal notranslate"><span class="pre">val</span></code> could have been generated by the <code class="docutils literal notranslate"><span class="pre">TensorSpec</span></code>, returns <code class="docutils literal notranslate"><span class="pre">True</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>More precisely, the <code class="docutils literal notranslate"><span class="pre">is_in</span></code> methods checks that the value <code class="docutils literal notranslate"><span class="pre">val</span></code> is within the limits defined by the <code class="docutils literal notranslate"><span class="pre">space</span></code>
attribute (the box), and that the <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, <code class="docutils literal notranslate"><span class="pre">device</span></code>, <code class="docutils literal notranslate"><span class="pre">shape</span></code> potentially other metadata match those
of the spec. If any of these checks fails, the <code class="docutils literal notranslate"><span class="pre">is_in</span></code> method will return <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>val</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – value to be checked.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>boolean indicating if values belongs to the TensorSpec box.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.make_neg_dim">
<span class="sig-name descname"><span class="pre">make_neg_dim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">T</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.make_neg_dim" title="Link to this definition">¶</a></dt>
<dd><p>Converts a specific dimension to <code class="docutils literal notranslate"><span class="pre">-1</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.memoize_encode">
<span class="sig-name descname"><span class="pre">memoize_encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.memoize_encode" title="Link to this definition">¶</a></dt>
<dd><p>Creates a cached sequence of callables for the <cite>encode</cite> method that speeds up its execution.</p>
<p>This should only be used whenever the input type, shape etc. are expected to be consistent across calls
for a given spec.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>mode</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether the cache should be used. Defaults to <cite>True</cite>.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>the cache can be erased via <a class="reference internal" href="torchrl.data.TensorSpec.html#torchrl.data.TensorSpec.erase_memoize_cache" title="torchrl.data.TensorSpec.erase_memoize_cache"><code class="xref py py-meth docutils literal notranslate"><span class="pre">erase_memoize_cache()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.ndim">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ndim</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#torchrl.data.NonTensor.ndim" title="Link to this definition">¶</a></dt>
<dd><p>Number of dimensions of the spec shape.</p>
<p>Shortcut for <code class="docutils literal notranslate"><span class="pre">len(spec.shape)</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.ndimension">
<span class="sig-name descname"><span class="pre">ndimension</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.ndimension" title="Link to this definition">¶</a></dt>
<dd><p>Number of dimensions of the spec shape.</p>
<p>Shortcut for <code class="docutils literal notranslate"><span class="pre">len(spec.shape)</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.one">
<span class="sig-name descname"><span class="pre">one</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.one"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.one" title="Link to this definition">¶</a></dt>
<dd><p>Returns a one-filled tensor in the box.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Even though there is no guarantee that <code class="docutils literal notranslate"><span class="pre">1</span></code> belongs to the spec domain,
this method will not raise an exception when this condition is violated.
The primary use case of <code class="docutils literal notranslate"><span class="pre">one</span></code> is to generate empty data buffers, not meaningful data.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>shape</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.10)"><em>torch.Size</em></a>) – shape of the one-tensor</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a one-filled tensor sampled in the TensorSpec box.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.ones">
<span class="sig-name descname"><span class="pre">ones</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.10)"><span class="pre">Size</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></span><a class="headerlink" href="#torchrl.data.NonTensor.ones" title="Link to this definition">¶</a></dt>
<dd><p>Proxy to <a class="reference internal" href="#torchrl.data.NonTensor.one" title="torchrl.data.NonTensor.one"><code class="xref py py-meth docutils literal notranslate"><span class="pre">one()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.project">
<span class="sig-name descname"><span class="pre">project</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></span><a class="headerlink" href="#torchrl.data.NonTensor.project" title="Link to this definition">¶</a></dt>
<dd><p>If the input tensor is not in the TensorSpec box, it maps it back to it given some defined heuristic.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>val</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – tensor to be mapped to the box.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a torch.Tensor belonging to the TensorSpec box.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.rand">
<span class="sig-name descname"><span class="pre">rand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.rand"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.rand" title="Link to this definition">¶</a></dt>
<dd><p>Returns a random tensor in the space defined by the spec.</p>
<p>The sampling will be done uniformly over the space, unless the box is unbounded in which case normal values
will be drawn.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>shape</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.10)"><em>torch.Size</em></a>) – shape of the random tensor</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a random tensor sampled in the TensorSpec box.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.reshape">
<span class="sig-name descname"><span class="pre">reshape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">T</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.reshape" title="Link to this definition">¶</a></dt>
<dd><p>Reshapes a <code class="docutils literal notranslate"><span class="pre">TensorSpec</span></code>.</p>
<p>Check <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape" title="(in PyTorch v2.10)"><code class="xref py py-func docutils literal notranslate"><span class="pre">reshape()</span></code></a> for more information on this method.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.sample">
<span class="sig-name descname"><span class="pre">sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.10)"><span class="pre">Size</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></span><a class="headerlink" href="#torchrl.data.NonTensor.sample" title="Link to this definition">¶</a></dt>
<dd><p>Returns a random tensor in the space defined by the spec.</p>
<p>See <a class="reference internal" href="#torchrl.data.NonTensor.rand" title="torchrl.data.NonTensor.rand"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rand()</span></code></a> for details.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.squeeze">
<span class="sig-name descname"><span class="pre">squeeze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrl.data.NonTensor" title="torchrl.data.tensor_specs.NonTensor"><span class="pre">NonTensor</span></a></span></span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.squeeze"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.squeeze" title="Link to this definition">¶</a></dt>
<dd><p>Returns a new Spec with all the dimensions of size <code class="docutils literal notranslate"><span class="pre">1</span></code> removed.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">dim</span></code> is given, a squeeze operation is done only in that dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dim</strong> (<em>int</em><em> or </em><em>None</em>) – the dimension to apply the squeeze operation to</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dest</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.10)"><span class="pre">dtype</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.10)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrl.data.NonTensor" title="torchrl.data.tensor_specs.NonTensor"><span class="pre">NonTensor</span></a></span></span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.to"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.to" title="Link to this definition">¶</a></dt>
<dd><p>Casts a TensorSpec to a device or a dtype.</p>
<p>Returns the same spec if no change is made.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.to_numpy">
<span class="sig-name descname"><span class="pre">to_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">safe</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ndarray</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dict</span></span></span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.to_numpy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.to_numpy" title="Link to this definition">¶</a></dt>
<dd><p>Returns the <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> correspondent of an input tensor.</p>
<p>This is intended to be the inverse operation of <a class="reference internal" href="#torchrl.data.NonTensor.encode" title="torchrl.data.NonTensor.encode"><code class="xref py py-meth docutils literal notranslate"><span class="pre">encode()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>val</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – tensor to be transformed_in to numpy.</p></li>
<li><p><strong>safe</strong> (<em>bool</em>) – boolean value indicating whether a check should be
performed on the value against the domain of the spec.
Defaults to the value of the <code class="docutils literal notranslate"><span class="pre">CHECK_SPEC_ENCODE</span></code> environment variable.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a np.ndarray.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.type_check">
<span class="sig-name descname"><span class="pre">type_check</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">NestedKey</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.type_check" title="Link to this definition">¶</a></dt>
<dd><p>Checks the input value <code class="docutils literal notranslate"><span class="pre">dtype</span></code> against the <code class="docutils literal notranslate"><span class="pre">TensorSpec</span></code> <code class="docutils literal notranslate"><span class="pre">dtype</span></code> and raises an exception if they don’t match.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><em>torch.Tensor</em></a>) – tensor whose dtype has to be checked.</p></li>
<li><p><strong>key</strong> (<em>str</em><em>, </em><em>optional</em>) – if the TensorSpec has keys, the value
dtype will be checked against the spec pointed by the
indicated key.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.unflatten">
<span class="sig-name descname"><span class="pre">unflatten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sizes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">T</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.unflatten" title="Link to this definition">¶</a></dt>
<dd><p>Unflattens a <code class="docutils literal notranslate"><span class="pre">TensorSpec</span></code>.</p>
<p>Check <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.unflatten.html#torch.unflatten" title="(in PyTorch v2.10)"><code class="xref py py-func docutils literal notranslate"><span class="pre">unflatten()</span></code></a> for more information on this method.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.unsqueeze">
<span class="sig-name descname"><span class="pre">unsqueeze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrl.data.NonTensor" title="torchrl.data.tensor_specs.NonTensor"><span class="pre">NonTensor</span></a></span></span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.unsqueeze"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.unsqueeze" title="Link to this definition">¶</a></dt>
<dd><p>Returns a new Spec with one more singleton dimension (at the position indicated by <code class="docutils literal notranslate"><span class="pre">dim</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dim</strong> (<em>int</em><em> or </em><em>None</em>) – the dimension to apply the unsqueeze operation to.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.view">
<span class="sig-name descname"><span class="pre">view</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">T</span></span></span><a class="headerlink" href="#torchrl.data.NonTensor.view" title="Link to this definition">¶</a></dt>
<dd><p>Reshapes a <code class="docutils literal notranslate"><span class="pre">TensorSpec</span></code>.</p>
<p>Check <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape" title="(in PyTorch v2.10)"><code class="xref py py-func docutils literal notranslate"><span class="pre">reshape()</span></code></a> for more information on this method.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.zero">
<span class="sig-name descname"><span class="pre">zero</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/torchrl/data/tensor_specs.html#NonTensor.zero"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.NonTensor.zero" title="Link to this definition">¶</a></dt>
<dd><p>Returns a zero-filled tensor in the box.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Even though there is no guarantee that <code class="docutils literal notranslate"><span class="pre">0</span></code> belongs to the spec domain,
this method will not raise an exception when this condition is violated.
The primary use case of <code class="docutils literal notranslate"><span class="pre">zero</span></code> is to generate empty data buffers, not meaningful data.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>shape</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.10)"><em>torch.Size</em></a>) – shape of the zero-tensor</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a zero-filled tensor sampled in the TensorSpec box.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.data.NonTensor.zeros">
<span class="sig-name descname"><span class="pre">zeros</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.10)"><span class="pre">Size</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.10)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase" title="(in tensordict v0.11)"><span class="pre">TensorDictBase</span></a></span></span><a class="headerlink" href="#torchrl.data.NonTensor.zeros" title="Link to this definition">¶</a></dt>
<dd><p>Proxy to <a class="reference internal" href="#torchrl.data.NonTensor.zero" title="torchrl.data.NonTensor.zero"><code class="xref py py-meth docutils literal notranslate"><span class="pre">zero()</span></code></a>.</p>
</dd></dl>

</dd></dl>

</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="torchrl.data.OneHot.html" class="btn btn-neutral float-right" title="OneHot" accesskey="n" rel="next">Next <img src="../../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="torchrl.data.MultiOneHot.html" class="btn btn-neutral" title="MultiOneHot" accesskey="p" rel="prev"><img src="../../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">NonTensor</a><ul>
<li><a class="reference internal" href="#torchrl.data.NonTensor"><code class="docutils literal notranslate"><span class="pre">NonTensor</span></code></a><ul>
<li><a class="reference internal" href="#torchrl.data.NonTensor.assert_is_in"><code class="docutils literal notranslate"><span class="pre">NonTensor.assert_is_in()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.cardinality"><code class="docutils literal notranslate"><span class="pre">NonTensor.cardinality()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.clear_device_"><code class="docutils literal notranslate"><span class="pre">NonTensor.clear_device_()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.clone"><code class="docutils literal notranslate"><span class="pre">NonTensor.clone()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.contains"><code class="docutils literal notranslate"><span class="pre">NonTensor.contains()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.cpu"><code class="docutils literal notranslate"><span class="pre">NonTensor.cpu()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.cuda"><code class="docutils literal notranslate"><span class="pre">NonTensor.cuda()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.device"><code class="docutils literal notranslate"><span class="pre">NonTensor.device</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.encode"><code class="docutils literal notranslate"><span class="pre">NonTensor.encode()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.enumerate"><code class="docutils literal notranslate"><span class="pre">NonTensor.enumerate()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.erase_memoize_cache"><code class="docutils literal notranslate"><span class="pre">NonTensor.erase_memoize_cache()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.expand"><code class="docutils literal notranslate"><span class="pre">NonTensor.expand()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.flatten"><code class="docutils literal notranslate"><span class="pre">NonTensor.flatten()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.implements_for_spec"><code class="docutils literal notranslate"><span class="pre">NonTensor.implements_for_spec()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.index"><code class="docutils literal notranslate"><span class="pre">NonTensor.index()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.is_in"><code class="docutils literal notranslate"><span class="pre">NonTensor.is_in()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.make_neg_dim"><code class="docutils literal notranslate"><span class="pre">NonTensor.make_neg_dim()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.memoize_encode"><code class="docutils literal notranslate"><span class="pre">NonTensor.memoize_encode()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.ndim"><code class="docutils literal notranslate"><span class="pre">NonTensor.ndim</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.ndimension"><code class="docutils literal notranslate"><span class="pre">NonTensor.ndimension()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.one"><code class="docutils literal notranslate"><span class="pre">NonTensor.one()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.ones"><code class="docutils literal notranslate"><span class="pre">NonTensor.ones()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.project"><code class="docutils literal notranslate"><span class="pre">NonTensor.project()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.rand"><code class="docutils literal notranslate"><span class="pre">NonTensor.rand()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.reshape"><code class="docutils literal notranslate"><span class="pre">NonTensor.reshape()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.sample"><code class="docutils literal notranslate"><span class="pre">NonTensor.sample()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.squeeze"><code class="docutils literal notranslate"><span class="pre">NonTensor.squeeze()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.to"><code class="docutils literal notranslate"><span class="pre">NonTensor.to()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.to_numpy"><code class="docutils literal notranslate"><span class="pre">NonTensor.to_numpy()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.type_check"><code class="docutils literal notranslate"><span class="pre">NonTensor.type_check()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.unflatten"><code class="docutils literal notranslate"><span class="pre">NonTensor.unflatten()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.unsqueeze"><code class="docutils literal notranslate"><span class="pre">NonTensor.unsqueeze()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.view"><code class="docutils literal notranslate"><span class="pre">NonTensor.view()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.zero"><code class="docutils literal notranslate"><span class="pre">NonTensor.zero()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.data.NonTensor.zeros"><code class="docutils literal notranslate"><span class="pre">NonTensor.zeros()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>
  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.11',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../_static/design-tabs.js"></script>

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://shiftlab.github.io/pytorch/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://shiftlab.github.io/pytorch/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/">PyTorch</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/features">Features</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/blog/">Blog</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    mobileMenu.bind();
    mobileTOC.bind();
    pytorchAnchors.bind();

    $(window).on("load", function() {
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
    })

    // Add class to links that have code blocks, since we cannot create links in code blocks
    $("article.pytorch-article a span.pre").each(function(e) {
      $(this).closest("a").addClass("has-code");
    });
  </script>
</body>
</html>