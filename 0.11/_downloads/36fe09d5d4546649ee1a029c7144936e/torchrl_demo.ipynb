{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Introduction to TorchRL\n\nGet started with reinforcement learning in PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TorchRL is an open-source Reinforcement Learning (RL) library for PyTorch.\nThis tutorial provides a hands-on introduction to its main components.\n\n**Key features:**\n\n- **PyTorch-native**: Seamless integration with PyTorch's ecosystem\n- **Modular**: Easily swap components and build custom pipelines\n- **Efficient**: Optimized for both research and production\n- **Comprehensive**: Environments, modules, losses, collectors, and more\n\nBy the end of this tutorial, you'll understand how TorchRL's components\nwork together to build RL training pipelines. Let's start with a quick\nexample to see what's possible:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Start\n\nBefore diving into the details, here's a taste of what TorchRL can do.\nIn just a few lines, we can create an environment, build a policy, and\ncollect a trajectory:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torchrl.envs import GymEnv\nfrom torchrl.modules import MLP, QValueActor\n\nenv = GymEnv(\"CartPole-v1\")\nactor = QValueActor(\n    MLP(\n        in_features=env.observation_spec[\"observation\"].shape[-1],\n        out_features=2,\n        num_cells=[64, 64],\n    ),\n    in_keys=[\"observation\"],\n    spec=env.action_spec,\n)\nrollout = env.rollout(max_steps=200, policy=actor)\nprint(\n    f\"Collected {rollout.shape[0]} steps, total reward: {rollout['next', 'reward'].sum().item():.0f}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's it! We wrapped a Gym environment, created a Q-value actor with an\nMLP backbone, and used :meth:`~torchrl.envs.EnvBase.rollout` to collect\na full trajectory. The result is a :class:`~tensordict.TensorDict`\ncontaining observations, actions, rewards, and more.\n\nNow let's understand each component in detail.\n\n## TensorDict: The Data Backbone\n\nAt the heart of TorchRL is :class:`~tensordict.TensorDict` - a dictionary-like\ncontainer that holds tensors and supports batched operations. Think of it as\na \"tensor of dictionaries\" or a \"dictionary of tensors\" that knows about its\nbatch dimensions.\n\nWhy TensorDict? In RL, we constantly pass around groups of related tensors:\nobservations, actions, rewards, done flags, next observations, etc. TensorDict\nkeeps these organized and lets us manipulate them as a unit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict import TensorDict\n\n# Create a TensorDict representing a batch of 4 transitions\nbatch_size = 4\ndata = TensorDict(\n    obs=torch.randn(batch_size, 3),\n    action=torch.randn(batch_size, 2),\n    reward=torch.randn(batch_size, 1),\n    batch_size=[batch_size],\n)\nprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TensorDicts support all the operations you'd expect from PyTorch tensors.\nYou can index them, slice them, move them between devices, and stack them\ntogether - all while keeping the dictionary structure intact:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Indexing works just like tensors - grab the first transition\nprint(\"First element:\", data[0])\nprint(\"Slice:\", data[:2])\n\n# Device transfer moves all contained tensors\ndata_cpu = data.to(\"cpu\")\n\n# Stacking is especially useful for building trajectories\ndata2 = data.clone()\nstacked = torch.stack([data, data2], dim=0)\nprint(\"Stacked shape:\", stacked.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TensorDicts can also be nested, which is useful for organizing complex\nobservations (e.g., an agent that receives both image pixels and vector\nstate) or for separating \"current\" from \"next\" step data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nested = TensorDict(\n    observation=TensorDict(\n        pixels=torch.randn(4, 3, 84, 84),\n        vector=torch.randn(4, 10),\n        batch_size=[4],\n    ),\n    action=torch.randn(4, 2),\n    batch_size=[4],\n)\nprint(nested)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environments\n\nTorchRL provides a unified interface for RL environments. Whether you're\nusing Gym, DMControl, IsaacGym, or other simulators, the API stays the same:\nenvironments accept and return TensorDicts.\n\n**Creating Environments**\n\nThe simplest way to create an environment is with :class:`~torchrl.envs.GymEnv`,\nwhich wraps any Gymnasium (or legacy Gym) environment:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import GymEnv\n\nenv = GymEnv(\"Pendulum-v1\")\nprint(\"Action spec:\", env.action_spec)\nprint(\"Observation spec:\", env.observation_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Every environment has *specs* that describe the shape and bounds of\nobservations, actions, rewards, and done flags. These specs are essential\nfor building correctly-shaped networks and for validating data.\n\nThe environment interaction follows a familiar pattern - reset, then step:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "td = env.reset()\nprint(\"Reset output:\", td)\n\n# Sample a random action and take a step\ntd[\"action\"] = env.action_spec.rand()\ntd = env.step(td)\nprint(\"Step output:\", td)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that :meth:`~torchrl.envs.EnvBase.step` returns the same TensorDict\nwith additional keys filled in: the ``\"next\"`` sub-TensorDict contains the\nresulting observation, reward, and done flag.\n\n**Transforms**\n\nJust like torchvision transforms for images, TorchRL provides transforms\nfor environments. These modify observations, actions, or rewards in a\ncomposable way. Common uses include normalizing observations, stacking\nframes, or adding step counters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import Compose, StepCounter, TransformedEnv\n\nenv = TransformedEnv(\n    GymEnv(\"Pendulum-v1\"),\n    Compose(\n        StepCounter(max_steps=200),  # Track steps and auto-terminate\n    ),\n)\nprint(\"Transformed env:\", env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Batched Environments**\n\nRL algorithms are data-hungry. Running multiple environment instances in\nparallel can dramatically speed up data collection. TorchRL's\n:class:`~torchrl.envs.ParallelEnv` runs environments in separate processes,\nreturning batched TensorDicts:\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>``ParallelEnv`` uses multiprocessing. The ``mp_start_method`` parameter\n   controls how processes are spawned: ``\"fork\"`` (Linux default) is fast but\n   can have issues with some libraries; ``\"spawn\"`` (Windows/macOS default)\n   is safer but requires code to be guarded with ``if __name__ == \"__main__\"``.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import ParallelEnv\n\n\ndef make_env():\n    return GymEnv(\"Pendulum-v1\")\n\n\n# Run 4 environments in parallel\nvec_env = ParallelEnv(4, make_env)\ntd = vec_env.reset()\nprint(\"Batched reset:\", td.batch_size)\n\ntd[\"action\"] = vec_env.action_spec.rand()\ntd = vec_env.step(td)\nprint(\"Batched step:\", td.batch_size)\n\nvec_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The batch dimension (4 in this case) propagates through all tensors,\nmaking it easy to process multiple environments with a single forward pass.\n\n## Modules and Policies\n\nTorchRL extends PyTorch's ``nn.Module`` system with modules that read from\nand write to TensorDicts. This makes it easy to build policies that\nintegrate seamlessly with the environment interface.\n\n**TensorDictModule**\n\nThe core building block is :class:`~tensordict.nn.TensorDictModule`. It wraps\nany ``nn.Module`` and specifies which TensorDict keys to read as inputs and\nwhich keys to write as outputs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import TensorDictModule\nfrom torch import nn\n\nmodule = nn.Linear(3, 2)\ntd_module = TensorDictModule(module, in_keys=[\"observation\"], out_keys=[\"action\"])\n\n# The module reads \"observation\" and writes \"action\"\ntd = TensorDict(observation=torch.randn(4, 3), batch_size=[4])\ntd_module(td)\nprint(td)  # Now has \"action\" key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This pattern has a powerful benefit: modules become composable. You can\nchain them together, and each module only needs to know about its own\ninput/output keys.\n\n**Built-in Networks**\n\nTorchRL includes common network architectures used in RL. These are\nregular PyTorch modules that you can wrap with TensorDictModule:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.modules import ConvNet, MLP\n\n# MLP for vector observations - specify input/output dims and hidden layers\nmlp = MLP(in_features=64, out_features=10, num_cells=[128, 128])\nprint(mlp(torch.randn(4, 64)).shape)\n\n# ConvNet for image observations - outputs a flat feature vector\ncnn = ConvNet(num_cells=[32, 64], kernel_sizes=[8, 4], strides=[4, 2])\nprint(cnn(torch.randn(4, 3, 84, 84)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Probabilistic Policies**\n\nMany RL algorithms (PPO, SAC, etc.) use stochastic policies that output\nprobability distributions over actions. TorchRL provides\n:class:`~tensordict.nn.ProbabilisticTensorDictModule` to sample from\ndistributions and optionally compute log-probabilities:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import (\n    ProbabilisticTensorDictModule,\n    ProbabilisticTensorDictSequential,\n)\nfrom torchrl.modules import NormalParamExtractor, TanhNormal\n\n# The network outputs mean and std (via NormalParamExtractor)\nnet = nn.Sequential(\n    nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 4), NormalParamExtractor()\n)\nbackbone = TensorDictModule(net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n\n# Combine backbone with a distribution sampler\npolicy = ProbabilisticTensorDictSequential(\n    backbone,\n    ProbabilisticTensorDictModule(\n        in_keys=[\"loc\", \"scale\"],\n        out_keys=[\"action\"],\n        distribution_class=TanhNormal,\n        return_log_prob=True,\n    ),\n)\n\ntd = TensorDict(observation=torch.randn(4, 3), batch_size=[4])\npolicy(td)\nprint(\"Sampled action:\", td[\"action\"].shape)\nprint(\"Log prob:\", td[\"action_log_prob\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``TanhNormal`` distribution squashes samples to [-1, 1], which is useful\nfor continuous control. The log-probability accounts for this transformation,\nwhich is crucial for policy gradient methods.\n\n## Data Collection\n\nIn RL, we need to repeatedly collect experience from the environment.\nWhile you can write your own rollout loop, TorchRL's *collectors* handle\nthis efficiently, including batching, device management, and multi-process\ncollection.\n\nThe :class:`~torchrl.collectors.SyncDataCollector` collects data\nsynchronously - it waits for a batch to be ready before returning:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.collectors import SyncDataCollector\n\n# A simple deterministic policy for demonstration\nactor = TensorDictModule(nn.Linear(3, 1), in_keys=[\"observation\"], out_keys=[\"action\"])\n\ncollector = SyncDataCollector(\n    create_env_fn=lambda: GymEnv(\"Pendulum-v1\"),\n    policy=actor,\n    frames_per_batch=200,  # Collect 200 frames per iteration\n    total_frames=1000,  # Stop after 1000 total frames\n)\n\nfor batch in collector:\n    print(\n        f\"Collected batch: {batch.shape}, reward: {batch['next', 'reward'].mean():.2f}\"\n    )\n\ncollector.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For async collection (useful when training takes longer than collecting),\nsee :class:`~torchrl.collectors.MultiaSyncDataCollector`.\n\n## Replay Buffers\n\nMost RL algorithms don't learn from experience immediately - they store\ntransitions in a buffer and sample mini-batches for training. TorchRL's\nreplay buffers handle this efficiently:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.data import LazyTensorStorage, ReplayBuffer\n\nbuffer = ReplayBuffer(storage=LazyTensorStorage(max_size=10000))\n\n# Add a batch of experience\nbuffer.extend(\n    TensorDict(obs=torch.randn(100, 4), action=torch.randn(100, 2), batch_size=[100])\n)\n\n# Sample a mini-batch for training\nsample = buffer.sample(32)\nprint(\"Sampled batch:\", sample.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`~torchrl.data.LazyTensorStorage` allocates memory lazily based\non the first batch added. For prioritized experience replay (used in DQN\nvariants), use :class:`~torchrl.data.PrioritizedReplayBuffer`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.data import PrioritizedReplayBuffer\n\nbuffer = PrioritizedReplayBuffer(\n    alpha=0.6,  # Priority exponent\n    beta=0.4,  # Importance sampling exponent\n    storage=LazyTensorStorage(max_size=10000),\n)\nbuffer.extend(TensorDict(obs=torch.randn(100, 4), batch_size=[100]))\n\n# Use return_info=True to get sampling metadata (indices, weights)\nsample, info = buffer.sample(32, return_info=True)\nprint(\"Prioritized sample indices:\", info[\"index\"][:5], \"...\")  # First 5 indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Functions\n\nThe final piece is the objective function. TorchRL provides loss classes\nfor major RL algorithms, encapsulating the often-complex loss computations:\n\n- :class:`~torchrl.objectives.DQNLoss` - Deep Q-Networks\n- :class:`~torchrl.objectives.DDPGLoss` - Deep Deterministic Policy Gradient\n- :class:`~torchrl.objectives.SACLoss` - Soft Actor-Critic\n- :class:`~torchrl.objectives.PPOLoss` - Proximal Policy Optimization\n- :class:`~torchrl.objectives.TD3Loss` - Twin Delayed DDPG\n\nHere's how to set up a DQN loss. We create a Q-network wrapped in a\n:class:`~torchrl.modules.QValueActor`, which handles action selection:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.objectives import DQNLoss\n\nqnet = TensorDictModule(\n    nn.Sequential(nn.Linear(4, 64), nn.ReLU(), nn.Linear(64, 2)),\n    in_keys=[\"observation\"],\n    out_keys=[\"action_value\"],\n)\n\n# QValueActor wraps the Q-network to select actions and output chosen values\nfrom torchrl.data import Categorical\n\nactor = QValueActor(qnet, in_keys=[\"observation\"], spec=Categorical(n=2))\nloss_fn = DQNLoss(actor, action_space=\"categorical\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss function expects batches with specific keys. Let's create a\ndummy batch to see it in action:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = TensorDict(\n    observation=torch.randn(32, 4),\n    action=torch.randint(0, 2, (32,)),\n    next=TensorDict(\n        observation=torch.randn(32, 4),\n        reward=torch.randn(32, 1),\n        done=torch.zeros(32, 1, dtype=torch.bool),\n        terminated=torch.zeros(32, 1, dtype=torch.bool),\n        batch_size=[32],\n    ),\n    batch_size=[32],\n)\n\nloss_td = loss_fn(batch)\nprint(\"Loss:\", loss_td[\"loss\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss function handles target network updates, Bellman backup\ncomputation, and all the bookkeeping needed for stable training.\n\n## Putting It All Together\n\nNow let's see how all these components work together in a complete\ntraining loop. We'll train a simple DQN agent on CartPole:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n\n# 1. Create the environment\nenv = GymEnv(\"CartPole-v1\")\n\n# 2. Build a Q-network and wrap it as a policy\nqnet = TensorDictModule(\n    nn.Sequential(nn.Linear(4, 128), nn.ReLU(), nn.Linear(128, 2)),\n    in_keys=[\"observation\"],\n    out_keys=[\"action_value\"],\n)\npolicy = QValueActor(qnet, in_keys=[\"observation\"], spec=env.action_spec)\n\n# 3. Set up the data collector\ncollector = SyncDataCollector(\n    create_env_fn=lambda: GymEnv(\"CartPole-v1\"),\n    policy=policy,\n    frames_per_batch=100,\n    total_frames=2000,\n)\n\n# 4. Create a replay buffer\nbuffer = ReplayBuffer(storage=LazyTensorStorage(max_size=10000))\n\n# 5. Set up the loss and optimizer (pass the QValueActor, not just the network)\nloss_fn = DQNLoss(policy, action_space=env.action_spec)\noptimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)\n\n# 6. Training loop: collect -> store -> sample -> train\nfor i, batch in enumerate(collector):\n    # Store collected experience\n    buffer.extend(batch)\n\n    # Wait until we have enough data\n    if len(buffer) < 100:\n        continue\n\n    # Sample a batch and compute the loss\n    sample = buffer.sample(64)\n    loss = loss_fn(sample)\n\n    # Standard PyTorch optimization step\n    optimizer.zero_grad()\n    loss[\"loss\"].backward()\n    optimizer.step()\n\n    if i % 5 == 0:\n        print(f\"Step {i}: loss={loss['loss'].item():.3f}\")\n\ncollector.shutdown()\nenv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a minimal example - a production DQN would include target network\nupdates, epsilon-greedy exploration, and more. Check out the full\nimplementations in ``sota-implementations/dqn/``.\n\n## What's Next?\n\nThis tutorial covered the basics. TorchRL has much more to offer:\n\n**Tutorials:**\n\n- [PPO Tutorial](../tutorials/coding_ppo.html) - Train PPO on MuJoCo\n- [DQN Tutorial](../tutorials/coding_dqn.html) - Deep Q-Learning from scratch\n- [Multi-Agent RL](../tutorials/multiagent_ppo.html) - Cooperative and competitive agents\n\n**SOTA Implementations:**\n\nThe [sota-implementations/](https://github.com/pytorch/rl/tree/main/sota-implementations)\nfolder contains production-ready implementations of:\n\n- PPO, A2C, SAC, TD3, DDPG, DQN\n- Offline RL: CQL, IQL, Decision Transformer\n- Multi-agent: IPPO, QMIX, MADDPG\n- LLM training: GRPO, Expert Iteration\n\n**Advanced Features:**\n\n- Distributed training with Ray and RPC\n- Offline RL datasets (D4RL, Minari)\n- Model-based RL (Dreamer)\n- LLM integration for RLHF\n\n**Resources:**\n\n- [API Reference](https://pytorch.org/rl/reference/index.html)\n- [GitHub](https://github.com/pytorch/rl)\n- [Contributing Guide](https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md)\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}