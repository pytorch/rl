version: 2.1

executors:
  windows-cpu:
    machine:
      resource_class: windows.xlarge
      image: windows-server-2019-vs2019:stable
      shell: bash.exe

  windows-gpu:
    machine:
      resource_class: windows.gpu.nvidia.medium
      image: windows-server-2019-nvidia:stable
      shell: bash.exe


commands:
  checkout_merge:
    description: "checkout merge branch"
    steps:
      - checkout
  designate_upload_channel:
    description: "inserts the correct upload channel into ${BASH_ENV}"
    steps:
      - run:
          name: adding UPLOAD_CHANNEL to BASH_ENV
          command: |
            our_upload_channel=nightly
            # On tags upload to test instead
            if [[ -n "${CIRCLE_TAG}" ]] || [[ ${CIRCLE_BRANCH} =~ release/* ]]; then
              our_upload_channel=test
            fi
            echo "export UPLOAD_CHANNEL=${our_upload_channel}" >> ${BASH_ENV}
  apt_install:
    parameters:
      args:
        type: string
      descr:
        type: string
        default: ""
      update:
        type: boolean
        default: true
    steps:
      - run:
          name: >
            <<^ parameters.descr >> apt install << parameters.args >> <</ parameters.descr >>
            <<# parameters.descr >> << parameters.descr >>            <</ parameters.descr >>
          command: |
            <<# parameters.update >> sudo apt update -qy  <</ parameters.update >>
            sudo apt install << parameters.args >>
  pip_install:
    parameters:
      args:
        type: string
      descr:
        type: string
        default: ""
      user:
        type: boolean
        default: true
    steps:
      - run:
          name: >
            <<^ parameters.descr >> pip install << parameters.args >> <</ parameters.descr >>
            <<# parameters.descr >> << parameters.descr >>            <</ parameters.descr >>
          command: >
            pip install
            <<# parameters.user >> --user <</ parameters.user >>
            --progress-bar=off
            << parameters.args >>

  install_torchrl:
    parameters:
      editable:
        type: boolean
        default: true
    steps:
      - pip_install:
          args: --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html
          descr: Install PyTorch from nightly releases
      - pip_install:
          args: --no-build-isolation <<# parameters.editable >> --editable <</ parameters.editable >> .
          descr: Install torchrl <<# parameters.editable >> in editable mode <</ parameters.editable >>


binary_common: &binary_common
  parameters:
    # Edit these defaults to do a release
    build_version:
      description: "version number of release binary; by default, build a nightly"
      type: string
      default: ""
    pytorch_version:
      description: "PyTorch version to build against; by default, use a nightly"
      type: string
      default: ""
    # Don't edit these
    python_version:
      description: "Python version to build against (e.g., 3.7)"
      type: string
    cu_version:
      description: "CUDA version to build against, in CU format (e.g., cpu or cu100)"
      type: string
      default: "cpu"
    unicode_abi:
      description: "Python 2.7 wheel only: whether or not we are cp27mu (default: no)"
      type: string
      default: ""
    wheel_docker_image:
      description: "Wheel only: what docker image to use"
      type: string
      default: "pytorch/manylinux-cuda116"
    conda_docker_image:
      description: "Conda only: what docker image to use"
      type: string
      default: "pytorch/conda-builder:cpu"
  environment:
    PYTHON_VERSION: << parameters.python_version >>
    PYTORCH_VERSION: << parameters.pytorch_version >>
    UNICODE_ABI: << parameters.unicode_abi >>
    CU_VERSION: << parameters.cu_version >>

smoke_test_common: &smoke_test_common
  <<: *binary_common
  docker:
    - image: torchrl/smoke_test:latest

jobs:
#  circleci_consistency:
#    docker:
#      - image: circleci/python:3.7
#    steps:
#      - checkout
#      - pip_install:
#          args: jinja2 pyyaml
#      - run:
#          name: Check CircleCI config consistency
#          command: |
#            python .circleci/regenerate.py
#            git diff --exit-code || (echo ".circleci/config.yml not in sync with config.yml.in! Run .circleci/regenerate.py to update config"; exit 1)

  lint_python_and_config:
    docker:
      - image: circleci/python:3.8
    steps:
      - checkout
      - pip_install:
          args: pre-commit
          descr: Install lint utilities
      - run:
          name: Install pre-commit hooks
          command: pre-commit install-hooks
      - run:
          name: Lint Python code and config files
          command: pre-commit run --all-files
      - run:
          name: Required lint modifications
          when: on_fail
          command: git --no-pager diff

  lint_c:
    docker:
      - image: circleci/python:3.7
    steps:
      - apt_install:
          args: libtinfo5
          descr: Install additional system libraries
      - checkout
      - run:
          name: Install lint utilities
          command: |
            curl https://oss-clang-format.s3.us-east-2.amazonaws.com/linux64/clang-format-linux64 -o clang-format
            chmod +x clang-format
            sudo mv clang-format /opt/clang-format
      - run:
          name: Lint C code
          command: ./.circleci/unittest/linux/scripts/run-clang-format.py -r torchrl/csrc --clang-format-executable /opt/clang-format
      - run:
          name: Required lint modifications
          when: on_fail
          command: git --no-pager diff

  type_check_python:
    docker:
      - image: circleci/python:3.7
    steps:
      - checkout
      - pip_install:
          args: cmake ninja
          descr: Install CMake and Ninja
      - install_torchrl:
          editable: true
      - pip_install:
          args: mypy
          descr: Install Python type check utilities
      - run:
          name: Check Python types statically
          command: mypy --install-types --non-interactive --config-file mypy.ini

  binary_linux_wheel:
    <<: *binary_common
    docker:
      - image: << parameters.wheel_docker_image >>
    resource_class: 2xlarge+
    steps:
      - checkout_merge
      - designate_upload_channel
      - run: packaging/build_wheels.sh
      - store_artifacts:
          path: dist
      - persist_to_workspace:
          root: dist
          paths:
            - "*"

  binary_macos_wheel:
    <<: *binary_common
    macos:
      xcode: "14.0"
    steps:
      - checkout_merge
      - designate_upload_channel
      - run:
          # Cannot easily deduplicate this as source'ing activate
          # will set environment variables which we need to propagate
          # to build_wheel.sh
          command: |
            curl -o conda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh
            sh conda.sh -b
            source $HOME/miniconda3/bin/activate
            packaging/build_wheels.sh
      - store_artifacts:
          path: dist
      - persist_to_workspace:
          root: dist
          paths:
            - "*"

  binary_win_wheel:
    <<: *binary_common
    executor: windows-cpu
    steps:
      - checkout_merge
      - designate_upload_channel
      - run:
          name: Build wheel packages
          no_output_timeout: 30m
          command: |
            set -ex
            source packaging/windows/internal/vc_install_helper.sh
            packaging/windows/internal/cuda_install.bat
            packaging/build_wheels.sh
      - store_artifacts:
          path: dist
      - persist_to_workspace:
          root: dist
          paths:
            - "*"

  unittest_linux_cpu:
    <<: *binary_common

    docker:
      - image: "pytorch/manylinux-cuda117"
    resource_class: 2xlarge+

    environment:
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v2-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}
      - run:
          name: Setup
          command: .circleci/unittest/linux/scripts/setup_env.sh

      - save_cache:

          key: env-v2-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
      - run:
          name: Install torchrl
          command: .circleci/unittest/linux/scripts/install.sh
      - run:
          name: Run tests
          command: .circleci/unittest/linux/scripts/run_test.sh

      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-cpu

      - run:
          name: Post process
          command: .circleci/unittest/linux/scripts/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda117"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:

          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

      - run:
          name: Setup
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux/scripts/setup_env.sh
      - save_cache:

          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
#      - run:
#          # Here we create an envlist file that contains some env variables that we want the docker container to be aware of.
#          # Normally, the CIRCLECI variable is set and available on all CI workflows: https://circleci.com/docs/2.0/env-vars/#built-in-environment-variables.
#          # They're available in all the other workflows (OSX and Windows).
#          # But here, we're running the unittest_linux_gpu workflows in a docker container, where those variables aren't accessible.
#          # So instead we dump the variables we need in env.list and we pass that file when invoking "docker run".
#          name: export CIRCLECI env var
#          command: echo "CIRCLECI=true" >> ./env.list
      - run:
          name: Install torchrl
#          command: bash .circleci/unittest/linux/scripts/install.sh
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux/scripts/install.sh
      - run:
          name: Run tests
          command: bash .circleci/unittest/linux/scripts/run_test.sh
#          command: docker run --env-file ./env.list -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux/scripts/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-gpu
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux/scripts/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_distributed_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda117"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:

          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_distributed/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

      - run:
          name: Setup
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_distributed/scripts/setup_env.sh
      - save_cache:

          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_distributed/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
#      - run:
#          # Here we create an envlist file that contains some env variables that we want the docker container to be aware of.
#          # Normally, the CIRCLECI variable is set and available on all CI workflows: https://circleci.com/docs/2.0/env-vars/#built-in-environment-variables.
#          # They're available in all the other workflows (OSX and Windows).
#          # But here, we're running the unittest_linux_distributed_gpu workflows in a docker container, where those variables aren't accessible.
#          # So instead we dump the variables we need in env.list and we pass that file when invoking "docker run".
#          name: export CIRCLECI env var
#          command: echo "CIRCLECI=true" >> ./env.list
      - run:
          name: Install torchrl
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux/scripts/install.sh
      - run:
          name: Run tests
          command: bash .circleci/unittest/linux_distributed/scripts/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-distributed-gpu
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_distributed/scripts/post_process.sh
      - store_test_results:
          path: test-results


  unittest_linux_examples_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda117"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:

          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_examples/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

      - run:
          name: Setup
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_examples/scripts/setup_env.sh
      - save_cache:

          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_examples/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
      - run:
          name: Install torchrl
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_examples/scripts/install.sh
      - run:
          name: Run tests
          command: bash .circleci/unittest/linux_examples/scripts/run_test.sh
#          command: docker run --env-file ./env.list -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux/scripts/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux_examples-gpu
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_examples/scripts/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_habitat_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "nvidia/cudagl:11.4.0-base"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_habitat/environment.yml" }}-{{ checksum ".circleci-weekly" }}
      - run:
          name: Setup
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_habitat/setup_env.sh
      - save_cache:

          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_habitat/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
      - run:
          # Here we create an envlist file that contains some env variables that we want the docker container to be aware of.
          # Normally, the CIRCLECI variable is set and available on all CI workflows: https://circleci.com/docs/2.0/env-vars/#built-in-environment-variables.
          # They're available in all the other workflows (OSX and Windows).
          # But here, we're running the unittest_linux_gpu workflows in a docker container, where those variables aren't accessible.
          # So instead we dump the variables we need in env.list and we pass that file when invoking "docker run".
          name: export CIRCLECI env var
          command: echo "CIRCLECI=true" >> ./env.list
      - run:
          name: Install torchrl
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_habitat/install.sh
      - run:
          name: Run tests
          command: docker run --env-file ./env.list -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_habitat/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F habitat-gpu
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_habitat/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_d4rl_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "nvidia/cudagl:11.4.0-base"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_d4rl/environment.yml" }}-{{ checksum ".circleci-weekly" }}
      - run:
          name: Setup
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_d4rl/setup_env.sh
      - save_cache:

          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_d4rl/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
      - run:
          # Here we create an envlist file that contains some env variables that we want the docker container to be aware of.
          # Normally, the CIRCLECI variable is set and available on all CI workflows: https://circleci.com/docs/2.0/env-vars/#built-in-environment-variables.
          # They're available in all the other workflows (OSX and Windows).
          # But here, we're running the unittest_linux_gpu workflows in a docker container, where those variables aren't accessible.
          # So instead we dump the variables we need in env.list and we pass that file when invoking "docker run".
          name: export CIRCLECI env var
          command: echo "CIRCLECI=true" >> ./env.list
      - run:
          name: Install torchrl
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_d4rl/install.sh
      - run:
          name: Run tests
          command: docker run --env-file ./env.list -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_d4rl/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F d4rl-gpu
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_d4rl/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_sklearn_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "nvidia/cudagl:11.4.0-base"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_sklearn/environment.yml" }}-{{ checksum ".circleci-weekly" }}
      - run:
          name: Setup
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_sklearn/setup_env.sh
      - save_cache:

          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_sklearn/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
      - run:
          # Here we create an envlist file that contains some env variables that we want the docker container to be aware of.
          # Normally, the CIRCLECI variable is set and available on all CI workflows: https://circleci.com/docs/2.0/env-vars/#built-in-environment-variables.
          # They're available in all the other workflows (OSX and Windows).
          # But here, we're running the unittest_linux_gpu workflows in a docker container, where those variables aren't accessible.
          # So instead we dump the variables we need in env.list and we pass that file when invoking "docker run".
          name: export CIRCLECI env var
          command: echo "CIRCLECI=true" >> ./env.list
      - run:
          name: Install torchrl
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_sklearn/install.sh
      - run:
          name: Run tests
          command: docker run --env-file ./env.list -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_sklearn/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F sklearn-gpu
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_sklearn/post_process.sh
      - store_test_results:
          path: test-results


  unittest_linux_jumanji_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda117"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_jumanji/environment.yml" }}-{{ checksum ".circleci-weekly" }}
      - run:
          name: Setup
          command: .circleci/unittest/linux_libs/scripts_jumanji/setup_env.sh
      - save_cache:
          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_jumanji/environment.yml" }}-{{ checksum ".circleci-weekly" }}
          paths:
            - conda
            - env
      - run:
          name: Install torchrl
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_libs/scripts_jumanji/install.sh
      - run:
          name: Run tests
          command: bash .circleci/unittest/linux_libs/scripts_jumanji/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-jumanji
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_jumanji/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_envpool_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda117"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_envpool/environment.yml" }}-{{ checksum ".circleci-weekly" }}
      - run:
          name: Setup
          command: .circleci/unittest/linux_libs/scripts_envpool/setup_env.sh
      - save_cache:
          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_envpool/environment.yml" }}-{{ checksum ".circleci-weekly" }}
          paths:
            - conda
            - env
      - run:
          name: Install torchrl
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_libs/scripts_envpool/install.sh
      - run:
          name: Run tests
          command: bash .circleci/unittest/linux_libs/scripts_envpool/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-envpool
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_envpool/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_brax_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda117"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_brax/environment.yml" }}-{{ checksum ".circleci-weekly" }}
      - run:
          name: Setup
          command: .circleci/unittest/linux_libs/scripts_brax/setup_env.sh
      - save_cache:
          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_brax/environment.yml" }}-{{ checksum ".circleci-weekly" }}
          paths:
            - conda
            - env
      - run:
          name: Install torchrl
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_libs/scripts_brax/install.sh
      - run:
          name: Run tests
          command: bash .circleci/unittest/linux_libs/scripts_brax/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-brax
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_brax/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_vmas_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda117"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_vmas/environment.yml" }}-{{ checksum ".circleci-weekly" }}
      - run:
          name: Setup
          command: .circleci/unittest/linux_libs/scripts_vmas/setup_env.sh
      - save_cache:
          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_vmas/environment.yml" }}-{{ checksum ".circleci-weekly" }}
          paths:
            - conda
            - env
      - run:
          name: Install torchrl
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_libs/scripts_vmas/install.sh
      - run:
          name: Run tests
          command: bash .circleci/unittest/linux_libs/scripts_vmas/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-vmas
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_vmas/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_gym_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda117"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v2-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_gym/environment.yml" }}-{{ checksum ".circleci-weekly" }}
      - run:
          name: Setup
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_libs/scripts_gym/setup_env.sh
      - save_cache:
          key: env-v2-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_libs/scripts_gym/environment.yml" }}-{{ checksum ".circleci-weekly" }}
          paths:
            - conda
            - env
      - run:
          name: Install torchrl, run tests
          command: |
            docker run -t --env=CUDA_VISIBLE_DEVICES="" --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_libs/scripts_gym/batch_scripts.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-gpu
      - run:
          name: Post process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_libs/scripts_gym/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_optdeps_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda117"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:

          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_optdeps/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

      - run:
          name: Setup
          command: .circleci/unittest/linux_optdeps/scripts/setup_env.sh
      - save_cache:

          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_optdeps/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
#      - run:
#          # Here we create an envlist file that contains some env variables that we want the docker container to be aware of.
#          # Normally, the CIRCLECI variable is set and available on all CI workflows: https://circleci.com/docs/2.0/env-vars/#built-in-environment-variables.
#          # They're available in all the other workflows (OSX and Windows).
#          # But here, we're running the unittest_linux_gpu workflows in a docker container, where those variables aren't accessible.
#          # So instead we dump the variables we need in env.list and we pass that file when invoking "docker run".
#          name: export CIRCLECI env var
#          command: echo "CIRCLECI=true" >> ./env.list
      - run:
          name: Install torchrl
#          command: bash .circleci/unittest/linux_optdeps/scripts/install.sh
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_optdeps/scripts/install.sh
      - run:
          name: Run tests
          command: bash .circleci/unittest/linux_optdeps/scripts/run_test.sh
#          command: docker run --env-file ./env.list -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux/scripts/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-outdeps-gpu
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_optdeps/scripts/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_stable_cpu:
    <<: *binary_common

    docker:
      - image: "pytorch/manylinux-cuda116"
    resource_class: 2xlarge+

    environment:
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:

          keys:
            - env-v2-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_stable/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

      - run:
          name: Setup
          command: .circleci/unittest/linux_stable/scripts/setup_env.sh

      - save_cache:

          key: env-v2-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_stable/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
      - run:
          name: Install torchrl
          command: .circleci/unittest/linux_stable/scripts/install.sh
      - run:
          name: Run tests
          command: .circleci/unittest/linux_stable/scripts/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-stable-cpu
      - run:
          name: Post process
          command: .circleci/unittest/linux_stable/scripts/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_stable_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda116"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:

          keys:
            - env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_stable/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

      - run:
          name: Setup
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_stable/scripts/setup_env.sh
      - save_cache:

          key: env-v3-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_stable/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
#      - run:
#          # Here we create an envlist file that contains some env variables that we want the docker container to be aware of.
#          # Normally, the CIRCLECI variable is set and available on all CI workflows: https://circleci.com/docs/2.0/env-vars/#built-in-environment-variables.
#          # They're available in all the other workflows (OSX and Windows).
#          # But here, we're running the unittest_linux_gpu workflows in a docker container, where those variables aren't accessible.
#          # So instead we dump the variables we need in env.list and we pass that file when invoking "docker run".
#          name: export CIRCLECI env var
#          command: echo "CIRCLECI=true" >> ./env.list
      - run:
          name: Install torchrl
#          command: bash .circleci/unittest/linux_stable/scripts/install.sh
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_stable/scripts/install.sh
      - run:
          name: Run tests
          command: bash .circleci/unittest/linux_stable/scripts/run_test.sh
#          command: docker run --env-file ./env.list -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux/scripts/run_test.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F linux-stable-gpu
      - run:
          name: Post Process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_stable/scripts/post_process.sh
      - store_test_results:
          path: test-results

  unittest_linux_olddeps_gpu:
    <<: *binary_common
    machine:
      image: ubuntu-2004-cuda-11.4:202110-01
    resource_class: gpu.nvidia.medium
    environment:
      image_name: "pytorch/manylinux-cuda116"
      TAR_OPTIONS: --no-same-owner
      PYTHON_VERSION: << parameters.python_version >>
      CU_VERSION: << parameters.cu_version >>

    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v2-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_olddeps/scripts_gym_0_13/environment.yml" }}-{{ checksum ".circleci-weekly" }}
      - run:
          name: Setup
          command: docker run -e PYTHON_VERSION -t --gpus all -v $PWD:$PWD -w $PWD "${image_name}" .circleci/unittest/linux_olddeps/scripts_gym_0_13/setup_env.sh
      - save_cache:
          key: env-v2-linux-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/linux_olddeps/scripts_gym_0_13/environment.yml" }}-{{ checksum ".circleci-weekly" }}
          paths:
            - conda
            - env
      - run:
          name: Install torchrl, run tests
          command: |
            docker run -t --env=CUDA_VISIBLE_DEVICES="" --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_olddeps/scripts_gym_0_13/batch_scripts.sh
#            docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_olddeps/scripts_gym_0_13/batch_scripts.sh
      - run:
          name: Codecov upload
          command: |
            curl -Os https://uploader.codecov.io/latest/linux/codecov
            chmod +x codecov
            ./codecov -t ${CODECOV_TOKEN} -s ./ -Z -F olddeps-gpu
      - run:
          name: Post process
          command: docker run -t --gpus all -v $PWD:$PWD -w $PWD -e UPLOAD_CHANNEL -e CU_VERSION "${image_name}" .circleci/unittest/linux_olddeps/scripts_gym_0_13/post_process.sh
      - store_test_results:
          path: test-results

  unittest_windows_optdepts_cpu:
    <<: *binary_common
    executor:
      name: windows-cpu
    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v2-windows-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/windows_optdepts/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

      - run:
          name: Setup
          command: .circleci/unittest/windows_optdepts/scripts/setup_env.sh
      - save_cache:
          key: env-v2-windows-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/windows_optdepts/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
      - run:
          name: Install torchrl
          command: .circleci/unittest/windows_optdepts/scripts/install.sh
      - run:
          name: Run tests
          command: .circleci/unittest/windows_optdepts/scripts/run_test.sh
      - run:
          name: Post process
          command: .circleci/unittest/windows_optdepts/scripts/post_process.sh
      - store_test_results:
          path: test-results

  unittest_windows_optdepts_gpu:
    <<: *binary_common
    executor:
      name: windows-gpu
    environment:
      CUDA_VERSION: "11.6"
      PYTHON_VERSION: << parameters.python_version >>
    steps:
      - checkout
      - designate_upload_channel
      - run:
          name: Generate cache key
          # This will refresh cache on Sundays, nightly build should generate new cache.
          command: echo "$(date +"%Y-%U")" > .circleci-weekly
      - restore_cache:
          keys:
            - env-v1-windows-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/windows_optdepts/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

      - run:
          name: Setup
          command: .circleci/unittest/windows_optdepts/scripts/setup_env.sh
      - save_cache:
          key: env-v1-windows-{{ arch }}-py<< parameters.python_version >>-{{ checksum ".circleci/unittest/windows_optdepts/scripts/environment.yml" }}-{{ checksum ".circleci-weekly" }}

          paths:
            - conda
            - env
      - run:
          name: Install CUDA
          command: packaging/windows/internal/cuda_install.bat
      - run:
          name: Update CUDA driver
          command: packaging/windows/internal/driver_update.bat
      - run:
          name: Install torchrl
          command: .circleci/unittest/windows_optdepts/scripts/install.sh
      - run:
          name: Run tests
          command: .circleci/unittest/windows_optdepts/scripts/run_test.sh
      - run:
          name: Post process
          command: .circleci/unittest/windows_optdepts/scripts/post_process.sh
      - store_test_results:
          path: test-results


workflows:
  lint:
    jobs:
      - lint_python_and_config
      - lint_c

  build:
    jobs:
      - binary_linux_wheel:
          conda_docker_image: pytorch/conda-builder:cpu
          cu_version: cpu
          name: binary_linux_wheel_py3.7_cpu
          python_version: '3.7'
          wheel_docker_image: pytorch/manylinux-cuda102

      - binary_linux_wheel:
          conda_docker_image: pytorch/conda-builder:cpu
          cu_version: cpu
          name: binary_linux_wheel_py3.8_cpu
          python_version: '3.8'
          wheel_docker_image: pytorch/manylinux-cuda102

      - binary_linux_wheel:
          conda_docker_image: pytorch/conda-builder:cpu
          cu_version: cpu
          name: binary_linux_wheel_py3.9_cpu
          python_version: '3.9'
          wheel_docker_image: pytorch/manylinux-cuda102

      - binary_linux_wheel:
          conda_docker_image: pytorch/conda-builder:cpu
          cu_version: cpu
          name: binary_linux_wheel_py3.10_cpu
          python_version: '3.10'
          wheel_docker_image: pytorch/manylinux-cuda102

      - binary_macos_wheel:
          conda_docker_image: pytorch/conda-builder:cpu
          cu_version: cpu
          name: binary_macos_wheel_py3.7_cpu
          python_version: '3.7'
          wheel_docker_image: pytorch/manylinux-cuda102

      - binary_macos_wheel:
          conda_docker_image: pytorch/conda-builder:cpu
          cu_version: cpu
          name: binary_macos_wheel_py3.8_cpu
          python_version: '3.8'
          wheel_docker_image: pytorch/manylinux-cuda102

      - binary_macos_wheel:
          conda_docker_image: pytorch/conda-builder:cpu
          cu_version: cpu
          name: binary_macos_wheel_py3.9_cpu
          python_version: '3.9'
          wheel_docker_image: pytorch/manylinux-cuda102

      - binary_macos_wheel:
          conda_docker_image: pytorch/conda-builder:cpu
          cu_version: cpu
          name: binary_macos_wheel_py3.10_cpu
          python_version: '3.10'
          wheel_docker_image: pytorch/manylinux-cuda102

      - binary_win_wheel:
          cu_version: cpu
          name: binary_win_wheel_py3.7_cpu
          python_version: '3.7'

      - binary_win_wheel:
          cu_version: cu116
          name: binary_win_wheel_py3.7_cu116
          python_version: '3.7'

      - binary_win_wheel:
          cu_version: cu117
          name: binary_win_wheel_py3.7_cu117
          python_version: '3.7'

      - binary_win_wheel:
          cu_version: cpu
          name: binary_win_wheel_py3.8_cpu
          python_version: '3.8'

      - binary_win_wheel:
          cu_version: cu116
          name: binary_win_wheel_py3.8_cu116
          python_version: '3.8'

      - binary_win_wheel:
          cu_version: cu117
          name: binary_win_wheel_py3.8_cu117
          python_version: '3.8'

      - binary_win_wheel:
          cu_version: cpu
          name: binary_win_wheel_py3.9_cpu
          python_version: '3.9'

      - binary_win_wheel:
          cu_version: cu116
          name: binary_win_wheel_py3.9_cu116
          python_version: '3.9'

      - binary_win_wheel:
          cu_version: cu117
          name: binary_win_wheel_py3.9_cu117
          python_version: '3.9'

      - binary_win_wheel:
          cu_version: cpu
          name: binary_win_wheel_py3.10_cpu
          python_version: '3.10'

      - binary_win_wheel:
          cu_version: cu116
          name: binary_win_wheel_py3.10_cu116
          python_version: '3.10'

      - binary_win_wheel:
          cu_version: cu117
          name: binary_win_wheel_py3.10_cu117
          python_version: '3.10'


  unittest:
    jobs:
      - unittest_linux_sklearn_gpu:
          cu_version: cu117
          name: unittest_linux_sklearn_gpu_py3.8
          python_version: '3.8'
      - unittest_linux_d4rl_gpu:
          cu_version: cu117
          name: unittest_linux_d4rl_gpu_py3.8
          python_version: '3.8'
      - unittest_linux_jumanji_gpu:
          cu_version: cu117
          name: unittest_linux_jumanji_gpu_py3.8
          python_version: '3.8'
      - unittest_linux_envpool_gpu:
          cu_version: cu117
          name: unittest_linux_envpool_gpu_py3.8
          python_version: '3.8'
      - unittest_linux_brax_gpu:
          cu_version: cu117
          name: unittest_linux_brax_gpu_py3.8
          python_version: '3.8'
      - unittest_linux_vmas_gpu:
          cu_version: cu117
          name: unittest_linux_vmas_gpu_py3.8
          python_version: '3.8'
      - unittest_linux_gym_gpu:
          cu_version: cu117
          name: unittest_linux_gym_gpu_py3.8
          python_version: '3.8'

      - unittest_linux_cpu:
          cu_version: cpu
          name: unittest_linux_cpu_py3.9
          python_version: '3.9'
      - unittest_linux_gpu:
          cu_version: cu117
          name: unittest_linux_gpu_py3.9
          python_version: '3.9'
      - unittest_linux_distributed_gpu:
          cu_version: cu117
          name: unittest_linux_distributed_gpu_py3.9
          python_version: '3.9'
      - unittest_linux_optdeps_gpu:
          cu_version: cu117
          name: unittest_linux_optdeps_gpu_py3.9
          python_version: '3.9'
      - unittest_linux_stable_gpu:
          cu_version: cu116
          name: unittest_linux_stable_gpu_py3.9
          python_version: '3.9'
      - unittest_linux_habitat_gpu:
          cu_version: cu117
          name: unittest_linux_habitat_gpu_py3.9
          python_version: '3.9'

      - unittest_linux_olddeps_gpu:
          cu_version: cu116
          name: unittest_linux_olddeps_gpu_py3.9
          python_version: '3.9'

      - unittest_linux_examples_gpu:
          cu_version: cu117
          name: unittest_linux_examples_gpu_py3.9
          python_version: '3.9'

      - unittest_windows_optdepts_cpu:
          cu_version: cpu
          name: unittest_windows_optdepts_cpu_py3.8
          python_version: '3.8'
      - unittest_windows_optdepts_gpu:
          cu_version: cu116
          name: unittest_windows_optdepts_gpu_py3.8
          python_version: '3.8'
