{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Getting started with model optimization\n\n**Author**: [Vincent Moens](https://github.com/vmoens)\n\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To run this tutorial in a notebook, add an installation cell\n  at the beginning containing:\n\n```\n!pip install tensordict\n!pip install torchrl</p></div>\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In TorchRL, we try to treat optimization as it is custom to do in PyTorch,\nusing dedicated loss modules which are designed with the sole purpose of\noptimizing the model. This approach efficiently decouples the execution of\nthe policy from its training and allows us to design training loops that are\nsimilar to what can be found in traditional supervised learning examples.\n\nThe typical training loop therefore looks like this:\n\n  >>> for i in range(n_collections):\n  ...     data = get_next_batch(env, policy)\n  ...     for j in range(n_optim):\n  ...         loss = loss_fn(data)\n  ...         loss.backward()\n  ...         optim.step()\n\nIn this concise tutorial, you will receive a brief overview of the loss modules. Due to the typically\nstraightforward nature of the API for basic usage, this tutorial will be kept brief.\n\n## RL objective functions\n\nIn RL, innovation typically involves the exploration of novel methods\nfor optimizing a policy (i.e., new algorithms), rather than focusing\non new architectures, as seen in other domains. Within TorchRL,\nthese algorithms are encapsulated within loss modules. A loss\nmodule orchestrates the various components of your algorithm and\nyields a set of loss values that can be backpropagated\nthrough to train the corresponding components.\n\nIn this tutorial, we will take a popular\noff-policy algorithm as an example,\n[DDPG](https://arxiv.org/abs/1509.02971).\n\nTo build a loss module, the only thing one needs is a set of networks\ndefined as :class:`~tensordict.nn.TensorDictModule`s. Most of the time, one\nof these modules will be the policy. Other auxiliary networks such as\nQ-Value networks or critics of some kind may be needed as well. Let's see\nwhat this looks like in practice: DDPG requires a deterministic\nmap from the observation space to the action space as well as a value\nnetwork that predicts the value of a state-action pair. The DDPG loss will\nattempt to find the policy parameters that output actions that maximize the\nvalue for a given state.\n\nTo build the loss, we need both the actor and value networks.\nIf they are built according to DDPG's expectations, it is all\nwe need to get a trainable loss module:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import GymEnv\n\nenv = GymEnv(\"Pendulum-v1\")\n\nfrom torchrl.modules import Actor, MLP, ValueOperator\nfrom torchrl.objectives import DDPGLoss\n\nn_obs = env.observation_spec[\"observation\"].shape[-1]\nn_act = env.action_spec.shape[-1]\nactor = Actor(MLP(in_features=n_obs, out_features=n_act, num_cells=[32, 32]))\nvalue_net = ValueOperator(\n    MLP(in_features=n_obs + n_act, out_features=1, num_cells=[32, 32]),\n    in_keys=[\"observation\", \"action\"],\n)\n\nddpg_loss = DDPGLoss(actor_network=actor, value_network=value_net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that is it! Our loss module can now be run with data coming from the\nenvironment (we omit exploration, storage and other features to focus on\nthe loss functionality):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rollout = env.rollout(max_steps=100, policy=actor)\nloss_vals = ddpg_loss(rollout)\nprint(loss_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LossModule's output\n\nAs you can see, the value we received from the loss isn't a single scalar\nbut a dictionary containing multiple losses.\n\nThe reason is simple: because more than one network may be trained at a time,\nand since some users may wish to separate the optimization of each module\nin distinct steps, TorchRL's objectives will return dictionaries containing\nthe various loss components.\n\nThis format also allows us to pass metadata along with the loss values. In\ngeneral, we make sure that only the loss values are differentiable such that\nyou can simply sum over the values of the dictionary to obtain the total\nloss. If you want to make sure you're fully in control of what is happening,\nyou can sum over only the entries which keys start with the ``\"loss_\"`` prefix:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_loss = 0\nfor key, val in loss_vals.items():\n    if key.startswith(\"loss_\"):\n        total_loss += val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a LossModule\n\nGiven all this, training the modules is not so different from what would be\ndone in any other training loop. Because it wraps the modules,\nthe easiest way to get the list of trainable parameters is to query\nthe :meth:`~torchrl.objectives.LossModule.parameters` method.\n\nWe'll need an optimizer (or one optimizer\nper module if that is your choice).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n\noptim = Adam(ddpg_loss.parameters())\ntotal_loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following items will typically be\nfound in your training loop:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optim.step()\noptim.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further considerations: Target parameters\n\nAnother important aspect to consider is the presence of target parameters\nin off-policy algorithms like DDPG. Target parameters typically represent\na delayed or smoothed version of the parameters over time, and they play\na crucial role in value estimation during policy training. Utilizing target\nparameters for policy training often proves to be significantly more\nefficient compared to using the current configuration of value network\nparameters. Generally, managing target parameters is handled by the loss\nmodule, relieving users of direct concern. However, it remains the user's\nresponsibility to update these values as necessary based on specific\nrequirements. TorchRL offers a couple of updaters, namely\n:class:`~torchrl.objectives.HardUpdate` and\n:class:`~torchrl.objectives.SoftUpdate`,\nwhich can be easily instantiated without requiring in-depth\nknowledge of the underlying mechanisms of the loss module.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.objectives import SoftUpdate\n\nupdater = SoftUpdate(ddpg_loss, eps=0.99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In your training loop, you will need to update the target parameters at each\noptimization step or each collection step:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "updater.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is all you need to know about loss modules to get started!\n\nTo further explore the topic, have a look at:\n\n- The `loss module reference page <ref_objectives>`;\n- The `Coding a DDPG loss tutorial <coding_ddpg>`;\n- Losses in action in `PPO <coding_ppo>` or `DQN <coding_dqn>`.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}