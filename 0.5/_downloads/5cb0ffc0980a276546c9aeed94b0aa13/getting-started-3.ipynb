{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Get started with data collection and storage\n\n**Author**: [Vincent Moens](https://github.com/vmoens)\n\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To run this tutorial in a notebook, add an installation cell\n  at the beginning containing:\n\n```\n!pip install tensordict\n!pip install torchrl</p></div>\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is no learning without data. In supervised learning, users are\naccustomed to using :class:`~torch.utils.data.DataLoader` and the like\nto integrate data in their training loop.\nDataloaders are iterable objects that provide you with the data that you will\nbe using to train your model.\n\nTorchRL approaches the problem of dataloading in a similar manner, although\nit is surprisingly unique in the ecosystem of RL libraries. TorchRL's\ndataloaders are referred to as ``DataCollectors``. Most of the time,\ndata collection does not stop at the collection of raw data,\nas the data needs to be stored temporarily in a buffer\n(or equivalent structure for on-policy algorithms) before being consumed\nby the `loss module <gs_optim>`. This tutorial will explore\nthese two classes.\n\n## Data collectors\n\n\n\nThe primary data collector discussed here is the\n:class:`~torchrl.collectors.SyncDataCollector`, which is the focus of this\ndocumentation. At a fundamental level, a collector is a straightforward\nclass responsible for executing your policy within the environment,\nresetting the environment when necessary, and providing batches of a\npredefined size. Unlike the :meth:`~torchrl.envs.EnvBase.rollout` method\ndemonstrated in `the env tutorial <gs_env_ted>`, collectors do not\nreset between consecutive batches of data. Consequently, two successive\nbatches of data may contain elements from the same trajectory.\n\nThe basic arguments you need to pass to your collector are the size of the\nbatches you want to collect (``frames_per_batch``), the length (possibly\ninfinite) of the iterator, the policy and the environment. For simplicity,\nwe will use a dummy, random policy in this example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\ntorch.manual_seed(0)\n\nfrom torchrl.collectors import SyncDataCollector\nfrom torchrl.envs import GymEnv\nfrom torchrl.envs.utils import RandomPolicy\n\nenv = GymEnv(\"CartPole-v1\")\nenv.set_seed(0)\n\npolicy = RandomPolicy(env.action_spec)\ncollector = SyncDataCollector(env, policy, frames_per_batch=200, total_frames=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now expect that our collector will deliver batches of size ``200`` no\nmatter what happens during collection. In other words, we may have multiple\ntrajectories in this batch! The ``total_frames`` indicates how long the\ncollector should be. A value of ``-1`` will produce a never\nending collector.\n\nLet's iterate over the collector to get a sense\nof what this data looks like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for data in collector:\n    print(data)\n    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, our data is augmented with some collector-specific metadata\ngrouped in a ``\"collector\"`` sub-tensordict that we did not see during\n`environment rollouts <gs_env_ted_rollout>`. This is useful to keep track of\nthe trajectory ids. In the following list, each item marks the trajectory\nnumber the corresponding transition belongs to:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data[\"collector\", \"traj_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data collectors are very useful when it comes to coding state-of-the-art\nalgorithms, as performance is usually measured by the capability of a\nspecific technique to solve a problem in a given number of interactions with\nthe environment (the ``total_frames`` argument in the collector).\nFor this reason, most training loops in our examples look like this:\n\n  >>> for data in collector:\n  ...     # your algorithm here\n\n\n## Replay Buffers\n\n\nNow that we have explored how to collect data, we would like to know how to\nstore it. In RL, the typical setting is that the data is collected, stored\ntemporarily and cleared after a little while given some heuristic:\nfirst-in first-out or other. A typical pseudo-code would look like this:\n\n  >>> for data in collector:\n  ...     storage.store(data)\n  ...     for i in range(n_optim):\n  ...         sample = storage.sample()\n  ...         loss_val = loss_fn(sample)\n  ...         loss_val.backward()\n  ...         optim.step() # etc\n\nThe parent class that stores the data in TorchRL\nis referred to as :class:`~torchrl.data.ReplayBuffer`. TorchRL's replay\nbuffers are composable: you can edit the storage type, their sampling\ntechnique, the writing heuristic or the transforms applied to them. We will\nleave the fancy stuff for a dedicated in-depth tutorial. The generic replay\nbuffer only needs to know what storage it has to use. In general, we\nrecommend a :class:`~torchrl.data.TensorStorage` subclass, which will work\nfine in most cases. We'll be using\n:class:`~torchrl.data.replay_buffers.LazyMemmapStorage`\nin this tutorial, which enjoys two nice properties: first, being \"lazy\",\nyou don't  need to explicitly tell it what your data looks like in advance.\nSecond, it uses :class:`~tensordict.MemoryMappedTensor` as a backend to save\nyour data on disk in an efficient way. The only thing you need to know is\nhow big you want your buffer to be.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.data.replay_buffers import LazyMemmapStorage, ReplayBuffer\n\nbuffer = ReplayBuffer(storage=LazyMemmapStorage(max_size=1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Populating the buffer can be done via the\n:meth:`~torchrl.data.ReplayBuffer.add` (single element) or\n:meth:`~torchrl.data.ReplayBuffer.extend` (multiple elements) methods. Using\nthe data we just collected, we initialize and populate the buffer in one go:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "indices = buffer.extend(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can check that the buffer now has the same number of elements than what\nwe got from the collector:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert len(buffer) == collector.frames_per_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The only thing left to know is how to gather data from the buffer.\nNaturally, this relies on the :meth:`~torchrl.data.ReplayBuffer.sample`\nmethod. Because we did not specify that sampling had to be done without\nrepetitions, it is not guaranteed that the samples gathered from our buffer\nwill be unique:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample = buffer.sample(batch_size=30)\nprint(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, our sample looks exactly the same as the data we gathered from the\ncollector!\n\n## Next steps\n\n- You can have look at other multirpocessed\n  collectors such as :class:`~torchrl.collectors.collectors.MultiSyncDataCollector` or\n  :class:`~torchrl.collectors.collectors.MultiaSyncDataCollector`.\n- TorchRL also offers distributed collectors if you have multiple nodes to\n  use for inference. Check them out in the\n  `API reference <ref_collectors>`.\n- Check the dedicated `Replay Buffer tutorial <rb_tuto>` to know\n  more about the options you have when building a buffer, or the\n  `API reference <ref_data>` which covers all the features in\n  details. Replay buffers have countless features such as multithreaded\n  sampling, prioritized experience replay, and many more...\n- We left out the capacity of replay buffers to be iterated over for\n  simplicity. Try it out for yourself: build a buffer and indicate its\n  batch-size in the constructor, then try to iterate over it. This is\n  equivalent to calling ``rb.sample()`` within a loop!\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}