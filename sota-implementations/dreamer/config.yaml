env:
  name: cheetah
  task: run
  seed: 0
  backend: dm_control
  frame_skip: 2
  from_pixels: True
  grayscale: False
  image_size : 64
  horizon: 500
  n_parallel_envs: 8
  device: cpu

collector:
  init_random_frames: 10000
  frames_per_batch: 1000
  # Number of parallel collector workers (async mode)
  # On multi-GPU: must be <= num_gpus - 1 (cuda:0 reserved for training)
  num_collectors: 7
  device:

optimization:
  # Total number of optimization steps (the main training target)
  total_optim_steps: 100_000
  # Log metrics every N optim steps
  log_every: 100
  grad_clip: 100

  world_model_lr: 6e-4
  actor_lr: 8e-5
  value_lr: 8e-5
  kl_scale: 1.0
  free_nats: 3.0
  gamma: 0.99
  lmbda: 0.95
  imagination_horizon: 15
  compile:
    # Note: world_model has RSSM loop that can cause CUDA graph issues
    enabled: True
    backend: inductor # or cudagraphs
    mode: default  # avoid reduce-overhead which uses CUDA graphs
    # Which losses to compile (subset of: world_model, actor, value)
    # Note: world_model causes CUDA graph tensor overwrite errors due to RSSM loop
    losses: ["actor", "value"]
  # Autocast options: false, true (=bfloat16), float16, bfloat16
  autocast: bfloat16

networks:
  exploration_noise: 0.3
  device:
  state_dim: 30
  rssm_hidden_dim: 200
  hidden_dim: 400
  activation: "elu"
  # Use torch.scan for RSSM rollout (faster, no graph breaks with torch.compile)
  use_scan: False
  rssm_rollout:
    # Compile only the per-timestep RSSM rollout step (keeps Python loop, avoids scan/unrolling).
    compile: False
    compile_backend: inductor
    compile_mode: reduce-overhead


replay_buffer: 
  batch_size: 10000
  buffer_size: 1000000
  batch_length: 50
  scratch_dir: null
  prefetch: 16

logger:
  backend: wandb
  project: dreamer-v1
  exp_name: ${env.name}-${env.task}-${env.seed}
  mode: online
  # eval interval, in optim steps
  eval_every: 1000
  eval_rollout_steps: 500
  # Video recording settings
  video: False
  video_skip: 1  # frames to skip (1 = record every frame, 2 = every other frame)

profiling:
  # Enable PyTorch profiling
  enabled: False
  # Total optim steps when profiling (overrides optimization.total_optim_steps)
  # When using distributed profiling (profiling.distributed.enabled=true), set this
  # higher than PROF_ITERATIONS end value to allow prof to complete its window.
  # e.g., if PROF_ITERATIONS=50-55, set total_optim_steps >= 60
  total_optim_steps: 70
  # Skip the first N optim steps (no profiling at all)
  skip_first: 1
  # Warmup steps (profiler runs but data discarded for warmup)
  warmup_steps: 1
  # Number of optim steps to profile (actual traced data)
  active_steps: 1
  # Export chrome trace to this file (if set)
  trace_file: dreamer_trace.json
  # Profile CUDA kernels (VERY heavy on GPU - 13GB vs 1GB trace!)
  profile_cuda: true
  # Record tensor shapes
  record_shapes: True
  # Profile memory usage
  profile_memory: True
  # Record Python call stacks
  with_stack: True
  # Compute FLOPs
  with_flops: True

  # Collector worker profiling
  collector:
    # Enable profiling of collector worker rollouts
    enabled: False
    # Which workers to profile (list of indices)
    workers: [0]
    # Number of rollouts to profile (including warmup)
    num_rollouts: 5
    # Number of warmup rollouts to skip
    warmup_rollouts: 2
    # Path to save collector trace (supports {worker_idx} placeholder)
    trace_file: collector_trace_{worker_idx}.json
    # Override init_random_frames when collector profiling is enabled (0 = skip random frames phase)
    init_random_frames_override: 0

  # Distributed profiling with the prof library
  # This enables coordinated profiling across training and collector workers
  # using shared memory signaling. Set environment variables to control behavior:
  #   PROF_ENABLED=1           - Enable distributed profiling
  #   PROF_ITERATIONS=50-55    - Which training steps to profile
  #   PROF_OUTPUT_DIR=./traces - Where to save trace files
  #   PROF_MODE=lite           - Only trace explicit regions (default)
  distributed:
    # Enable prof-based distributed profiling
    enabled: False
    # Backend for coordination: "shm" (shared memory) for subprocess collectors,
    # "ray" for Ray actors (requires Ray-based collectors)
    backend: shm
