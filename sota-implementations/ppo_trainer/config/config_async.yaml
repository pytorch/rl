# PPO Trainer Configuration for Pendulum-v1
# TODO: Add GAE RB transform
# Run with `python sota-implementations/ppo_trainer/train.py --config-name=config_async`
# This configuration uses the new configurable trainer system

defaults:
  - transform@noop: noop_reset
  - transform@stepcount: step_counter
  - transform@reward_sum: reward_sum
  - transform@flatten: flatten_tensordict
  - transform@gae_transform: module
  - transform@compose_rb_transform: compose
  
  - env@training_env: batched_env
  - env@training_env.create_env_fn: transformed_env
  - env@training_env.create_env_fn.base_env: gym
  - transform@training_env.create_env_fn.transform: compose
  
  - model@models.policy_model: tanh_normal
  - model@models.value_model: value
  
  - network@networks.policy_network: mlp
  - network@networks.value_network: mlp
  
  - collector@collector: multi_async

  - value@gae: gae

  - replay_buffer@replay_buffer: base
  - storage@replay_buffer.storage: lazy_tensor
  - writer@replay_buffer.writer: round_robin
  - sampler@replay_buffer.sampler: random
  - trainer@trainer: ppo
  - optimizer@optimizer: adam
  - loss@loss: ppo
  - logger@logger: wandb
  
  # Weight sync schemes: shared_mem uses a single shared buffer for all workers
  # Much faster than multiprocess (pipe-based) as workers read directly from shared memory
  - weight_sync_scheme@policy_scheme: shared_mem
  - weight_sync_scheme@rb_transform_scheme: shared_mem
  
  - _self_

# Network configurations
networks:
  policy_network:
    out_features: 2  # Pendulum action space is 1-dimensional
    in_features: 3   # Pendulum observation space is 3-dimensional
    num_cells: [128, 128]

  value_network:
    out_features: 1  # Value output
    in_features: 3   # Pendulum observation space
    num_cells: [128, 128]

# Model configurations
models:
  policy_model:
    return_log_prob: true
    in_keys: ["observation"]
    param_keys: ["loc", "scale"]
    out_keys: ["action"]
    network: ${networks.policy_network}

  value_model:
    in_keys: ["observation"]
    out_keys: ["state_value"]
    network: ${networks.value_network}
    shared: true

# Environment configuration
noop:
  noops: 30
  random: true

stepcount:
  max_steps: 200
  step_count_key: "step_count"

reward_sum:
  in_keys: ["reward"]
  out_keys: ["reward_sum"]

training_env:
  num_workers: 4
  create_env_fn:
    base_env:
      env_name: Pendulum-v1
    transform:
      transforms:
        - ${noop}
        - ${stepcount}
        - ${reward_sum}
    _partial_: true

# Loss configuration
loss:
  actor_network: ${models.policy_model}
  critic_network: ${models.value_model}
  entropy_coeff: 0.01

gae:
  gamma: 0.99
  lmbda: 0.95
  value_network: ${loss.critic_network}
  average_gae: true

gae_transform:
  module: ${gae}
  inverse: true
  no_grad: true

# Transform for the rb
compose_rb_transform:
  # Transforms for the buffer will be applied in reverse order
  transforms:
    # Reverse the flatten_tensordict transform
    - ${flatten}
    # Apply the gae model
    - ${gae_transform}

# Optimizer configuration
optimizer:
  lr: 0.001

# Weight sync scheme configurations
# Both use shared memory for fast, zero-copy weight updates
# Workers read directly from a single shared buffer instead of receiving weights via pipes
rb_transform_scheme:
  strategy: tensordict

# Weight sync schemes mapping
# Define as a flat dictionary with string keys for destination paths
weight_sync_schemes:
  policy: ${policy_scheme}
  "replay_buffer.transform[1].module.value_network": ${rb_transform_scheme}

# Collector configuration
collector:
  create_env_fn: ${training_env}
  policy: ${models.policy_model}
  total_frames: 10_000_000
  frames_per_batch: 1024
  num_workers: 8
  track_policy_version: true
  extend_buffer: true
  local_init_rb: true
  _partial_: true
  weight_sync_schemes: ${weight_sync_schemes}

# Replay buffer configuration
replay_buffer:
  storage:
    max_size: 50_000
    device: cpu
    ndim: 1
    shared_init: true
  writer:
    compilable: false
  batch_size: 128
  shared: true
  transform: ${compose_rb_transform}

logger:
  exp_name: ppo_pendulum_v1
  offline: false
  project: torchrl-sota-implementations

# Trainer configuration
trainer:
  collector: ${collector}
  optimizer: ${optimizer}
  replay_buffer: ${replay_buffer}
  loss_module: ${loss}
  logger: ${logger}
  total_frames: 1_000_000
  frame_skip: 1
  clip_grad_norm: true
  clip_norm: 100.0
  progress_bar: true
  seed: 42
  save_trainer_interval: 1000
  log_interval: 100
  save_trainer_file: null
  # these numbers can be tuned but have little impact, just change the amount of data collected and size of the replay buffer
  optim_steps_per_batch: 128
  num_epochs: 1
  async_collection: true
  add_gae: false
  # Weight update map: explicit mapping from collector destinations to trainer sources
  # Keys must match the keys in weight_sync_schemes
  weight_update_map:
    policy: "loss_module.actor_network"
    "replay_buffer.transform[1].module.value_network": "loss_module.critic_network"
  # Enable timing logs for performance monitoring
  log_timings: true
