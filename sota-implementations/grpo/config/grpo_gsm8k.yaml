# @package _global_
defaults:
  - mode: ${mode:async}  # Default to async mode, can be overridden by scripts
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

# Environment configuration
env:
  dataset: gsm8k  # choices: [gsm8k, ifeval]
  # Number of environments to run in parallel. This determines the batch size passed to vLLM.
  #  More envs consume more GPU memory.
  num_envs: 8  # Reduced from 8 to save memory
  # Number of times to repeat the same prompt for GRPO. This does not affect the GPU memory usage.
  repeats: 16

# Base model configuration
model:
  name: Qwen/Qwen2.5-3B
  compile: false

# Base training configuration - will be merged with mode-specific settings
train:
  # Fields defined in mode configs (async.yaml and sync.yaml)
  # mixed_precision: true  # Whether to use mixed precision training
  # epochs: 1  # Number of training epochs
  # steps_per_batch: 32  # Number of steps per batch
  # total_dialog_turns: 1_000_000  # Total number of dialog turns to collect
  # optim_batch_size: 2  # Batch size for optimization
  # gradient_accumulation_steps: 1  # Number of gradient accumulation steps
  # kl_coef_in_loss: true  # Whether to include KL coefficient in loss
  # sync: false  # Default to async, will be overridden by mode configs
  # buffer_size: 128  # Size of replay buffer
  exp_name: "grpo-gsm8k"

  # Fields used by both scripts but with different semantics
  checkpoint_frequency: 100  # Save checkpoint every N steps/batches

  # KL coefficients for the KL divergence to the reference and inference policies
  kl_to_ref_coeff: 1e-2
  kl_to_inference_coeff: 0.0
  entropy_coeff: 0.01
  
  # Fields used only by grpo-async.py / grpo-sync.py
  logging_frequency: 10  # Log metrics every N steps

# Training model configuration
train_model:
  gradient_checkpointing: true  # Enabled for memory efficiency
  num_devices: 1  # Number of devices to use
  lora:
    enabled: true  # Using LoRA for memory efficiency
    r: 8  # LoRA rank - controls capacity of adaptations
    alpha: 16  # LoRA alpha - scales the adaptations
    dropout: 0.1  # Dropout probability for LoRA layers
  quantization:
    enabled: false  # Enable 4-bit quantization for base model
  attn_implementation: sdpa  # Using flash attention for memory efficiency
  torch_dtype: bfloat16

# Inference model configuration
inference_model:
  num_devices: 1  # Number of devices to use
  quantization:
    enabled: false  # Enable 4-bit quantization for base model
  attn_implementation: sdpa  # Using flash attention for memory efficiency
  torch_dtype: bfloat16
  gpu_memory_utilization: 0.5  # Limit GPU memory usage
  temperature: 0.8
  max_tokens: 1024
  include_stop_str_in_output: true
  enforce_eager: false

# Reference model configuration
ref_model:
  gradient_checkpointing: false  # Always false, no backprop
  num_devices: 1  # Number of devices to use
  lora:
    enabled: true  # Using LoRA for memory efficiency
    r: 8  # LoRA rank - controls capacity of adaptations
    alpha: 16  # LoRA alpha - scales the adaptations
    dropout: 0.1  # Dropout probability for LoRA layers
  quantization:
    enabled: false  # Enable 4-bit quantization for base model
  attn_implementation: sdpa  # Using flash attention for memory efficiency
  torch_dtype: bfloat16

# Optimizer configuration
optimizer:
  name: AdamW
  lr: 1e-5
  clip_grad_norm: 1.0
  weight_decay: 0.0

# Ray configuration
ray:
  init_config:
    num_cpus: 96  # Total available CPUs
    num_gpus: 8  # Explicitly set number of GPUs
    runtime_env:
      working_dir: "."
    _temp_dir: "/tmp/ray_grpo"  # Custom temp directory
    _system_config:
      object_spilling_threshold: 0.8  # Spill when 80% full
      max_direct_memory_size: 10 * 1024 * 1024 * 1024  # 10GB limit
      object_store_full_delay_ms: 100  # Delay when store is full
      object_store_full_max_retries: 3  # Max retries when store is full
  collector_config:
    num_cpus: 24  # CPUs for inference and ref model
  train_handler_config:
    num_cpus: 24  # Dedicated CPUs for training
  replay_buffer_config:
    num_cpus: 24  # CPUs for replay buffer
    num_gpus: 0.0  # No GPU needed for replay buffer

# Logging configuration
logging:
  experiment_name: null  # Will be auto-generated if not provided
  checkpoint_dir: "checkpoints"
  checkpoint_frequency: 10  # Save checkpoint every N batches

hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num} 
