# @package _global_
train:
  # Mode-specific setting
  sync: false  # Force asynchronous mode
  
  # Shared training settings
  # Whether to use mixed precision training.
  mixed_precision: true
  # Number of epochs to train for, every time a batch is collected. Per se, not directly used in async - aside from computing the total number of steps.
  epochs: 1
  # Number of steps in each batch. Higher values will cause the inference step to be slower, but won't use more GPU memory.
  steps_per_batch: 16
  # Leave buffer_size empty to use steps_per_batch in async mode
  buffer_size:
  # Total number of dialog turns to collect during training.
  total_dialog_turns: 100_000
  # Batch size for optimization. Higher values will use more GPU memory.
  optim_batch_size: 1
  # Number of gradient accumulation steps. Higher values will use less GPU memory (comparing with bigger batches and lower gradient_accumulation_steps), 
  # but will make the optimization step slower.
  gradient_accumulation_steps: 4
  # Whether to include the KL coefficient in the loss function. Alternatively, the KL ref-to-train will be added to the reward.
  kl_coef_in_loss: true 
  # Update policy weights every N steps - can be set to any positive integer in async mode
  weight_update_frequency: 10
