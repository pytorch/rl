defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

# Environment configuration
env:
  dataset: ifeval  # choices: [gsm8k, ifeval]
  # Number of environments to run in parallel. This determines the batch size passed to vLLM.
  # More envs consume more GPU memory.
  num_envs: 2
  # Number of times to repeat the same prompt for GRPO. This does not affect the GPU memory usage.
  repeats: 16

# Base model configuration
model:
  name: Qwen/Qwen2.5-3B
  compile: false

# Training model configuration
train_model:
  gradient_checkpointing: true  # Only for training model
  devices: [0]  # List of GPU devices to use for training
  lora:
    enabled: true
    r: 8  # LoRA rank - controls capacity of adaptations
    alpha: 16  # LoRA alpha - scales the adaptations
    dropout: 0.1  # Dropout probability for LoRA layers
  quantization:
    enabled: false  # Quantization might interfere with training
  attn_implementation: sdpa  # choices: [flash_attention_2, flex_attention, sdpa]
  torch_dtype: bfloat16

# Inference model configuration (vLLM)
inference_model:
  devices: [1]  # List of GPU devices to use for inference
  gpu_memory_utilization: 0.5
  temperature: 0.8
  max_tokens: 2048
  include_stop_str_in_output: true

# Reference model configuration
ref_model:
  devices: [2]  # List of GPU devices to use for reference model
  quantization:
    enabled: false  # Enable quantization for memory efficiency
  gradient_checkpointing: false  # Not needed for reference model
  attn_implementation: 
  torch_dtype: bfloat16

# Policy configuration
policy:
  kl_coef: 1e-2

# Training configuration
train:
  epochs: 1
  # Number of dialog turns per batch. This is passed to the collector and buffer.
  # More steps do not consume more GPU memory, but it does affect the inference speed in
  # that in sync contexts the training node will need to wait for a batch to be completed
  # before starting the next one.
  steps_per_batch: 16
  # Total number of dialog turns to collect during training
  total_dialog_turns: 1_000_000
  # Number of batches to run in parallel. This determines the batch size passed to the optimizer.
  # More batches consume more GPU memory.
  optim_batch_size: 1
  # Number of gradient accumulation steps. This determines the number of steps to run before
  # updating the parameters.
  gradient_accumulation_steps: 4
  # Whether to include the KL coefficient in the loss or in the environment reward.
  kl_coef_in_loss: true
  # Whether to use mixed precision.
  mixed_precision: true  # Disable mixed precision since we're not using it
  optimizer:
    name: AdamW
    lr: 1e-5
    clip_grad_norm: 0.5

# Logging configuration
logging:
  checkpoint_dir: checkpoints
  experiment_name: null  # auto-generated if null
  checkpoint_frequency: 10  # save every N batches

hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num} 
