defaults:
  - _self_
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

# Environment configuration
env:
  dataset: gsm8k  # choices: [gsm8k, ifeval]
  # Number of environments to run in parallel. This determines the batch size passed to vLLM.
  #  More envs consume more GPU memory.
  num_envs: 4  # Reduced from 8 to save memory
  # Number of times to repeat the same prompt for GRPO. This does not affect the GPU memory usage.
  repeats: 16

# Base model configuration
model:
  name: Qwen/Qwen2.5-3B
  compile: false

# Training model configuration
train_model:
  gradient_checkpointing: true  # Enabled for memory efficiency
  lora:
    enabled: true  # Using LoRA for memory efficiency
    r: 8  # LoRA rank - controls capacity of adaptations
    alpha: 16  # LoRA alpha - scales the adaptations
    dropout: 0.1  # Dropout probability for LoRA layers
  quantization:
    enabled: false  # Enable 4-bit quantization for base model
  attn_implementation: flex_attention  # Using flash attention for memory efficiency
  torch_dtype: bfloat16

# Inference model configuration (vLLM)
inference_model:
  gpu_memory_utilization: 0.5
  temperature: 0.8
  max_tokens: 1024
  include_stop_str_in_output: true

# Reference model configuration
ref_model:
  quantization:
    enabled: false  # Enable quantization for memory efficiency
  gradient_checkpointing: false  # Not needed for reference model
  attn_implementation: 
  torch_dtype: bfloat16

# Policy configuration
policy:
  kl_coef: 1e-2

# Training configuration
train:
  epochs: 1
   # Number of dialog turns per batch. This is passed to the collector and buffer.
   #  More steps do not consume more GPU memory, but it does affect the inference speed in
   #  that in sync contexts the training node will need to wait for a batch to be completed
   #  before starting the next one.
  steps_per_batch: 64
  # Number of batches to run in parallel. This determines the batch size passed to the optimizer.
  #  More batches consume more GPU memory.
  optim_batch_size: 1
  # Number of gradient accumulation steps. This determines the number of steps to run before
  #  updating the parameters.
  gradient_accumulation_steps: 4  # Increased for gradient accumulation
  # Whether to include the KL coefficient in the loss or in the environment reward.
  kl_coef_in_loss: true
  # Whether to use mixed precision.
  mixed_precision: true  # Disable mixed precision since we're not using it
  optimizer:
    name: AdamW
    lr: 1e-5
    clip_grad_norm: 0.5

# Logging configuration
logging:
  checkpoint_dir: checkpoints
  experiment_name: null  # auto-generated if null
  checkpoint_frequency: 10  # save every N batches

hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num} 
