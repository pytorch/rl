{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Get started with Environments, TED and transforms\n\n**Author**: [Vincent Moens](https://github.com/vmoens)\n\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To run this tutorial in a notebook, add an installation cell\n  at the beginning containing:\n\n```\n!pip install tensordict\n!pip install torchrl</p></div>\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Welcome to the getting started tutorials!\n\nBelow is the list of the topics we will be covering.\n\n- `Environments, TED and transforms <gs_env_ted>`;\n- `TorchRL's modules <gs_modules>`;\n- `Losses and optimization <gs_optim>`;\n- `Data collection and storage <gs_storage>`;\n- `TorchRL's logging API <gs_logging>`.\n\nIf you are in a hurry, you can jump straight away to the last tutorial,\n`Your own first training loop <gs_first_training>`, from where you can\nbacktrack every other \"Getting Started\" tutorial if things are not clear or\nif you want to learn more about a specific topic!\n\n## Environments in RL\n\nThe standard RL (Reinforcement Learning) training loop involves a model,\nalso known as a policy, which is trained to accomplish a task within a\nspecific environment. Often, this environment is a simulator that accepts\nactions as input and produces an observation along with some metadata as\noutput.\n\nIn this document, we will explore the environment API of TorchRL: we will\nlearn how to create an environment, interact with it, and understand the\ndata format it uses.\n\n## Creating an environment\n\nIn essence, TorchRL does not directly provide environments, but instead\noffers wrappers for other libraries that encapsulate the simulators. The\n:mod:`~torchrl.envs` module can be viewed as a provider for a generic\nenvironment API, as well as a central hub for simulation backends like\n[gym](https://arxiv.org/abs/1606.01540) (:class:`~torchrl.envs.GymEnv`),\n[Brax](https://arxiv.org/abs/2106.13281) (:class:`~torchrl.envs.BraxEnv`)\nor [DeepMind Control Suite](https://arxiv.org/abs/1801.00690)\n(:class:`~torchrl.envs.DMControlEnv`).\n\nCreating your environment is typically as straightforward as the underlying\nbackend API allows. Here's an example using gym:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import GymEnv\n\nenv = GymEnv(\"Pendulum-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running an environment\n\nEnvironments in TorchRL have two crucial methods:\n:meth:`~torchrl.envs.EnvBase.reset`, which initiates\nan episode, and :meth:`~torchrl.envs.EnvBase.step`, which executes an\naction selected by the actor.\nIn TorchRL, environment methods read and write\n:class:`~tensordict.TensorDict` instances.\nEssentially, :class:`~tensordict.TensorDict` is a generic key-based data\ncarrier for tensors.\nThe benefit of using TensorDict over plain tensors is that it enables us to\nhandle simple and complex data structures interchangeably. As our function\nsignatures are very generic, it eliminates the challenge of accommodating\ndifferent data formats. In simpler terms, after this brief tutorial,\nyou will be capable of operating on both simple and highly complex\nenvironments, as their user-facing API is identical and simple!\n\nLet's put the environment into action and see what a tensordict instance\nlooks like:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reset = env.reset()\nprint(reset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's take a random action in the action space. First, sample the action:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reset_with_action = env.rand_action(reset)\nprint(reset_with_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tensordict has the same structure as the one obtained from\n:meth:`~torchrl.envs.EnvBase` with an additional ``\"action\"`` entry.\nYou can access the action easily, like you would do with a regular\ndictionary:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(reset_with_action[\"action\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now need to pass this action to the environment.\nWe'll be passing the entire tensordict to the ``step`` method, since there\nmight be more than one tensor to be read in more advanced cases like\nMulti-Agent RL or stateless environments:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stepped_data = env.step(reset_with_action)\nprint(stepped_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, this new tensordict is identical to the previous one except for the\nfact that it has a ``\"next\"`` entry (itself a tensordict!) containing the\nobservation, reward and done state resulting from\nour action.\n\nWe call this format TED, for\n`TorchRL Episode Data format <TED-format>`. It is\nthe ubiquitous way of representing data in the library, both dynamically like\nhere, or statically with offline datasets.\n\nThe last bit of information you need to run a rollout in the environment is\nhow to bring that ``\"next\"`` entry at the root to perform the next step.\nTorchRL provides a dedicated :func:`~torchrl.envs.utils.step_mdp` function\nthat does just that: it filters out the information you won't need and\ndelivers a data structure corresponding to your observation after a step in\nthe Markov Decision Process, or MDP.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import step_mdp\n\ndata = step_mdp(stepped_data)\nprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment rollouts\n\n\nWriting down those three steps (computing an action, making a step,\nmoving in the MDP) can be a bit tedious and repetitive. Fortunately,\nTorchRL provides a nice :meth:`~torchrl.envs.EnvBase.rollout` function that\nallows you to run them in a closed loop at will:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rollout = env.rollout(max_steps=10)\nprint(rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This data looks pretty much like the ``stepped_data`` above with the\nexception of its batch-size, which now equates the number of steps we\nprovided through the ``max_steps`` argument. The magic of tensordict\ndoesn't end there: if you're interested in a single transition of this\nenvironment, you can index the tensordict like you would index a tensor:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transition = rollout[3]\nprint(transition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`~tensordict.TensorDict` will automatically check if the index you\nprovided is a key (in which case we index along the key-dimension) or a\nspatial index like here.\n\nExecuted as such (without a policy), the ``rollout`` method may seem rather\nuseless: it just runs random actions. If a policy is available, it can\nbe passed to the method and used to collect data.\n\nNevertheless, it can useful to run a naive, policyless rollout at first to\ncheck what is to be expected from an environment at a glance.\n\nTo appreciate the versatility of TorchRL's API, consider the fact that the\nrollout method is universally applicable. It functions across **all** use\ncases, whether you're working with a single environment like this one,\nmultiple copies across various processes, a multi-agent environment, or even\na stateless version of it!\n\n\n## Transforming an environment\n\nMost of the time, you'll want to modify the output of the environment to\nbetter suit your requirements. For example, you might want to monitor the\nnumber of steps executed since the last reset, resize images, or stack\nconsecutive observations together.\n\nIn this section, we'll examine a simple transform, the\n:class:`~torchrl.envs.transforms.StepCounter` transform.\nThe complete list of transforms can be found\n`here <transforms>`.\n\nThe transform is integrated with the environment through a\n:class:`~torchrl.envs.transforms.TransformedEnv`:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import StepCounter, TransformedEnv\n\ntransformed_env = TransformedEnv(env, StepCounter(max_steps=10))\nrollout = transformed_env.rollout(max_steps=100)\nprint(rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, our environment now has one more entry, ``\"step_count\"`` that\ntracks the number of steps since the last reset.\nGiven that we passed the optional\nargument ``max_steps=10`` to the transform constructor, we also truncated the\ntrajectory after 10 steps (not completing a full rollout of 100 steps like\nwe asked with the ``rollout`` call). We can see that the trajectory was\ntruncated by looking at the truncated entry:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(rollout[\"next\", \"truncated\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is all for this short introduction to TorchRL's environment API!\n\n## Next steps\n\nTo explore further what TorchRL's environments can do, go and check:\n\n- The :meth:`~torchrl.envs.EnvBase.step_and_maybe_reset` method that packs\n  together :meth:`~torchrl.envs.EnvBase.step`,\n  :func:`~torchrl.envs.utils.step_mdp` and\n  :meth:`~torchrl.envs.EnvBase.reset`.\n- Some environments like :class:`~torchrl.envs.GymEnv` support rendering\n  through the ``from_pixels`` argument. Check the class docstrings to know\n  more!\n- The batched environments, in particular :class:`~torchrl.envs.ParallelEnv`\n  which allows you to run multiple copies of one same (or different!)\n  environments on multiple processes.\n- Design your own environment with the\n  `Pendulum tutorial <pendulum_tuto>` and learn about specs and\n  stateless environments.\n- See the more in-depth tutorial about environments\n  `in the dedicated tutorial <envs_tuto>`;\n- Check the\n  `multi-agent  environment API <MARL-environment-API>`\n  if you're interested in MARL;\n- TorchRL has many tools to interact with the Gym API such as\n  a way to register TorchRL envs in the Gym register through\n  :meth:`~torchrl.envs.EnvBase.register_gym`, an API to read\n  the info dictionaries through\n  :meth:`~torchrl.envs.EnvBase.set_info_dict_reader` or a way\n  to control the gym backend thanks to\n  :func:`~torchrl.envs.set_gym_backend`.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}