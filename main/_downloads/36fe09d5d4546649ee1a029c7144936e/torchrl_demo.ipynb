{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Introduction to TorchRL\nThis demo was presented at ICML 2022 on the industry demo day.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It gives a good overview of TorchRL functionalities. Feel free to reach out\nto vmoens@fb.com or submit issues if you have questions or comments about\nit.\n\nTorchRL is an open-source Reinforcement Learning (RL) library for PyTorch.\n\nhttps://github.com/pytorch/rl\n\nThe PyTorch ecosystem team (Meta) has decided to invest in that library to\nprovide a leading platform to develop RL solutions in research settings.\n\nIt provides pytorch and **python-first**, low and high level\n**abstractions** # for RL that are intended to be efficient, documented and\nproperly tested.\nThe code is aimed at supporting research in RL. Most of it is written in\npython in a highly modular way, such that researchers can easily swap\ncomponents, transform them or write new ones with little effort.\n\nThis repo attempts to align with the existing pytorch ecosystem libraries\nin that it has a dataset pillar (torchrl/envs), transforms, models, data\nutilities (e.g. collectors and containers), etc. TorchRL aims at having as\nfew dependencies as possible (python standard library, numpy and pytorch).\nCommon environment libraries (e.g. OpenAI gym) are only optional.\n\n**Content**:\n   .. aafig::\n\n      \"torchrl\"\n      \u2502\n      \u251c\u2500\u2500 \"collectors\"\n      \u2502   \u2514\u2500\u2500 \"collectors.py\"\n      \u2502   \u2502\n      \u2502   \u2514\u2500\u2500 \"distributed\"\n      \u2502       \u2514\u2500\u2500 \"default_configs.py\"\n      \u2502       \u2514\u2500\u2500 \"generic.py\"\n      \u2502       \u2514\u2500\u2500 \"ray.py\"\n      \u2502       \u2514\u2500\u2500 \"rpc.py\"\n      \u2502       \u2514\u2500\u2500 \"sync.py\"\n      \u251c\u2500\u2500 \"data\"\n      \u2502   \u2502\n      \u2502   \u251c\u2500\u2500 \"datasets\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"atari_dqn.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"d4rl.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"d4rl_infos.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"gen_dgrl.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"minari_data.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"openml.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"openx.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"roboset.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"vd4rl.py\"\n      \u2502   \u251c\u2500\u2500 \"postprocs\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"postprocs.py\"\n      \u2502   \u251c\u2500\u2500 \"replay_buffers\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"replay_buffers.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"samplers.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"storages.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"writers.py\"\n      \u2502   \u251c\u2500\u2500 \"llm\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"dataset.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"prompt.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"reward.py\"\n      \u2502   \u2514\u2500\u2500 \"tensor_specs.py\"\n      \u251c\u2500\u2500 \"envs\"\n      \u2502   \u2514\u2500\u2500 \"batched_envs.py\"\n      \u2502   \u2514\u2500\u2500 \"common.py\"\n      \u2502   \u2514\u2500\u2500 \"env_creator.py\"\n      \u2502   \u2514\u2500\u2500 \"gym_like.py\"\n      \u2502   \u251c\u2500\u2500 \"libs\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"brax.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"dm_control.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"envpool.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"gym.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"habitat.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"isaacgym.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"jumanji.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"openml.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"pettingzoo.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"robohive.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"smacv2.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"vmas.py\"\n      \u2502   \u251c\u2500\u2500 \"model_based\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"common.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"dreamer.py\"\n      \u2502   \u251c\u2500\u2500 \"transforms\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"functional.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"gym_transforms.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"r3m.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"llm.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"vc1.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"vip.py\"\n      \u2502   \u2514\u2500\u2500 \"vec_envs.py\"\n      \u251c\u2500\u2500 \"modules\"\n      \u2502   \u251c\u2500\u2500 \"distributions\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"continuous.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"discrete.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"truncated_normal.py\"\n      \u2502   \u251c\u2500\u2500 \"models\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"decision_transformer.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"exploration.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"model_based.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"models.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"multiagent.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"llm.py\"\n      \u2502   \u251c\u2500\u2500 \"planners\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"cem.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"common.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"mppi.py\"\n      \u2502   \u2514\u2500\u2500 \"tensordict_module\"\n      \u2502       \u2514\u2500\u2500 \"actors.py\"\n      \u2502       \u2514\u2500\u2500 \"common.py\"\n      \u2502       \u2514\u2500\u2500 \"exploration.py\"\n      \u2502       \u2514\u2500\u2500 \"probabilistic.py\"\n      \u2502       \u2514\u2500\u2500 \"rnn.py\"\n      \u2502       \u2514\u2500\u2500 \"sequence.py\"\n      \u2502       \u2514\u2500\u2500 \"world_models.py\"\n      \u251c\u2500\u2500 \"objectives\"\n      \u2502   \u2514\u2500\u2500 \"a2c.py\"\n      \u2502   \u2514\u2500\u2500 \"common.py\"\n      \u2502   \u2514\u2500\u2500 \"cql.py\"\n      \u2502   \u2514\u2500\u2500 \"ddpg.py\"\n      \u2502   \u2514\u2500\u2500 \"decision_transformer.py\"\n      \u2502   \u2514\u2500\u2500 \"deprecated.py\"\n      \u2502   \u2514\u2500\u2500 \"dqn.py\"\n      \u2502   \u2514\u2500\u2500 \"dreamer.py\"\n      \u2502   \u2514\u2500\u2500 \"functional.py\"\n      \u2502   \u2514\u2500\u2500 \"iql.py\"\n      \u2502   \u251c\u2500\u2500 \"multiagent\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"qmixer.py\"\n      \u2502   \u2514\u2500\u2500 \"ppo.py\"\n      \u2502   \u2514\u2500\u2500 \"redq.py\"\n      \u2502   \u2514\u2500\u2500 \"reinforce.py\"\n      \u2502   \u2514\u2500\u2500 \"sac.py\"\n      \u2502   \u2514\u2500\u2500 \"td3.py\"\n      \u2502   \u251c\u2500\u2500 \"value\"\n      \u2502       \u2514\u2500\u2500 \"advantages.py\"\n      \u2502       \u2514\u2500\u2500 \"functional.py\"\n      \u2502       \u2514\u2500\u2500 \"pg.py\"\n      \u251c\u2500\u2500 \"record\"\n      \u2502   \u251c\u2500\u2500 \"loggers\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"common.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"csv.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"mlflow.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"tensorboard.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"wandb.py\"\n      \u2502   \u2514\u2500\u2500 \"recorder.py\"\n      \u251c\u2500\u2500 \"trainers\"\n      \u2502   \u2502\n      \u2502   \u251c\u2500\u2500 \"helpers\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"collectors.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"envs.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"logger.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"losses.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"models.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"replay_buffer.py\"\n      \u2502   \u2502   \u2514\u2500\u2500 \"trainers.py\"\n      \u2502   \u2514\u2500\u2500 \"trainers.py\"\n      \u2514\u2500\u2500 \"version.py\"\n\nUnlike other domains, RL is less about media than *algorithms*. As such, it\nis harder to make truly independent components.\n\nWhat TorchRL is not:\n\n* a collection of algorithms: we do not intend to provide SOTA implementations of RL algorithms,\n  but we provide these algorithms only as examples of how to use the library.\n\n* a research framework: modularity in TorchRL comes in two flavors. First, we try\n  to build re-usable components, such that they can be easily swapped with each other.\n  Second, we make our best such that components can be used independently of the rest\n  of the library.\n\nTorchRL has very few core dependencies, predominantly PyTorch and numpy. All\nother dependencies (gym, torchvision, wandb / tensorboard) are optional.\n\n## Data\n\n### TensorDict\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom tensordict import TensorDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create a TensorDict. The constructor accepts many different formats, like passing a dict\nor with keyword arguments:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 5\ndata = TensorDict(\n    key1=torch.zeros(batch_size, 3),\n    key2=torch.zeros(batch_size, 5, 6, dtype=torch.bool),\n    batch_size=[batch_size],\n)\nprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can index a TensorDict along its ``batch_size``, as well as query keys.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data[2])\nprint(data[\"key1\"] is data.get(\"key1\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following shows how to stack multiple TensorDicts. This is particularly useful when writing rollout loops!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data1 = TensorDict(\n    {\n        \"key1\": torch.zeros(batch_size, 1),\n        \"key2\": torch.zeros(batch_size, 5, 6, dtype=torch.bool),\n    },\n    batch_size=[batch_size],\n)\n\ndata2 = TensorDict(\n    {\n        \"key1\": torch.ones(batch_size, 1),\n        \"key2\": torch.ones(batch_size, 5, 6, dtype=torch.bool),\n    },\n    batch_size=[batch_size],\n)\n\ndata = torch.stack([data1, data2], 0)\ndata.batch_size, data[\"key1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are some other functionalities of TensorDict: viewing, permute, sharing memory or expanding.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    \"view(-1): \",\n    data.view(-1).batch_size,\n    data.view(-1).get(\"key1\").shape,\n)\n\nprint(\"to device: \", data.to(\"cpu\"))\n\n# print(\"pin_memory: \", data.pin_memory())\n\nprint(\"share memory: \", data.share_memory_())\n\nprint(\n    \"permute(1, 0): \",\n    data.permute(1, 0).batch_size,\n    data.permute(1, 0).get(\"key1\").shape,\n)\n\nprint(\n    \"expand: \",\n    data.expand(3, *data.batch_size).batch_size,\n    data.expand(3, *data.batch_size).get(\"key1\").shape,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can create a **nested data** as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = TensorDict(\n    source={\n        \"key1\": torch.zeros(batch_size, 3),\n        \"key2\": TensorDict(\n            source={\"sub_key1\": torch.zeros(batch_size, 2, 1)},\n            batch_size=[batch_size, 2],\n        ),\n    },\n    batch_size=[batch_size],\n)\ndata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replay buffers\n\n`Replay buffers <ref_buffers>` are a crucial component in many RL algorithms. TorchRL provides a range of replay buffer implementations.\nMost basic features will work with any data scturcture (list, tuples, dict) but to use the replay buffers to their\nfull extend and with fast read and write access, TensorDict APIs should be preferred.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.data import PrioritizedReplayBuffer, ReplayBuffer\n\nrb = ReplayBuffer(collate_fn=lambda x: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding can be done with :meth:`~torchrl.data.ReplayBuffer.add` (n=1)\nor :meth:`~torchrl.data.ReplayBuffer.extend` (n>1).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rb.add(1)\nrb.sample(1)\nrb.extend([2, 3])\nrb.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prioritized Replay Buffers can also be used:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rb = PrioritizedReplayBuffer(alpha=0.7, beta=1.1, collate_fn=lambda x: x)\nrb.add(1)\nrb.sample(1)\nrb.update_priority(1, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are examples of using a replaybuffer with data_stack.\nUsing them makes it easy to abstract away the behaviour of the replay buffer for multiple use cases.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "collate_fn = torch.stack\nrb = ReplayBuffer(collate_fn=collate_fn)\nrb.add(TensorDict({\"a\": torch.randn(3)}, batch_size=[]))\nlen(rb)\n\nrb.extend(TensorDict({\"a\": torch.randn(2, 3)}, batch_size=[2]))\nprint(len(rb))\nprint(rb.sample(10))\nprint(rb.sample(2).contiguous())\n\ntorch.manual_seed(0)\nfrom torchrl.data import TensorDictPrioritizedReplayBuffer\n\nrb = TensorDictPrioritizedReplayBuffer(alpha=0.7, beta=1.1, priority_key=\"td_error\")\nrb.extend(TensorDict({\"a\": torch.randn(2, 3)}, batch_size=[2]))\ndata_sample = rb.sample(2).contiguous()\nprint(data_sample)\n\nprint(data_sample[\"index\"])\n\ndata_sample[\"td_error\"] = torch.rand(2)\nrb.update_tensordict_priority(data_sample)\n\nfor i, val in enumerate(rb._sampler._sum_tree):\n    print(i, val)\n    if i == len(rb):\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Envs\nTorchRL provides a range of `environment <Environment-API>` wrappers and utilities.\n\n### Gym Environment\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    import gymnasium as gym\nexcept ModuleNotFoundError:\n    import gym\n\nfrom torchrl.envs.libs.gym import GymEnv, GymWrapper, set_gym_backend\n\ngym_env = gym.make(\"Pendulum-v1\")\nenv = GymWrapper(gym_env)\nenv = GymEnv(\"Pendulum-v1\")\n\ndata = env.reset()\nenv.rand_step(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Changing environments config\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\nenv.reset()\n\nenv.close()\ndel env\n\nfrom torchrl.envs import (\n    Compose,\n    NoopResetEnv,\n    ObservationNorm,\n    ToTensorImage,\n    TransformedEnv,\n)\n\nbase_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\nenv = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\nenv.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Transforms\n\nTransforms act like Gym wrappers but with an API closer to torchvision's ``torch.distributions``' transforms.\nThere is a wide range of `transforms <transforms>` to choose from.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import (\n    Compose,\n    NoopResetEnv,\n    ObservationNorm,\n    StepCounter,\n    ToTensorImage,\n    TransformedEnv,\n)\n\nbase_env = GymEnv(\"HalfCheetah-v4\", frame_skip=3, from_pixels=True, pixels_only=False)\nenv = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\nenv = env.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\n\nenv.reset()\n\nprint(\"env: \", env)\nprint(\"last transform parent: \", env.transform[2].parent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vectorized Environments\n\nVectorized / parallel environments can provide some significant speed-ups.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import ParallelEnv\n\n\ndef make_env():\n    # You can control whether to use gym or gymnasium for your env\n    with set_gym_backend(\"gym\"):\n        return GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\n\n\nbase_env = ParallelEnv(\n    4,\n    make_env,\n    mp_start_method=\"fork\",  # This will break on Windows machines! Remove and decorate with if __name__ == \"__main__\"\n)\nenv = TransformedEnv(\n    base_env, Compose(StepCounter(), ToTensorImage())\n)  # applies transforms on batch of envs\nenv.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\nenv.reset()\n\nprint(env.action_spec)\n\nenv.close()\ndel env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modules\n\nMultiple `modules <ref_modules>`  (utils, models and wrappers) can be found in the library.\n\n### Models\n\nExample of a MLP model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn\nfrom torchrl.modules import ConvNet, MLP\nfrom torchrl.modules.models.utils import SquashDims\n\nnet = MLP(num_cells=[32, 64], out_features=4, activation_class=nn.ELU)\nprint(net)\nprint(net(torch.randn(10, 3)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example of a CNN model:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn = ConvNet(\n    num_cells=[32, 64],\n    kernel_sizes=[8, 4],\n    strides=[2, 1],\n    aggregator_class=SquashDims,\n)\nprint(cnn)\nprint(cnn(torch.randn(10, 3, 32, 32)).shape)  # last tensor is squashed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorDictModules\n\n`Some modules <tdmodules>` are specifically designed to work with tensordict inputs.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import TensorDictModule\n\ndata = TensorDict({\"key1\": torch.randn(10, 3)}, batch_size=[10])\nmodule = nn.Linear(3, 4)\ntd_module = TensorDictModule(module, in_keys=[\"key1\"], out_keys=[\"key2\"])\ntd_module(data)\nprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sequences of Modules\n\nMaking sequences of modules is made easy by :class:`~tensordict.nn.TensorDictSequential`:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import TensorDictSequential\n\nbackbone_module = nn.Linear(5, 3)\nbackbone = TensorDictModule(\n    backbone_module, in_keys=[\"observation\"], out_keys=[\"hidden\"]\n)\nactor_module = nn.Linear(3, 4)\nactor = TensorDictModule(actor_module, in_keys=[\"hidden\"], out_keys=[\"action\"])\nvalue_module = MLP(out_features=1, num_cells=[4, 5])\nvalue = TensorDictModule(value_module, in_keys=[\"hidden\", \"action\"], out_keys=[\"value\"])\n\nsequence = TensorDictSequential(backbone, actor, value)\nprint(sequence)\n\nprint(sequence.in_keys, sequence.out_keys)\n\ndata = TensorDict(\n    {\"observation\": torch.randn(3, 5)},\n    [3],\n)\nbackbone(data)\nactor(data)\nvalue(data)\n\ndata = TensorDict(\n    {\"observation\": torch.randn(3, 5)},\n    [3],\n)\nsequence(data)\nprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functional Programming (Ensembling / Meta-RL)\n\nFunctional calls have never been easier. Extract the parameters with :func:`~tensordict.from_module`, and\nreplace them with :meth:`~tensordict.TensorDict.to_module`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict import from_module\n\nparams = from_module(sequence)\nprint(\"extracted params\", params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "functional call using tensordict:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with params.to_module(sequence):\n    data = sequence(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VMAP\n\nFast execution of multiple copies of a similar architecture is key to train your models fast.\n:func:`~torch.vmap` is tailored to do just that:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import vmap\n\nparams_expand = params.expand(4)\n\n\ndef exec_sequence(params, data):\n    with params.to_module(sequence):\n        return sequence(data)\n\n\ntensordict_exp = vmap(exec_sequence, (0, None))(params_expand, data)\nprint(tensordict_exp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specialized Classes\n\nTorchRL provides also some specialized modules that run checks on the output values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\nfrom torchrl.data import Bounded\nfrom torchrl.modules import SafeModule\n\nspec = Bounded(-torch.ones(3), torch.ones(3))\nbase_module = nn.Linear(5, 3)\nmodule = SafeModule(\n    module=base_module, spec=spec, in_keys=[\"obs\"], out_keys=[\"action\"], safe=True\n)\ndata = TensorDict({\"obs\": torch.randn(5)}, batch_size=[])\nmodule(data)[\"action\"]\n\ndata = TensorDict({\"obs\": torch.randn(5) * 100}, batch_size=[])\nmodule(data)[\"action\"]  # safe=True projects the result within the set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`~torchrl.modules.Actor` class has has a predefined output key (``\"action\"``):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.modules import Actor\n\nbase_module = nn.Linear(5, 3)\nactor = Actor(base_module, in_keys=[\"obs\"])\ndata = TensorDict({\"obs\": torch.randn(5)}, batch_size=[])\nactor(data)  # action is the default value\n\nfrom tensordict.nn import (\n    ProbabilisticTensorDictModule,\n    ProbabilisticTensorDictSequential,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Working with probabilistic models is also made easy thanks to the ``tensordict.nn`` API:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.modules import NormalParamExtractor, TanhNormal\n\ntd = TensorDict({\"input\": torch.randn(3, 5)}, [3])\nnet = nn.Sequential(\n    nn.Linear(5, 4), NormalParamExtractor()\n)  # splits the output in loc and scale\nmodule = TensorDictModule(net, in_keys=[\"input\"], out_keys=[\"loc\", \"scale\"])\ntd_module = ProbabilisticTensorDictSequential(\n    module,\n    ProbabilisticTensorDictModule(\n        in_keys=[\"loc\", \"scale\"],\n        out_keys=[\"action\"],\n        distribution_class=TanhNormal,\n        return_log_prob=False,\n    ),\n)\ntd_module(td)\nprint(td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# returning the log-probability\ntd = TensorDict({\"input\": torch.randn(3, 5)}, [3])\ntd_module = ProbabilisticTensorDictSequential(\n    module,\n    ProbabilisticTensorDictModule(\n        in_keys=[\"loc\", \"scale\"],\n        out_keys=[\"action\"],\n        distribution_class=TanhNormal,\n        return_log_prob=True,\n    ),\n)\ntd_module(td)\nprint(td)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Controlling randomness and sampling strategies is achieved via a context manager,\n:class:`~torchrl.envs.set_exploration_type`:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.utils import ExplorationType, set_exploration_type\n\ntd = TensorDict({\"input\": torch.randn(3, 5)}, [3])\n\ntorch.manual_seed(0)\nwith set_exploration_type(ExplorationType.RANDOM):\n    td_module(td)\n    print(\"random:\", td[\"action\"])\n\nwith set_exploration_type(ExplorationType.DETERMINISTIC):\n    td_module(td)\n    print(\"mode:\", td[\"action\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Environments and Modules\n\nLet us see how environments and modules can be combined:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.utils import step_mdp\n\nenv = GymEnv(\"Pendulum-v1\")\n\naction_spec = env.action_spec\nactor_module = nn.Linear(3, 1)\nactor = SafeModule(\n    actor_module, spec=action_spec, in_keys=[\"observation\"], out_keys=[\"action\"]\n)\n\ntorch.manual_seed(0)\nenv.set_seed(0)\n\nmax_steps = 100\ndata = env.reset()\ndata_stack = TensorDict(batch_size=[max_steps])\nfor i in range(max_steps):\n    actor(data)\n    data_stack[i] = env.step(data)\n    if data[\"done\"].any():\n        break\n    data = step_mdp(data)  # roughly equivalent to obs = next_obs\n\ntensordicts_prealloc = data_stack.clone()\nprint(\"total steps:\", i)\nprint(data_stack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# equivalent\ntorch.manual_seed(0)\nenv.set_seed(0)\n\nmax_steps = 100\ndata = env.reset()\ndata_stack = []\nfor _ in range(max_steps):\n    actor(data)\n    data_stack.append(env.step(data))\n    if data[\"done\"].any():\n        break\n    data = step_mdp(data)  # roughly equivalent to obs = next_obs\ntensordicts_stack = torch.stack(data_stack, 0)\nprint(\"total steps:\", i)\nprint(tensordicts_stack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(tensordicts_stack == tensordicts_prealloc).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\nenv.set_seed(0)\ntensordict_rollout = env.rollout(policy=actor, max_steps=max_steps)\ntensordict_rollout\n\n\n(tensordict_rollout == tensordicts_prealloc).all()\n\nfrom tensordict.nn import TensorDictModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collectors\n\nWe also provide a set of `data collectors <ref_collectors>`, that automaticall gather as many frames per batch as required.\nThey work from single-node, single worker to multi-nodes, multi-workers settings.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.collectors import MultiaSyncDataCollector, MultiSyncDataCollector\n\nfrom torchrl.envs import EnvCreator, SerialEnv\nfrom torchrl.envs.libs.gym import GymEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EnvCreator makes sure that we can send a lambda function from process to process\nWe use a :class:`~torchrl.envs.SerialEnv` for simplicity (single worker), but for larger jobs a\n:class:`~torchrl.envs.ParallelEnv` (multi-workers) would be better suited.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Multiprocessed envs and multiprocessed collectors can be combined!</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parallel_env = SerialEnv(\n    3,\n    EnvCreator(lambda: GymEnv(\"Pendulum-v1\")),\n)\ncreate_env_fn = [parallel_env, parallel_env]\n\nactor_module = nn.Linear(3, 1)\nactor = TensorDictModule(actor_module, in_keys=[\"observation\"], out_keys=[\"action\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sync multiprocessed data collector\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "devices = [\"cpu\", \"cpu\"]\n\ncollector = MultiSyncDataCollector(\n    create_env_fn=create_env_fn,  # either a list of functions or a ParallelEnv\n    policy=actor,\n    total_frames=240,\n    max_frames_per_traj=-1,  # envs are terminating, we don't need to stop them early\n    frames_per_batch=60,  # we want 60 frames at a time (we have 3 envs per sub-collector)\n    device=devices,\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, d in enumerate(collector):\n    if i == 0:\n        print(d)  # trajectories are split automatically in [6 workers x 10 steps]\n    collector.update_policy_weights_()  # make sure that our policies have the latest weights if working on multiple devices\nprint(i)\ncollector.shutdown()\ndel collector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Async multiprocessed data collector\n\nThis class allows you to collect data while the model is training. This is particularily useful in off-policy settings\nas it decouples the inference and the model trainning. Data is delived in a first-ready-first-served basis (workers\nwill queue their results):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "collector = MultiaSyncDataCollector(\n    create_env_fn=create_env_fn,  # either a list of functions or a ParallelEnv\n    policy=actor,\n    total_frames=240,\n    max_frames_per_traj=-1,  # envs are terminating, we don't need to stop them early\n    frames_per_batch=60,  # we want 60 frames at a time (we have 3 envs per sub-collector)\n    device=devices,\n)\n\nfor i, d in enumerate(collector):\n    if i == 0:\n        print(d)  # trajectories are split automatically in [6 workers x 10 steps]\n    collector.update_policy_weights_()  # make sure that our policies have the latest weights if working on multiple devices\nprint(i)\ncollector.shutdown()\ndel collector\ndel create_env_fn\ndel parallel_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objectives\n`Objectives <ref_objectives>` are the main entry points when coding up a new algorithm.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.objectives import DDPGLoss\n\nactor_module = nn.Linear(3, 1)\nactor = TensorDictModule(actor_module, in_keys=[\"observation\"], out_keys=[\"action\"])\n\n\nclass ConcatModule(nn.Linear):\n    def forward(self, obs, action):\n        return super().forward(torch.cat([obs, action], -1))\n\n\nvalue_module = ConcatModule(4, 1)\nvalue = TensorDictModule(\n    value_module, in_keys=[\"observation\", \"action\"], out_keys=[\"state_action_value\"]\n)\n\nloss_fn = DDPGLoss(actor, value)\nloss_fn.make_value_estimator(loss_fn.default_value_estimator, gamma=0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = TensorDict(\n    {\n        \"observation\": torch.randn(10, 3),\n        \"next\": {\n            \"observation\": torch.randn(10, 3),\n            \"reward\": torch.randn(10, 1),\n            \"done\": torch.zeros(10, 1, dtype=torch.bool),\n        },\n        \"action\": torch.randn(10, 1),\n    },\n    batch_size=[10],\n    device=\"cpu\",\n)\nloss_td = loss_fn(data)\n\nprint(loss_td)\n\nprint(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing the Library\n\nThe library is on PyPI: *pip install torchrl*\nSee the [README](https://github.com/pytorch/rl/blob/main/README.md) for more information.\n\n## Contributing\n\nWe are actively looking for contributors and early users. If you're working in\nRL (or just curious), try it! Give us feedback: what will make the success of\nTorchRL is how well it covers researchers needs. To do that, we need their input!\nSince the library is nascent, it is a great time for you to shape it the way you want!\n\nSee the [Contributing guide](https://github.com/pytorch/rl/blob/main/CONTRIBUTING.md) for more info.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}