{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Get started with your own first training loop\n\n**Author**: [Vincent Moens](https://github.com/vmoens)\n\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To run this tutorial in a notebook, add an installation cell\n  at the beginning containing:\n\n```\n!pip install tensordict\n!pip install torchrl</p></div>\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Time to wrap up everything we've learned so far in this Getting Started\nseries!\n\nIn this tutorial, we will be writing the most basic training loop there is\nusing only components we have presented in the previous lessons.\n\nWe'll be using DQN with a CartPole environment as a prototypical example.\n\nWe will be voluntarily keeping the verbosity to its minimum, only linking\neach section to the related tutorial.\n\n## Building the environment\n\nWe'll be using a gym environment with a :class:`~torchrl.envs.transforms.StepCounter`\ntransform. If you need a refresher, check our these features are presented in\n`the environment tutorial <gs_env_ted>`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\ntorch.manual_seed(0)\n\nimport time\n\nfrom torchrl.envs import GymEnv, StepCounter, TransformedEnv\n\nenv = TransformedEnv(GymEnv(\"CartPole-v1\"), StepCounter())\nenv.set_seed(0)\n\nfrom tensordict.nn import TensorDictModule as Mod, TensorDictSequential as Seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Designing a policy\n\nThe next step is to build our policy.\nWe'll be making a regular, deterministic\nversion of the actor to be used within the\n`loss module <gs_optim>` and during\n`evaluation <gs_logging>`.\nNext, we will augment it with an exploration module\nfor `inference <gs_storage>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.modules import EGreedyModule, MLP, QValueModule\n\nvalue_mlp = MLP(out_features=env.action_spec.shape[-1], num_cells=[64, 64])\nvalue_net = Mod(value_mlp, in_keys=[\"observation\"], out_keys=[\"action_value\"])\npolicy = Seq(value_net, QValueModule(spec=env.action_spec))\nexploration_module = EGreedyModule(\n    env.action_spec, annealing_num_steps=100_000, eps_init=0.5\n)\npolicy_explore = Seq(policy, exploration_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Collector and replay buffer\n\nHere comes the data part: we need a\n`data collector <gs_storage_collector>` to easily get batches of data\nand a `replay buffer <gs_storage_rb>` to store that data for training.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.collectors import SyncDataCollector\nfrom torchrl.data import LazyTensorStorage, ReplayBuffer\n\ninit_rand_steps = 5000\nframes_per_batch = 100\noptim_steps = 10\ncollector = SyncDataCollector(\n    env,\n    policy_explore,\n    frames_per_batch=frames_per_batch,\n    total_frames=-1,\n    init_random_frames=init_rand_steps,\n)\nrb = ReplayBuffer(storage=LazyTensorStorage(100_000))\n\nfrom torch.optim import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss module and optimizer\n\nWe build our loss as indicated in the `dedicated tutorial <gs_optim>`, with\nits optimizer and target parameter updater:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.objectives import DQNLoss, SoftUpdate\n\nloss = DQNLoss(value_network=policy, action_space=env.action_spec, delay_value=True)\noptim = Adam(loss.parameters(), lr=0.02)\nupdater = SoftUpdate(loss, eps=0.99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logger\n\nWe'll be using a CSV logger to log our results, and save rendered videos.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl._utils import logger as torchrl_logger\nfrom torchrl.record import CSVLogger, VideoRecorder\n\npath = \"./training_loop\"\nlogger = CSVLogger(exp_name=\"dqn\", log_dir=path, video_format=\"mp4\")\nvideo_recorder = VideoRecorder(logger, tag=\"video\")\nrecord_env = TransformedEnv(\n    GymEnv(\"CartPole-v1\", from_pixels=True, pixels_only=False), video_recorder\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n\nInstead of fixing a specific number of iterations to run, we will keep on\ntraining the network until it reaches a certain performance (arbitrarily\ndefined as 200 steps in the environment -- with CartPole, success is defined\nas having longer trajectories).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_count = 0\ntotal_episodes = 0\nt0 = time.time()\nfor i, data in enumerate(collector):\n    # Write data in replay buffer\n    rb.extend(data)\n    max_length = rb[:][\"next\", \"step_count\"].max()\n    if len(rb) > init_rand_steps:\n        # Optim loop (we do several optim steps\n        # per batch collected for efficiency)\n        for _ in range(optim_steps):\n            sample = rb.sample(128)\n            loss_vals = loss(sample)\n            loss_vals[\"loss\"].backward()\n            optim.step()\n            optim.zero_grad()\n            # Update exploration factor\n            exploration_module.step(data.numel())\n            # Update target params\n            updater.step()\n            if i % 10:\n                torchrl_logger.info(f\"Max num steps: {max_length}, rb length {len(rb)}\")\n            total_count += data.numel()\n            total_episodes += data[\"next\", \"done\"].sum()\n    if max_length > 200:\n        break\n\nt1 = time.time()\n\ntorchrl_logger.info(\n    f\"solved after {total_count} steps, {total_episodes} episodes and in {t1-t0}s.\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rendering\n\nFinally, we run the environment for as many steps as we can and save the\nvideo locally (notice that we are not exploring).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "record_env.rollout(max_steps=1000, policy=policy)\nvideo_recorder.dump()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is what your rendered CartPole video will look like after a full\ntraining loop:\n\n.. figure:: /_static/img/cartpole.gif\n\nThis concludes our series of \"Getting started with TorchRL\" tutorials!\nFeel free to share feedback about it on GitHub.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.24"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}