{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using pretrained models\nThis tutorial explains how to use pretrained models in TorchRL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tempfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At the end of this tutorial, you will be capable of using pretrained models\nfor efficient image representation, and fine-tune them.\n\nTorchRL provides pretrained models that are to be used either as transforms or as\ncomponents of the policy. As the semantic is the same, they can be used interchangeably\nin one or the other context. In this tutorial, we will be using R3M (https://arxiv.org/abs/2203.12601),\nbut other models (e.g. VIP) will work equally well.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.cuda\nfrom tensordict.nn import TensorDictSequential\nfrom torch import nn\nfrom torchrl.envs import Compose, R3MTransform, TransformedEnv\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.modules import Actor\n\nis_fork = multiprocessing.get_start_method() == \"fork\"\ndevice = (\n    torch.device(0)\n    if torch.cuda.is_available() and not is_fork\n    else torch.device(\"cpu\")\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us first create an environment. For the sake of simplicity, we will be using\na common gym environment. In practice, this will work in more challenging, embodied\nAI contexts (e.g. have a look at our Habitat wrappers).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "base_env = GymEnv(\"Ant-v4\", from_pixels=True, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us fetch our pretrained model. We ask for the pretrained version of the model through the\ndownload=True flag. By default this is turned off.\nNext, we will append our transform to the environment. In practice, what will happen is that\neach batch of data collected will go through the transform and be mapped on a \"r3m_vec\" entry\nin the output tensordict. Our policy, consisting of a single layer MLP, will then read this vector and compute\nthe corresponding action.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r3m = R3MTransform(\n    \"resnet50\",\n    in_keys=[\"pixels\"],\n    download=False,  # Turn to true for real-life testing\n)\nenv_transformed = TransformedEnv(base_env, r3m)\nnet = nn.Sequential(\n    nn.LazyLinear(128, device=device),\n    nn.Tanh(),\n    nn.Linear(128, base_env.action_spec.shape[-1], device=device),\n)\npolicy = Actor(net, in_keys=[\"r3m_vec\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the number of parameters of the policy:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"number of params:\", len(list(policy.parameters())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We collect a rollout of 32 steps and print its output:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rollout = env_transformed.rollout(32, policy)\nprint(\"rollout with transform:\", rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For fine tuning, we integrate the transform in the policy after making the parameters\ntrainable. In practice, it may be wiser to restrict this to a subset of the parameters (say the last layer\nof the MLP).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r3m.train()\npolicy = TensorDictSequential(r3m, policy)\nprint(\"number of params after r3m is integrated:\", len(list(policy.parameters())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, we collect a rollout with R3M. The structure of the output has changed slightly, as now\nthe environment returns pixels (and not an embedding). The embedding \"r3m_vec\" is an intermediate\nresult of our policy.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rollout = base_env.rollout(32, policy)\nprint(\"rollout, fine tuning:\", rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The easiness with which we have swapped the transform from the env to the policy\nis due to the fact that both behave like TensorDictModule: they have a set of `\"in_keys\"` and\n`\"out_keys\"` that make it easy to read and write output in different context.\n\nTo conclude this tutorial, let's have a look at how we could use R3M to read\nimages stored in a replay buffer (e.g. in an offline RL context). First, let's build our dataset:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.data import LazyMemmapStorage, ReplayBuffer\n\nbuffer_scratch_dir = tempfile.TemporaryDirectory().name\nstorage = LazyMemmapStorage(1000, scratch_dir=buffer_scratch_dir)\nrb = ReplayBuffer(storage=storage, transform=Compose(lambda td: td.to(device), r3m))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now collect the data (random rollouts for our purpose) and fill the replay\nbuffer with it:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total = 0\nwhile total < 1000:\n    tensordict = base_env.rollout(1000)\n    rb.extend(tensordict)\n    total += tensordict.numel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check what our replay buffer storage looks like. It should not contain the \"r3m_vec\" entry\nsince we haven't used it yet:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"stored data:\", storage._storage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When sampling, the data will go through the R3M transform, giving us the processed data that we wanted.\nIn this way, we can train an algorithm offline on a dataset made of images:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = rb.sample(32)\nprint(\"data after sampling:\", batch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.24"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}