{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Recurrent DQN: Training recurrent policies\n\n**Author**: [Vincent Moens](https://github.com/vmoens)\n\n\n.. grid:: 2\n\n    .. grid-item-card:: :octicon:`mortar-board;1em;` What you will learn\n\n      * How to incorporating an RNN in an actor in TorchRL\n      * How to use that memory-based policy with a replay buffer and a loss module\n\n    .. grid-item-card:: :octicon:`list-unordered;1em;` Prerequisites\n\n      * PyTorch v2.0.0\n      * gym[mujoco]\n      * tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tempfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n\nMemory-based policies are crucial not only when the observations are partially\nobservable but also when the time dimension must be taken into account to\nmake informed decisions.\n\nRecurrent neural networks have long been a popular tool for memory-based\npolicies. The idea is to keep a recurrent state in memory between two\nconsecutive steps, and use this as an input to the policy along with the\ncurrent observation.\n\nThis tutorial shows how to incorporate an RNN in a policy using TorchRL.\n\nKey learnings:\n\n- Incorporating an RNN in an actor in TorchRL;\n- Using that memory-based policy with a replay buffer and a loss module.\n\nThe core idea of using RNNs in TorchRL is to use TensorDict as a data carrier\nfor the hidden states from one step to another. We'll build a policy that\nreads the previous recurrent state from the current TensorDict, and writes the\ncurrent recurrent states in the TensorDict of the next state:\n\n.. figure:: /_static/img/rollout_recurrent.png\n   :alt: Data collection with a recurrent policy\n\nAs this figure shows, our environment populates the TensorDict with zeroed recurrent\nstates which are read by the policy together with the observation to produce an\naction, and recurrent states that will be used for the next step.\nWhen the :func:`~torchrl.envs.utils.step_mdp` function is called, the recurrent states\nfrom the next state are brought to the current TensorDict. Let's see how this\nis implemented in practice.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are running this in Google Colab, make sure you install the following dependencies:\n\n```bash\n!pip3 install torchrl\n!pip3 install gym[mujoco]\n!pip3 install tqdm\n```\n## Setup\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport tqdm\nfrom tensordict.nn import (\n    TensorDictModule as Mod,\n    TensorDictSequential,\n    TensorDictSequential as Seq,\n)\nfrom torch import nn\nfrom torchrl.collectors import SyncDataCollector\nfrom torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer\nfrom torchrl.data.replay_buffers.samplers import SliceSampler\nfrom torchrl.envs import (\n    Compose,\n    ExplorationType,\n    GrayScale,\n    InitTracker,\n    ObservationNorm,\n    Resize,\n    RewardScaling,\n    set_exploration_type,\n    StepCounter,\n    ToTensorImage,\n    TransformedEnv,\n)\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.modules import ConvNet, EGreedyModule, LSTMModule, MLP, QValueModule\nfrom torchrl.objectives import DQNLoss, SoftUpdate\n\nis_fork = multiprocessing.get_start_method() == \"fork\"\ndevice = (\n    torch.device(0)\n    if torch.cuda.is_available() and not is_fork\n    else torch.device(\"cpu\")\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment\n\nAs usual, the first step is to build our environment: it helps us\ndefine the problem and build the policy network accordingly. For this tutorial,\nwe'll be running a single pixel-based instance of the CartPole gym\nenvironment with some custom transforms: turning to grayscale, resizing to\n84x84, scaling down the rewards and normalizing the observations.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The :class:`~torchrl.envs.transforms.StepCounter` transform is accessory. Since the CartPole\n  task goal is to make trajectories as long as possible, counting the steps\n  can help us track the performance of our policy.</p></div>\n\nTwo transforms are important for the purpose of this tutorial:\n\n- :class:`~torchrl.envs.transforms.InitTracker` will stamp the\n  calls to :meth:`~torchrl.envs.EnvBase.reset` by adding a ``\"is_init\"``\n  boolean mask in the TensorDict that will track which steps require a reset\n  of the RNN hidden states.\n- The :class:`~torchrl.envs.transforms.TensorDictPrimer` transform is a bit more\n  technical. It is not required to use RNN policies. However, it\n  instructs the environment (and subsequently the collector) that some extra\n  keys are to be expected. Once added, a call to `env.reset()` will populate\n  the entries indicated in the primer with zeroed tensors. Knowing that\n  these tensors are expected by the policy, the collector will pass them on\n  during collection. Eventually, we'll be storing our hidden states in the\n  replay buffer, which will help us bootstrap the computation of the\n  RNN operations in the loss module (which would otherwise be initiated\n  with 0s). In summary: not including this transform will not impact hugely\n  the training of our policy, but it will make the recurrent keys disappear\n  from the collected data and the replay buffer, which will in turn lead to\n  a slightly less optimal training.\n  Fortunately, the :class:`~torchrl.modules.LSTMModule` we propose is\n  equipped with a helper method to build just that transform for us, so\n  we can wait until we build it!\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = TransformedEnv(\n    GymEnv(\"CartPole-v1\", from_pixels=True, device=device),\n    Compose(\n        ToTensorImage(),\n        GrayScale(),\n        Resize(84, 84),\n        StepCounter(),\n        InitTracker(),\n        RewardScaling(loc=0.0, scale=0.1),\n        ObservationNorm(standard_normal=True, in_keys=[\"pixels\"]),\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As always, we need to initialize manually our normalization constants:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.transform[-1].init_stats(1000, reduce_dim=[0, 1, 2], cat_dim=0, keep_dims=[0])\ntd = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy\n\nOur policy will have 3 components: a :class:`~torchrl.modules.ConvNet`\nbackbone, an :class:`~torchrl.modules.LSTMModule` memory layer and a shallow\n:class:`~torchrl.modules.MLP` block that will map the LSTM output onto the\naction values.\n\n### Convolutional network\n\nWe build a convolutional network flanked with a :class:`torch.nn.AdaptiveAvgPool2d`\nthat will squash the output in a vector of size 64. The :class:`~torchrl.modules.ConvNet`\ncan assist us with this:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature = Mod(\n    ConvNet(\n        num_cells=[32, 32, 64],\n        squeeze_output=True,\n        aggregator_class=nn.AdaptiveAvgPool2d,\n        aggregator_kwargs={\"output_size\": (1, 1)},\n        device=device,\n    ),\n    in_keys=[\"pixels\"],\n    out_keys=[\"embed\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we execute the first module on a batch of data to gather the size of the\noutput vector:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_cells = feature(env.reset())[\"embed\"].shape[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSTM Module\n\nTorchRL provides a specialized :class:`~torchrl.modules.LSTMModule` class\nto incorporate LSTMs in your code-base. It is a :class:`~tensordict.nn.TensorDictModuleBase`\nsubclass: as such, it has a set of ``in_keys`` and ``out_keys`` that indicate\nwhat values should be expected to be read and written/updated during the\nexecution of the module. The class comes with customizable predefined\nvalues for these attributes to facilitate its construction.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>*Usage limitations*: The class supports almost all LSTM features such as\n  dropout or multi-layered LSTMs.\n  However, to respect TorchRL's conventions, this LSTM must have the ``batch_first``\n  attribute set to ``True`` which is **not** the default in PyTorch. However,\n  our :class:`~torchrl.modules.LSTMModule` changes this default\n  behavior, so we're good with a native call.\n\n  Also, the LSTM cannot have a ``bidirectional`` attribute set to ``True`` as\n  this wouldn't be usable in online settings. In this case, the default value\n  is the correct one.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lstm = LSTMModule(\n    input_size=n_cells,\n    hidden_size=128,\n    device=device,\n    in_key=\"embed\",\n    out_key=\"embed\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us look at the LSTM Module class, specifically its in and out_keys:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"in_keys\", lstm.in_keys)\nprint(\"out_keys\", lstm.out_keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that these values contain the key we indicated as the in_key (and out_key)\nas well as recurrent key names. The out_keys are preceded by a \"next\" prefix\nthat indicates that they will need to be written in the \"next\" TensorDict.\nWe use this convention (which can be overridden by passing the in_keys/out_keys\narguments) to make sure that a call to :func:`~torchrl.envs.utils.step_mdp` will\nmove the recurrent state to the root TensorDict, making it available to the\nRNN during the following call (see figure in the intro).\n\nAs mentioned earlier, we have one more optional transform to add to our\nenvironment to make sure that the recurrent states are passed to the buffer.\nThe :meth:`~torchrl.modules.LSTMModule.make_tensordict_primer` method does\nexactly that:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.append_transform(lstm.make_tensordict_primer())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and that's it! We can print the environment to check that everything looks good now\nthat we have added the primer:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MLP\n\nWe use a single-layer MLP to represent the action values we'll be using for\nour policy.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mlp = MLP(\n    out_features=2,\n    num_cells=[\n        64,\n    ],\n    device=device,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and fill the bias with zeros:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mlp[-1].bias.data.fill_(0.0)\nmlp = Mod(mlp, in_keys=[\"embed\"], out_keys=[\"action_value\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the Q-Values to select an action\n\nThe last part of our policy is the Q-Value Module.\nThe Q-Value module :class:`~torchrl.modules.tensordict_module.QValueModule`\nwill read the ``\"action_values\"`` key that is produced by our MLP and\nfrom it, gather the action that has the maximum value.\nThe only thing we need to do is to specify the action space, which can be done\neither by passing a string or an action-spec. This allows us to use\nCategorical (sometimes called \"sparse\") encoding or the one-hot version of it.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qval = QValueModule(action_space=None, spec=env.action_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>TorchRL also provides a wrapper class :class:`torchrl.modules.QValueActor` that\n  wraps a module in a Sequential together with a :class:`~torchrl.modules.tensordict_module.QValueModule`\n  like we are doing explicitly here. There is little advantage to do this\n  and the process is less transparent, but the end results will be similar to\n  what we do here.</p></div>\n\nWe can now put things together in a :class:`~tensordict.nn.TensorDictSequential`\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy = Seq(feature, lstm, mlp, qval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DQN being a deterministic algorithm, exploration is a crucial part of it.\nWe'll be using an $\\epsilon$-greedy policy with an epsilon of 0.2 decaying\nprogressively to 0.\nThis decay is achieved via a call to :meth:`~torchrl.modules.EGreedyModule.step`\n(see training loop below).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exploration_module = EGreedyModule(\n    annealing_num_steps=1_000_000, spec=env.action_spec, eps_init=0.2\n)\nstoch_policy = TensorDictSequential(\n    policy,\n    exploration_module,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the model for the loss\n\nThe model as we've built it is well-equipped to be used in sequential settings.\nHowever, the class :class:`torch.nn.LSTM` can use a cuDNN-optimized backend\nto run the RNN sequence faster on GPU device. We would not want to miss\nsuch an opportunity to speed up our training loop!\n\nBy default, torchrl losses will use this when executing any\n:class:`~torchrl.modules.LSTMModule` or :class:`~torchrl.modules.GRUModule`\nforward call. If you need to control this manually, the RNN modules are sensitive\nto a context manager/decorator, :class:`~torchrl.modules.set_recurrent_mode`,\nthat handles the behaviour of the underlying RNN module.\n\nBecause we still have a couple of uninitialized parameters we should\ninitialize them before creating an optimizer and such.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy(env.reset())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DQN Loss\n\nOut DQN loss requires us to pass the policy and, again, the action-space.\nWhile this may seem redundant, it is important as we want to make sure that\nthe :class:`~torchrl.objectives.DQNLoss` and the :class:`~torchrl.modules.tensordict_module.QValueModule`\nclasses are compatible, but aren't strongly dependent on each other.\n\nTo use the Double-DQN, we ask for a ``delay_value`` argument that will\ncreate a non-differentiable copy of the network parameters to be used\nas a target network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fn = DQNLoss(policy, action_space=env.action_spec, delay_value=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we are using a double DQN, we need to update the target parameters.\nWe'll use a  :class:`~torchrl.objectives.SoftUpdate` instance to carry out\nthis work.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "updater = SoftUpdate(loss_fn, eps=0.95)\n\noptim = torch.optim.Adam(policy.parameters(), lr=3e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collector and replay buffer\n\nFor RNN-based policies, we need to sample sequences of consecutive transitions\nrather than independent transitions. We'll use a :class:`~torchrl.data.replay_buffers.samplers.SliceSampler`\nto sample trajectory slices of length 50. This ensures that the LSTM hidden states\ncan be properly propagated through the sequence during training.\n\nThe buffer will store 1,000,000 individual transitions, and when sampling, we'll\nget slices of up to 50 consecutive steps. At each optimization step (16 per data collection),\nwe'll sample batches totaling 200 transitions from trajectory slices.\n\nWe'll use a :class:`~torchrl.data.replay_buffers.LazyMemmapStorage` storage to keep the data\non disk, and pass the replay buffer directly to the data collector so it can\nautomatically populate the buffer as data is collected.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>For the sake of efficiency, we're only running a few thousands iterations\n  here. In a real setting, the total number of frames should be set to 1M.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "buffer_scratch_dir = tempfile.TemporaryDirectory().name\n\nrb = TensorDictReplayBuffer(\n    storage=LazyMemmapStorage(1_000_000, scratch_dir=buffer_scratch_dir),\n    sampler=SliceSampler(\n        slice_len=50, end_key=(\"next\", \"done\"), cache_values=True, strict_length=False\n    ),\n    batch_size=200,\n    prefetch=10,\n    transform=lambda td: td.to(device),\n)\n\ncollector = SyncDataCollector(\n    env,\n    stoch_policy,\n    frames_per_batch=50,\n    total_frames=200,\n    storing_device=\"cpu\",\n    replay_buffer=rb,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop\n\nSince we passed the replay buffer to the collector, the collector no longer yields\ndata directly - instead it automatically populates the replay buffer. We iterate\nthrough the collector to trigger data collection, then sample from the buffer for training.\nTo keep track of the progress, we will run the policy in the environment once\nevery 50 data collection iterations, and plot the results after training.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "utd = 16\npbar = tqdm.tqdm(total=collector.total_frames)\nlongest = 0\n\ntraj_lens = []\nfor i, _ in enumerate(collector):\n    pbar.update(collector.frames_per_batch)\n    # Only start training once we have enough data in the buffer\n    if len(rb) < 1000:\n        continue\n    for j in range(utd):\n        s = rb.sample()\n        if i == 0 and j == 0:\n            # Let's print the first sample to see the data structure\n            print(\n                \"Let us print the first batch of sampled data from the replay buffer.\\n\"\n                \"Pay attention to the key names which will reflect what can be found in this data structure, \"\n                \"in particular: the output of the QValueModule (action_values, action and chosen_action_value),\"\n                \"the 'is_init' key that will tell us if a step is initial or not, and the \"\n                \"recurrent_state keys.\\n\",\n                s,\n            )\n        loss_vals = loss_fn(s)\n        loss_vals[\"loss\"].backward()\n        optim.step()\n        optim.zero_grad()\n    pbar.set_description(\n        f\"buffer_size: {len(rb)}, loss_val: {loss_vals['loss'].item(): 4.4f}\"\n    )\n    exploration_module.step(collector.frames_per_batch)\n    updater.step()\n\n    with set_exploration_type(ExplorationType.DETERMINISTIC), torch.no_grad():\n        rollout = env.rollout(10000, stoch_policy)\n        traj_lens.append(rollout.get((\"next\", \"step_count\")).max().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot our results:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if traj_lens:\n    from matplotlib import pyplot as plt\n\n    plt.plot(traj_lens)\n    plt.xlabel(\"Test collection\")\n    plt.title(\"Test trajectory lengths\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nWe have seen how an RNN can be incorporated in a policy in TorchRL.\nYou should now be able:\n\n- Create an LSTM module that acts as a :class:`~tensordict.nn.TensorDictModule`\n- Indicate to the LSTM module that a reset is needed via an :class:`~torchrl.envs.transforms.InitTracker`\n  transform\n- Incorporate this module in a policy and in a loss module\n- Make sure that the collector is made aware of the recurrent state entries\n  such that they can be stored in the replay buffer along with the rest of\n  the data\n\n## Further Reading\n\n- The TorchRL documentation can be found [here](https://pytorch.org/rl/).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}