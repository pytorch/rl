{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# LLM Wrappers in TorchRL\n\nThis tutorial demonstrates how to use TorchRL's LLM wrappers for integrating Large Language Models\ninto reinforcement learning workflows. TorchRL provides two main wrappers:\n\n- :class:`~torchrl.modules.llm.policies.vLLMWrapper` for vLLM models\n- :class:`~torchrl.modules.llm.policies.TransformersWrapper` for Hugging Face Transformers models\n\nBoth wrappers provide a unified API with consistent input/output interfaces using TensorClass objects,\nmaking them interchangeable in RL environments.\n\nKey Features:\n- Multiple input modes: history, text, or tokens\n- Configurable outputs: text, tokens, masks, and log probabilities\n- TensorClass-based structured outputs\n- Seamless integration with TorchRL's TensorDict framework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\nFirst, let's set up the environment and import the necessary modules.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport warnings\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings(\"ignore\")\n\n# Set vLLM environment variables\nos.environ[\"VLLM_USE_V1\"] = \"0\"\n\nimport torch\nfrom tensordict import TensorDict\nfrom torchrl.data.llm import History\nfrom torchrl.modules.llm.policies import TransformersWrapper, vLLMWrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: vLLM Wrapper with History Input\nThe vLLM wrapper is optimized for high-performance inference and is ideal for production environments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    from transformers import AutoTokenizer\n    from vllm import LLM\n\n    print(\"Loading vLLM model...\")\n    model = LLM(model=\"Qwen/Qwen2.5-0.5B\")\n    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n\n    # Create conversation history\n    chats = [\n        [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n        ],\n        [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is the capital of Canada?\"},\n        ],\n    ]\n    history = History.from_chats(chats)\n\n    # Create vLLM wrapper with history input (recommended for RL environments)\n    vllm_wrapper = vLLMWrapper(\n        model,\n        tokenizer=tokenizer,\n        input_mode=\"history\",\n        generate=True,\n        return_log_probs=True,\n        return_text=True,\n        return_tokens=True,\n        return_masks=True,\n        pad_output=False,  # Use False to avoid stacking issues\n    )\n\n    print(f\"vLLM wrapper input keys: {vllm_wrapper.in_keys}\")\n    print(f\"vLLM wrapper output keys: {vllm_wrapper.out_keys}\")\n\n    # Process the data\n    data_history = TensorDict(history=history, batch_size=(2,))\n    result = vllm_wrapper(data_history)\n\n    print(\"vLLM Results:\")\n    print(f\"Generated responses: {result['text'].response}\")\n    print(\n        f\"Response tokens shape: {result['tokens'].response.shape if result['tokens'].response is not None else 'None'}\"\n    )\n    print(f\"Log probabilities available: {result['log_probs'].response is not None}\")\n\nexcept ImportError:\n    print(\"vLLM not available, skipping vLLM example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Transformers Wrapper with History Input\nThe Transformers wrapper provides more flexibility and is great for research and development.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n    print(\"\\nLoading Transformers model...\")\n    transformers_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n    transformers_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n\n    # Create Transformers wrapper with same interface\n    transformers_wrapper = TransformersWrapper(\n        transformers_model,\n        tokenizer=transformers_tokenizer,\n        input_mode=\"history\",\n        generate=True,\n        return_log_probs=True,\n        return_text=True,\n        return_tokens=True,\n        return_masks=True,\n        pad_output=True,  # Transformers typically use padded outputs\n        generate_kwargs={\"max_new_tokens\": 50},\n    )\n\n    print(f\"Transformers wrapper input keys: {transformers_wrapper.in_keys}\")\n    print(f\"Transformers wrapper output keys: {transformers_wrapper.out_keys}\")\n\n    # Process the same data\n    result_tf = transformers_wrapper(data_history)\n\n    print(\"Transformers Results:\")\n    print(f\"Generated responses: {result_tf['text'].response}\")\n    print(\n        f\"Response tokens shape: {result_tf['tokens'].response.shape if result_tf['tokens'].response is not None else 'None'}\"\n    )\n    print(f\"Log probabilities available: {result_tf['log_probs'].response is not None}\")\n\nexcept ImportError:\n    print(\"Transformers not available, skipping Transformers example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Text Input Mode\nBoth wrappers support direct text input for simpler use cases.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    # Create text input data\n    prompts = [\"The capital of France is\", \"The capital of Canada is\"]\n    data_text = TensorDict(text=prompts, batch_size=(2,))\n\n    # vLLM with text input\n    vllm_text_wrapper = vLLMWrapper(\n        model,\n        tokenizer=tokenizer,\n        input_mode=\"text\",\n        generate=True,\n        return_text=True,\n        return_tokens=True,\n        pad_output=False,\n    )\n\n    result_vllm_text = vllm_text_wrapper(data_text)\n    print(\"\\nvLLM Text Input Results:\")\n    print(f\"Generated text: {result_vllm_text['text'].response}\")\n\n    # Transformers with text input\n    transformers_text_wrapper = TransformersWrapper(\n        transformers_model,\n        tokenizer=transformers_tokenizer,\n        input_mode=\"text\",\n        generate=True,\n        return_text=True,\n        return_tokens=True,\n        pad_output=True,\n        generate_kwargs={\"max_new_tokens\": 20},\n    )\n\n    result_tf_text = transformers_text_wrapper(data_text)\n    print(\"Transformers Text Input Results:\")\n    print(f\"Generated text: {result_tf_text['text'].response}\")\n\nexcept NameError:\n    print(\"Models not loaded, skipping text input example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Log Probabilities Only Mode\nBoth wrappers can compute log probabilities without generating new tokens.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    # vLLM log-probs only\n    vllm_logprobs_wrapper = vLLMWrapper(\n        model,\n        tokenizer=tokenizer,\n        input_mode=\"history\",\n        generate=False,  # Only compute log-probs\n        return_log_probs=True,\n        return_text=True,\n        return_tokens=True,\n        pad_output=False,\n    )\n\n    result_vllm_lp = vllm_logprobs_wrapper(data_history)\n    print(\"\\nvLLM Log Probabilities:\")\n    print(\n        f\"Prompt log-probs shape: {result_vllm_lp['log_probs'].prompt.shape if result_vllm_lp['log_probs'].prompt is not None else 'None'}\"\n    )\n\n    # Transformers log-probs only\n    transformers_logprobs_wrapper = TransformersWrapper(\n        transformers_model,\n        tokenizer=transformers_tokenizer,\n        input_mode=\"history\",\n        generate=False,\n        return_log_probs=True,\n        return_text=True,\n        return_tokens=True,\n        pad_output=True,\n    )\n\n    result_tf_lp = transformers_logprobs_wrapper(data_history)\n    print(\"Transformers Log Probabilities:\")\n    print(\n        \"Prompt log-probs shape: {result_tf_lp['log_probs'].prompt.shape if result_tf_lp['log_probs'].prompt is not None else 'None'}\"\n    )\n\nexcept NameError:\n    print(\"Models not loaded, skipping log-probs example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 5: TensorClass Structure Exploration\nLet's explore the structured outputs provided by both wrappers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    # Get a result from vLLM wrapper\n    result = vllm_wrapper(data_history)\n\n    print(\"\\nTensorClass Structure Analysis:\")\n    print(\"=\" * 50)\n\n    # Explore Text TensorClass\n    print(\"\\nText TensorClass:\")\n    print(f\"  Fields: {list(result['text'].__class__.__annotations__.keys())}\")\n    print(f\"  Prompt: {result['text'].prompt}\")\n    print(f\"  Response: {result['text'].response}\")\n    print(f\"  Full: {result['text'].full}\")\n    print(f\"  Padded: {result['text'].padded}\")\n\n    # Explore Tokens TensorClass\n    print(\"\\nTokens TensorClass:\")\n    print(f\"  Fields: {list(result['tokens'].__class__.__annotations__.keys())}\")\n    print(\n        f\"  Prompt tokens shape: {result['tokens'].prompt.shape if result['tokens'].prompt is not None else 'None'}\"\n    )\n    print(\n        f\"  Response tokens shape: {result['tokens'].response.shape if result['tokens'].response is not None else 'None'}\"\n    )\n    print(\n        f\"  Full tokens shape: {result['tokens'].full.shape if result['tokens'].full is not None else 'None'}\"\n    )\n\n    # Explore LogProbs TensorClass\n    print(\"\\nLogProbs TensorClass:\")\n    print(f\"  Fields: {list(result['log_probs'].__class__.__annotations__.keys())}\")\n    print(\n        f\"  Prompt log-probs shape: {result['log_probs'].prompt.shape if result['log_probs'].prompt is not None else 'None'}\"\n    )\n    print(\n        f\"  Response log-probs shape: {result['log_probs'].response.shape if result['log_probs'].response is not None else 'None'}\"\n    )\n\n    # Explore Masks TensorClass\n    print(\"\\nMasks TensorClass:\")\n    print(f\"  Fields: {list(result['masks'].__class__.__annotations__.keys())}\")\n    print(\n        f\"  Attention mask shape: {result['masks'].all_attention_mask.shape if result['masks'].all_attention_mask is not None else 'None'}\"\n    )\n    print(\n        f\"  Assistant mask shape: {result['masks'].all_assistant_mask.shape if result['masks'].all_assistant_mask is not None else 'None'}\"\n    )\n\nexcept NameError:\n    print(\"Models not loaded, skipping structure exploration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 6: Error Handling and Validation\nBoth wrappers provide clear error messages for invalid inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\nError Handling Examples:\")\nprint(\"=\" * 30)\n\n# Example of missing required key\ntry:\n    wrapper = vLLMWrapper(\n        model,\n        tokenizer=tokenizer,\n        input_mode=\"tokens\",\n        input_key=\"tokens\",\n    )\n    result = wrapper(TensorDict(batch_size=(2,)))  # Missing tokens key\nexcept (ValueError, NameError) as e:\n    print(f\"Expected error for missing key: {e}\")\n\n# Example of invalid input mode\ntry:\n    wrapper = vLLMWrapper(\n        model,\n        tokenizer=tokenizer,\n        input_mode=\"invalid_mode\",  # Invalid mode\n    )\nexcept ValueError as e:\n    print(f\"Expected error for invalid input mode: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 7: RL Environment Integration\nThe wrappers are designed to work seamlessly with TorchRL environments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\nRL Environment Integration:\")\nprint(\"=\" * 35)\n\n# Simulate an RL environment step\ntry:\n    # Create a simple environment state\n    env_state = TensorDict(\n        {\n            \"history\": history,\n            \"action_mask\": torch.ones(2, 1000),  # Example action mask\n            \"reward\": torch.zeros(2),\n            \"done\": torch.zeros(2, dtype=torch.bool),\n        },\n        batch_size=(2,),\n    )\n\n    # Use the wrapper as a policy\n    action_output = vllm_wrapper(env_state)\n\n    print(\"Environment integration successful!\")\n    print(f\"Generated actions: {action_output['text'].response}\")\n    print(\n        f\"Action log probabilities: {action_output['log_probs'].response is not None}\"\n    )\n\nexcept NameError:\n    print(\"Models not loaded, skipping RL integration example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\nTorchRL's LLM wrappers provide a unified interface for integrating Large Language Models\ninto reinforcement learning workflows. Key benefits include:\n\n1. **Consistent API**: Both vLLM and Transformers wrappers share the same interface\n2. **Flexible Input Modes**: Support for history, text, and token inputs\n3. **Structured Outputs**: TensorClass-based outputs for easy data handling\n4. **RL Integration**: Seamless integration with TorchRL's TensorDict framework\n5. **Configurable Outputs**: Selective return of text, tokens, masks, and log probabilities\n\nThe wrappers are designed to be interchangeable, allowing you to switch between\ndifferent LLM backends without changing your RL code.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\nprint(\"Tutorial completed successfully!\")\nprint(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}