{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# TorchRL envs\n\n**Author**: [Vincent Moens](https://github.com/vmoens)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Environments play a crucial role in RL settings, often somewhat similar to\ndatasets in supervised and unsupervised settings. The RL community has\nbecome quite familiar with OpenAI gym API which offers a flexible way of\nbuilding environments, initializing them and interacting with them. However,\nmany other libraries exist, and the way one interacts with them can be quite\ndifferent from what is expected with *gym*.\n\nLet us start by describing how TorchRL interacts with gym, which will serve\nas an introduction to other frameworks.\n\n## Gym environments\nTo run this part of the tutorial, you will need to have a recent version of\nthe gym library installed, as well as the atari suite. You can get this\ninstalled by installing the following packages:\n\n  .. code-block::\n    $ pip install gym atari-py ale-py gym[accept-rom-license] pygame\n\nTo unify all frameworks, torchrl environments are built inside the\n``__init__`` method with a private method called ``_build_env`` that\nwill pass the arguments and keyword arguments to the root library builder.\n\nWith gym, it means that building an environment is as easy as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom matplotlib import pyplot as plt\nfrom tensordict import TensorDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.libs.gym import GymEnv\n\nenv = GymEnv(\"Pendulum-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The list of available environment can be accessed through this command:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "list(GymEnv.available_envs)[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Env Specs\n\nLike other frameworks, TorchRL envs have attributes that indicate what\nspace is for the observations, action, done and reward. Because it often happens\nthat more than one observation is retrieved, we expect the observation spec\nto be of type ``CompositeSpec``.\nReward and action do not have this restriction:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Env observation_spec: \\n\", env.observation_spec)\nprint(\"Env action_spec: \\n\", env.action_spec)\nprint(\"Env reward_spec: \\n\", env.reward_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those spec come with a series of useful tools: one can assert whether a\nsample is in the defined space. We can also use some heuristic to project\na sample in the space if it is out of space, and generate random (possibly\nuniformly distributed) numbers in that space:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "action = torch.ones(1) * 3\nprint(\"action is in bounds?\\n\", bool(env.action_spec.is_in(action)))\nprint(\"projected action: \\n\", env.action_spec.project(action))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"random action: \\n\", env.action_spec.rand())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Out of these specs, the ``done_spec`` deserves a special attention. In TorchRL,\nall environments write end-of-trajectory signals of at least two types:\n``\"terminated\"`` (indicating that the Markov Decision Process has reached\na final state - the __episode__ is finished) and ``\"done\"``, indicating that\nthis is the last step of a __trajectory__ (but not necessarily the end of\nthe task). In general, a ``\"done\"`` entry that is ``True`` when a ``\"terminal\"``\nis ``False`` is caused by a ``\"truncated\"`` signal. Gym environments account for\nthese three signals:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(env.done_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Envs are also packed with an ``env.state_spec`` attribute of type\n``CompositeSpec`` which contains all the specs that are inputs to the env\nbut are not the action.\nFor stateful\nenvs (e.g. gym) this will be void most of the time.\nWith stateless environments\n(e.g. Brax) this should also include a representation of the previous state,\nor any other input to the environment (including inputs at reset time).\n\n### Seeding, resetting and steps\nThe basic operations on an environment are (1) ``set_seed``, (2) ``reset``\nand (3) ``step``.\n\nLet's see how these methods work with TorchRL:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)  # make sure that all torch code is also reproductible\nenv.set_seed(0)\nreset_data = env.reset()\nprint(\"reset data\", reset_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now execute a step in the environment. Since we don't have a policy,\nwe can just generate a random action:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "policy = TensorDictModule(env.action_spec.rand, in_keys=[], out_keys=[\"action\"])\n\n\npolicy(reset_data)\ntensordict_out = env.step(reset_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, the tensordict returned by ``step`` is the same as the input...\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert tensordict_out is reset_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... but with new keys\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What we just did (a random step using ``action_spec.rand()``) can also be\ndone via the simple shortcut.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.rand_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The new key ``(\"next\", \"observation\")`` (as all keys under the ``\"next\"``\ntensordict) have a special role in TorchRL: they indicate that they come\nafter the key with the same name but without the prefix.\n\nWe provide a function ``step_mdp`` that executes a step in the tensordict:\nit returns a new tensordict updated such that *t < -t'*:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.utils import step_mdp\n\ntensordict_out.set(\"some other key\", torch.randn(1))\ntensordict_tprime = step_mdp(tensordict_out)\n\nprint(tensordict_tprime)\nprint(\n    (\n        tensordict_tprime.get(\"observation\")\n        == tensordict_out.get((\"next\", \"observation\"))\n    ).all()\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe that ``step_mdp`` has removed all the time-dependent\nkey-value pairs, but not ``\"some other key\"``. Also, the new\nobservation matches the previous one.\n\nFinally, note that the ``env.reset`` method also accepts a tensordict to update:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = TensorDict()\nassert env.reset(data) is data\ndata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rollouts\nThe generic environment class provided by TorchRL allows you to run rollouts\neasily for a given number of steps:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict_rollout = env.rollout(max_steps=20, policy=policy)\nprint(tensordict_rollout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting tensordict has a ``batch_size`` of ``[20]``, which is the\nlength of the trajectory. We can check that the observation match their\nnext value:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(\n    tensordict_rollout.get(\"observation\")[1:]\n    == tensordict_rollout.get((\"next\", \"observation\"))[:-1]\n).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ``frame_skip``\nIn some instances, it is useful to use a ``frame_skip`` argument to use the\nsame action for several consecutive frames.\n\nThe resulting tensordict will contain only the last frame observed in the\nsequence, but the rewards will be summed over the number of frames.\n\nIf the environment reaches a done state during this process, it'll stop\nand return the result of the truncated chain.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = GymEnv(\"Pendulum-v1\", frame_skip=4)\nenv.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rendering\nRendering plays an important role in many RL settings, and this is why the\ngeneric environment class from torchrl provides a ``from_pixels`` keyword\nargument that allows the user to quickly ask for image-based environments:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = GymEnv(\"Pendulum-v1\", from_pixels=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = env.reset()\nenv.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.imshow(data.get(\"pixels\").numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's have a look at what the tensordict contains:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We still have a ``\"state\"`` that describes what ``\"observation\"`` used to\ndescribe in the previous case (the naming difference comes from the fact that\ngym now returns a dictionary and TorchRL gets the names from the dictionary\nif it exists, otherwise it names the step output ``\"observation\"``: in a\nfew words, this is due to inconsistencies in the object type returned by\ngym environment step method).\n\nOne can also discard this supplementary output by asking for the pixels only:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = GymEnv(\"Pendulum-v1\", from_pixels=True, pixels_only=True)\nenv.reset()\nenv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some environments only come in image-based format\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = GymEnv(\"ALE/Pong-v5\")\nprint(\"from pixels: \", env.from_pixels)\nprint(\"data: \", env.reset())\nenv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DeepMind Control environments\nTo run this part of the tutorial, make sure you have installed dm_control:\n   $ pip install dm_control\nWe also provide a wrapper for DM Control suite. Again, building an\nenvironment is easy: first let's look at what environments can be accessed.\nThe ``available_envs`` now returns a dict of envs and possible tasks:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.libs.dm_control import DMControlEnv\n\nDMControlEnv.available_envs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = DMControlEnv(\"acrobot\", \"swingup\")\ndata = env.reset()\nprint(\"result of reset: \", data)\nenv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course we can also use pixel-based environments:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = DMControlEnv(\"acrobot\", \"swingup\", from_pixels=True, pixels_only=True)\ndata = env.reset()\nprint(\"result of reset: \", data)\nplt.imshow(data.get(\"pixels\").numpy())\nenv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforming envs\nIt is common to pre-process the output of an environment before having it\nread by the policy or stored in a buffer.\n\nIn many instances, the RL community has adopted a wrapping scheme of the type\n   $ env_transformed = wrapper1(wrapper2(env))\nto transform environments. This has numerous advantages: it makes accessing\nthe environment specs obvious (the outer wrapper is the source of truth for\nthe external world), and it makes it easy to interact with vectorized\nenvironment. However it also makes it hard to access inner environments:\nsay one wants to remove a wrapper (e.g. ``wrapper2``) from the chain,\nthis operation requires us to collect\n   $ env0 = env.env.env\n\n   $ env_transformed_bis = wrapper1(env0)\nTorchRL takes the stance of using sequences of transforms instead, as it is\ndone in other pytorch domain libraries (e.g. ``torchvision``). This\napproach is also similar to the way distributions are transformed in\n``torch.distribution``, where a ``TransformedDistribution`` object is\nbuilt around a ``base_dist`` distribution and (a sequence of) ``transforms``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.transforms import ToTensorImage, TransformedEnv\n\n# ToTensorImage transforms a numpy-like image into a tensor one,\nenv = DMControlEnv(\"acrobot\", \"swingup\", from_pixels=True, pixels_only=True)\nprint(\"reset before transform: \", env.reset())\n\nenv = TransformedEnv(env, ToTensorImage())\nprint(\"reset after transform: \", env.reset())\nenv.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compose transforms, simply use the ``Compose`` class:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.transforms import Compose, Resize\n\nenv = DMControlEnv(\"acrobot\", \"swingup\", from_pixels=True, pixels_only=True)\nenv = TransformedEnv(env, Compose(ToTensorImage(), Resize(32, 32)))\nenv.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transforms can also be added one at a time:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.transforms import GrayScale\n\nenv.append_transform(GrayScale())\nenv.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the metadata get updated too:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"original obs spec: \", env.base_env.observation_spec)\nprint(\"current obs spec: \", env.observation_spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also concatenate tensors if needed:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.transforms import CatTensors\n\nenv = DMControlEnv(\"acrobot\", \"swingup\")\nprint(\"keys before concat: \", env.reset())\n\nenv = TransformedEnv(\n    env,\n    CatTensors(in_keys=[\"orientations\", \"velocity\"], out_key=\"observation\"),\n)\nprint(\"keys after concat: \", env.reset())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This feature makes it easy to mofidy the sets of transforms applied to an\nenvironment input and output. In fact, transforms are run both before and\nafter a step is executed: for the pre-step pass, the ``in_keys_inv`` list of\nkeys will be passed to the ``_inv_apply_transform`` method. An example of\nsuch a transform would be to transform floating-point actions (output from\na neural network) to the double dtype (requires by the wrapped environment).\nAfter the step is executed, the ``_apply_transform`` method will be\nexecuted on the keys indicated by the ``in_keys`` list of keys.\n\nAnother interesting feature of the environment transforms is that they\nallow the user to retrieve the equivalent of ``env.env`` in the wrapped\ncase, or in other words the parent environment. The parent environment can\nbe retrieved by calling ``transform.parent``: the returned environment\nwill consist in a ``TransformedEnvironment`` with all the transforms up to\n(but not including) the current transform. This is be used for instance in\nthe ``NoopResetEnv`` case, which when reset executes the following steps:\nresets the parent environment before executing a certain number of steps\nat random in that environment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = DMControlEnv(\"acrobot\", \"swingup\")\nenv = TransformedEnv(env)\nenv.append_transform(\n    CatTensors(in_keys=[\"orientations\", \"velocity\"], out_key=\"observation\")\n)\nenv.append_transform(GrayScale())\n\nprint(\"env: \\n\", env)\nprint(\"GrayScale transform parent env: \\n\", env.transform[1].parent)\nprint(\"CatTensors transform parent env: \\n\", env.transform[0].parent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment device\nTransforms can work on device, which can bring a significant speedup when\noperations are moderetely or highly computationally demanding. These include\n``ToTensorImage``, ``Resize``, ``GrayScale`` etc.\n\nOne could legitimately ask what that implies on the wrapped environment\nside. Very little for regular environments: the operations will still happen\non the device where they're supposed to happen. The environment device\nattribute in torchrl indicates on which device is the incoming data supposed\nto be and on which device the output data will be. Casting from and to that\ndevice is the responsibility of the torchrl environment class. The big\nadvantage of storing data on GPU is (1) speedup of transforms as mentioned\nabove and (2) sharing data amongst workers in multiprocessing settings.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.transforms import CatTensors, GrayScale, TransformedEnv\n\nenv = DMControlEnv(\"acrobot\", \"swingup\")\nenv = TransformedEnv(env)\nenv.append_transform(\n    CatTensors(in_keys=[\"orientations\", \"velocity\"], out_key=\"observation\")\n)\n\nif torch.has_cuda and torch.cuda.device_count():\n    env.to(\"cuda:0\")\n    env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running environments in parallel\nTorchRL provides utilities to run environment in parallel. It is expected\nthat the various environment read and return tensors of similar shapes and\ndtypes (but one could design masking functions to make this possible in case\nthose tensors differ in shapes). Creating such environments is quite easy.\nLet us look at the simplest case:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import ParallelEnv\n\n\ndef env_make():\n    return GymEnv(\"Pendulum-v1\")\n\n\nparallel_env = ParallelEnv(3, env_make)  # -> creates 3 envs in parallel\nparallel_env = ParallelEnv(\n    3, [env_make, env_make, env_make]\n)  # similar to the previous command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``SerialEnv`` class is similar to the ``ParallelEnv`` except for the\nfact that environments are run sequentially. This is mostly useful for\ndebugging purposes.\n\n``ParallelEnv`` instances are created in lazy mode: the environment will\nstart running only when called. This allows us to move ``ParallelEnv``\nobjects from process to process without worrying too much about running\nprocesses. A ``ParallelEnv`` can be started by calling ``start``, ``reset``\nor simply by calling ``step`` (if ``reset`` does not need to be called first).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parallel_env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One can check that the parallel environment has the right batch size.\nConventionally, the first part of the ``batch_size`` indicates the batch,\nthe second the time frame. Let's check that with the ``rollout`` method:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parallel_env.rollout(max_steps=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Closing parallel environments\n**Important**: before closing a program, it is important to close the\nparallel environment. In general, even with regular environments, it is good\npractice to close a function with a call to ``close``. In some instances,\nTorchRL will throw an error if this is not done (and often it will be at the\nend of a program, when the environment gets out of scope!)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parallel_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seeding\nWhen seeding a parallel environment, the difficulty we face is that we don't\nwant to provide the same seed to all environments. The heuristic used by\nTorchRL is that we produce a deterministic chain of seeds given the input\nseed in a - so to say - Markovian way, such that it can be reconstructed\nfrom any of its elements. All ``set_seed`` methods will return the next seed to\nbe used, such that one can easily keep the chain going given the last seed.\nThis is useful when several collectors all contain a ``ParallelEnv``\ninstance and we want each of the sub-sub-environments to have a different seed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "out_seed = parallel_env.set_seed(10)\nprint(out_seed)\n\ndel parallel_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accessing environment attributes\nIt sometimes occurs that a wrapped environment has an attribute that is of\ninterest. First, note that TorchRL environment wrapper constrains the toolings\nto access this attribute. Here's an example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from uuid import uuid1\n\n\ndef env_make():\n    env = GymEnv(\"Pendulum-v1\")\n    env._env.foo = f\"bar_{uuid1()}\"\n    env._env.get_something = lambda r: r + 1\n    return env\n\n\nenv = env_make()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Goes through env._env\nenv.foo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parallel_env = ParallelEnv(3, env_make)  # -> creates 3 envs in parallel\n\n# env has not been started --> error:\ntry:\n    parallel_env.foo\nexcept RuntimeError:\n    print(\"Aargh what did I do!\")\n    sleep(2)  # make sure we don't get ahead of ourselves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if parallel_env.is_closed:\n    parallel_env.start()\nfoo_list = parallel_env.foo\nfoo_list  # needs to be instantiated, for instance using list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "list(foo_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, methods can also be accessed:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "something = parallel_env.get_something(0)\nprint(something)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parallel_env.close()\ndel parallel_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### kwargs for parallel environments\nOne may want to provide kwargs to the various environments. This can achieved\neither at construction time or afterwards:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import ParallelEnv\n\n\ndef env_make(env_name):\n    env = TransformedEnv(\n        GymEnv(env_name, from_pixels=True, pixels_only=True),\n        Compose(ToTensorImage(), Resize(64, 64)),\n    )\n    return env\n\n\nparallel_env = ParallelEnv(\n    2,\n    [env_make, env_make],\n    create_env_kwargs=[{\"env_name\": \"ALE/AirRaid-v5\"}, {\"env_name\": \"ALE/Pong-v5\"}],\n)\ndata = parallel_env.reset()\n\nplt.figure()\nplt.subplot(121)\nplt.imshow(data[0].get(\"pixels\").permute(1, 2, 0).numpy())\nplt.subplot(122)\nplt.imshow(data[1].get(\"pixels\").permute(1, 2, 0).numpy())\nparallel_env.close()\ndel parallel_env\n\nfrom matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforming parallel environments\nThere are two equivalent ways of transforming parallel environments: in each\nprocess separately, or on the main process. It is even possible to do both.\nOne can therefore think carefully about the transform design to leverage the\ndevice capabilities (e.g. transforms on cuda devices) and vectorizing\noperations on the main process if possible.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import (\n    Compose,\n    GrayScale,\n    ParallelEnv,\n    Resize,\n    ToTensorImage,\n    TransformedEnv,\n)\n\n\ndef env_make(env_name):\n    env = TransformedEnv(\n        GymEnv(env_name, from_pixels=True, pixels_only=True),\n        Compose(ToTensorImage(), Resize(64, 64)),\n    )  # transforms on remote processes\n    return env\n\n\nparallel_env = ParallelEnv(\n    2,\n    [env_make, env_make],\n    create_env_kwargs=[{\"env_name\": \"ALE/AirRaid-v5\"}, {\"env_name\": \"ALE/Pong-v5\"}],\n)\nparallel_env = TransformedEnv(parallel_env, GrayScale())  # transforms on main process\ndata = parallel_env.reset()\n\nprint(\"grayscale data: \", data)\nplt.figure()\nplt.subplot(121)\nplt.imshow(data[0].get(\"pixels\").permute(1, 2, 0).numpy())\nplt.subplot(122)\nplt.imshow(data[1].get(\"pixels\").permute(1, 2, 0).numpy())\nparallel_env.close()\ndel parallel_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VecNorm\nIn RL, we commonly face the problem of normalizing data before inputting\nthem into a model. Sometimes, we can get a good approximation of the\nnormalizing statistics from data gathered in the environment with, say, a\nrandom policy (or demonstrations). It might, however, be advisable to\nnormalize the data \"on-the-fly\", updating the normalizing constants\nprogressively to what has been observed so far. This is particularly\nuseful when we expect the normalizing statistics to change following\nchanges in performance in the task, or when the environment is evolving\ndue to external factors.\n\n**Caution**: this feature should be used with caution with off-policy\nlearning, as old data will be \"deprecated\" due to its normalization with\npreviously valid normalizing statistics. In on-policy settings too, this\nfeature makes learning non-steady and may have unexpected effects. One\nwould therefore advice users to rely on this feature with caution and compare\nit with data normalizing given a fixed version of the normalizing constants.\n\nIn regular setting, using VecNorm is quite easy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.transforms import TransformedEnv, VecNorm\n\nenv = TransformedEnv(GymEnv(\"Pendulum-v1\"), VecNorm())\ndata = env.rollout(max_steps=100)\n\nprint(\"mean: :\", data.get(\"observation\").mean(0))  # Approx 0\nprint(\"std: :\", data.get(\"observation\").std(0))  # Approx 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In **parallel envs** things are slightly more complicated, as we need to\nshare the running statistics amongst the processes. We created a class\n``EnvCreator`` that is responsible for looking at an environment creation\nmethod, retrieving tensordicts to share amongst processes in the environment\nclass, and pointing each process to the right common, shared data\nonce created:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import EnvCreator, ParallelEnv\nfrom torchrl.envs.libs.gym import GymEnv\nfrom torchrl.envs.transforms import TransformedEnv, VecNorm\n\nmake_env = EnvCreator(lambda: TransformedEnv(GymEnv(\"CartPole-v1\"), VecNorm(decay=1.0)))\nenv = ParallelEnv(3, make_env)\nprint(\"env state dict:\")\nsd = TensorDict(make_env.state_dict())\nprint(sd)\n# Zeroes all tensors\nsd *= 0\n\ndata = env.rollout(max_steps=5)\n\nprint(\"data: \", data)\nprint(\"mean: :\", data.get(\"observation\").view(-1, 3).mean(0))  # Approx 0\nprint(\"std: :\", data.get(\"observation\").view(-1, 3).std(0))  # Approx 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The count is slightly higher than the number of steps (since we\ndid not use any decay). The difference between the two is due to the fact\nthat ``ParallelEnv`` creates a dummy environment to initialize the shared\n``TensorDict`` that is used to collect data from the dispatched environments.\nThis small difference will usually be absored throughout training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    \"update counts: \",\n    make_env.state_dict()[\"_extra_state\"][\"observation_count\"],\n)\n\nenv.close()\ndel env"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.24"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}