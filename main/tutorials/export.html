


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Exporting TorchRL modules &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial" href="multiagent_competitive_ddpg.html" />
    <link rel="prev" title="Using Replay Buffers" href="rb_tutorial.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../versions.html"><span style="font-size:110%">main (0.7.0+f4713f9) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Exporting TorchRL modules</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Exporting TorchRL modules</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorials/export.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
    

    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">tutorials/export</div>

      <div id="google-colab-link">
        <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
        <div class="call-to-action-desktop-view">Run in Google Colab</div>
        <div class="call-to-action-mobile-view">Colab</div>
      </div>
      <div id="download-notebook-link">
        <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
        <div class="call-to-action-desktop-view">Download Notebook</div>
        <div class="call-to-action-mobile-view">Notebook</div>
      </div>
      <div id="github-view-link">
        <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
        <div class="call-to-action-desktop-view">View on GitHub</div>
        <div class="call-to-action-mobile-view">GitHub</div>
      </div>
    </div>

    
    
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-tutorials-export-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="exporting-torchrl-modules">
<span id="sphx-glr-tutorials-export-py"></span><h1>Exporting TorchRL modules<a class="headerlink" href="#exporting-torchrl-modules" title="Permalink to this heading">¶</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/vmoens">Vincent Moens</a></p>
<div class="admonition note" id="export-tuto">
<p class="admonition-title">Note</p>
<p>To run this tutorial in a notebook, add an installation cell
at the beginning containing:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install tensordict
!pip install torchrl
!pip install &quot;gymnasium[atari,accept-rom-license]&quot;&lt;1.0.0
</pre></div>
</div>
</div></blockquote>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>Learning a policy has little value if that policy cannot be deployed in real-world settings.
As shown in other tutorials, TorchRL has a strong focus on modularity and composability: thanks to <code class="docutils literal notranslate"><span class="pre">tensordict</span></code>,
the components of the library can be written in the most generic way there is by abstracting their signature to a
mere set of operations on an input <code class="docutils literal notranslate"><span class="pre">TensorDict</span></code>.
This may give the impression that the library is bound to be used only for training, as typical low-level execution
hardwares (edge devices, robots, arduino, Raspberry Pi) do not execute python code, let alone with pytorch, tensordict
or torchrl installed.</p>
<p>Fortunately, PyTorch provides a full ecosystem of solutions to export code and trained models to devices and
hardwares, and TorchRL is fully equipped to interact with it.
It is possible to choose from a varied set of backends, including ONNX or AOTInductor examplified in this tutorial.
This tutorial gives a quick overview of how a trained model can be isolated and shipped as a standalone executable
to be exported on hardware.</p>
<p>Key learnings:</p>
<ul class="simple">
<li><p>Export any TorchRL module after training;</p></li>
<li><p>Using various backends;</p></li>
<li><p>Testing your exported model.</p></li>
</ul>
</section>
<section id="fast-recap-a-simple-torchrl-training-loop">
<h2>Fast recap: a simple TorchRL training loop<a class="headerlink" href="#fast-recap-a-simple-torchrl-training-loop" title="Permalink to this heading">¶</a></h2>
<p>In this section, we reproduce the training loop from the last Getting Started tutorial, slightly adapted to be used
with Atari games as they are rendered by the gymnasium library.
We will stick to the DQN example, and show how a policy that outputs a distribution over values can be used instead
later.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict.nn</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">TensorDictModule</span> <span class="k">as</span> <span class="n">Mod</span><span class="p">,</span>
    <span class="n">TensorDictSequential</span><span class="p">,</span>
    <span class="n">TensorDictSequential</span> <span class="k">as</span> <span class="n">Seq</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="kn">import</span> <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Adam</span></a>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl._utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">timeit</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors</span><span class="w"> </span><span class="kn">import</span> <span class="n">SyncDataCollector</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">LazyTensorStorage</span><span class="p">,</span> <span class="n">ReplayBuffer</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">Compose</span><span class="p">,</span>
    <span class="n">GrayScale</span><span class="p">,</span>
    <span class="n">GymEnv</span><span class="p">,</span>
    <span class="n">Resize</span><span class="p">,</span>
    <span class="n">set_exploration_type</span><span class="p">,</span>
    <span class="n">StepCounter</span><span class="p">,</span>
    <span class="n">ToTensorImage</span><span class="p">,</span>
    <span class="n">TransformedEnv</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules</span><span class="w"> </span><span class="kn">import</span> <span class="n">ConvNet</span><span class="p">,</span> <span class="n">EGreedyModule</span><span class="p">,</span> <span class="n">QValueModule</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.objectives</span><span class="w"> </span><span class="kn">import</span> <span class="n">DQNLoss</span><span class="p">,</span> <span class="n">SoftUpdate</span>

<a href="https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed" title="torch.manual_seed" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span></a><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">TransformedEnv</span><span class="p">(</span>
    <span class="n">GymEnv</span><span class="p">(</span><span class="s2">&quot;ALE/Pong-v5&quot;</span><span class="p">,</span> <span class="n">categorical_action_encoding</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">Compose</span><span class="p">(</span>
        <span class="n">ToTensorImage</span><span class="p">(),</span> <span class="n">Resize</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">),</span> <span class="n">GrayScale</span><span class="p">(),</span> <span class="n">StepCounter</span><span class="p">()</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">value_mlp</span> <span class="o">=</span> <span class="n">ConvNet</span><span class="o">.</span><span class="n">default_atari_dqn</span><span class="p">(</span><span class="n">num_actions</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span><span class="o">.</span><span class="n">space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
<span class="n">value_net</span> <span class="o">=</span> <span class="n">Mod</span><span class="p">(</span><span class="n">value_mlp</span><span class="p">,</span> <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;pixels&quot;</span><span class="p">],</span> <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;action_value&quot;</span><span class="p">])</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">Seq</span><span class="p">(</span><span class="n">value_net</span><span class="p">,</span> <span class="n">QValueModule</span><span class="p">(</span><span class="n">spec</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">))</span>
<span class="n">exploration_module</span> <span class="o">=</span> <span class="n">EGreedyModule</span><span class="p">(</span>
    <span class="n">env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">,</span> <span class="n">annealing_num_steps</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span> <span class="n">eps_init</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>
<span class="n">policy_explore</span> <span class="o">=</span> <span class="n">Seq</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">exploration_module</span><span class="p">)</span>

<span class="n">init_rand_steps</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">frames_per_batch</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">optim_steps</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">collector</span> <span class="o">=</span> <span class="n">SyncDataCollector</span><span class="p">(</span>
    <span class="n">env</span><span class="p">,</span>
    <span class="n">policy_explore</span><span class="p">,</span>
    <span class="n">frames_per_batch</span><span class="o">=</span><span class="n">frames_per_batch</span><span class="p">,</span>
    <span class="n">total_frames</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">init_random_frames</span><span class="o">=</span><span class="n">init_rand_steps</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">rb</span> <span class="o">=</span> <span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">storage</span><span class="o">=</span><span class="n">LazyTensorStorage</span><span class="p">(</span><span class="mi">100_000</span><span class="p">))</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">DQNLoss</span><span class="p">(</span><span class="n">value_network</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span> <span class="n">action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">,</span> <span class="n">delay_value</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam" title="torch.optim.Adam" class="sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">Adam</span></a><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">updater</span> <span class="o">=</span> <span class="n">SoftUpdate</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>

<span class="n">total_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total_episodes</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">collector</span><span class="p">:</span>
    <span class="c1"># Write data in replay buffer</span>
    <span class="n">rb</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">max_length</span> <span class="o">=</span> <span class="n">rb</span><span class="p">[:][</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;step_count&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rb</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">init_rand_steps</span><span class="p">:</span>
        <span class="c1"># Optim loop (we do several optim steps</span>
        <span class="c1"># per batch collected for efficiency)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">optim_steps</span><span class="p">):</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="n">rb</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>
            <span class="n">loss_vals</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
            <span class="n">loss_vals</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># Update exploration factor</span>
            <span class="n">exploration_module</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">numel</span><span class="p">())</span>
            <span class="c1"># Update target params</span>
            <span class="n">updater</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">total_count</span> <span class="o">+=</span> <span class="n">data</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
            <span class="n">total_episodes</span> <span class="o">+=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;done&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">200</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
</div>
</section>
<section id="exporting-a-tensordictmodule-based-policy">
<h2>Exporting a TensorDictModule-based policy<a class="headerlink" href="#exporting-a-tensordictmodule-based-policy" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">TensorDict</span></code> allowed us to build a policy with a great flexibility: from a regular <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> that
outputs action values from an observation, we added a <code class="xref py py-class docutils literal notranslate"><span class="pre">QValueModule</span></code> module that
read these values and computed an action using some heuristic (e.g., an argmax call).</p>
<p>However, there’s a small technical catch in our case: the environment (the actual Atari game) doesn’t return
grayscale, 84x84 images but raw screen-size color ones. The transforms we appended to the environment make sure that
the images can be read by the model. We can see that, from the training perspective, the boundary between environment
and model is blurry, but at execution time things are much clearer: the model should take care of transforming
the input data (images) to the format that can be processed by our CNN.</p>
<p>Here again, the magic of tensordict will unblock us: it happens that most of local (non-recursive) TorchRL’s
transforms can be used both as environment transforms or preprocessing blocks within a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a>
instance. Let’s see how we can prepend them to our policy:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">policy_transform</span> <span class="o">=</span> <span class="n">TensorDictSequential</span><span class="p">(</span>
    <span class="n">env</span><span class="o">.</span><span class="n">transform</span><span class="p">[</span>
        <span class="p">:</span><span class="o">-</span><span class="mi">1</span>
    <span class="p">],</span>  <span class="c1"># the last transform is a step counter which we don&#39;t need for preproc</span>
    <span class="n">policy_explore</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span>
        <span class="kc">False</span>
    <span class="p">),</span>  <span class="c1"># Using the explorative version of the policy for didactic purposes, see below.</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We create a fake input, and pass it to <a class="reference external" href="https://pytorch.org/docs/stable/export.html#torch.export.export" title="(in PyTorch v2.6)"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a> with the policy. This will give a “raw” python
function that will read our input tensor and output an action without any reference to TorchRL or tensordict modules.</p>
<p>A good practice is to call <code class="xref py py-meth docutils literal notranslate"><span class="pre">select_out_keys()</span></code> to let the model know that
we only want a certain set of outputs (in case the policy returns more than one tensor).</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">fake_td</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">base_env</span><span class="o">.</span><span class="n">fake_tensordict</span><span class="p">()</span>
<span class="n">pixels</span> <span class="o">=</span> <span class="n">fake_td</span><span class="p">[</span><span class="s2">&quot;pixels&quot;</span><span class="p">]</span>
<span class="k">with</span> <span class="n">set_exploration_type</span><span class="p">(</span><span class="s2">&quot;DETERMINISTIC&quot;</span><span class="p">):</span>
    <span class="n">exported_policy</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a><span class="p">(</span>
        <span class="c1"># Select only the &quot;action&quot; output key</span>
        <span class="n">policy_transform</span><span class="o">.</span><span class="n">select_out_keys</span><span class="p">(</span><span class="s2">&quot;action&quot;</span><span class="p">),</span>
        <span class="n">args</span><span class="o">=</span><span class="p">(),</span>
        <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;pixels&quot;</span><span class="p">:</span> <span class="n">pixels</span><span class="p">},</span>
        <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Representing the policy can be quite insightful: we can see that the first operations are a permute, a div, unsqueeze,
resize followed by the convolutional and MLP layers.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Deterministic policy&quot;</span><span class="p">)</span>
<span class="n">exported_policy</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">()</span>
</pre></div>
</div>
<p>As a final check, we can execute the policy with a dummy input. The output (for a single image) should be an integer
from 0 to 6 representing the action to be executed in the game.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">exported_policy</span><span class="o">.</span><span class="n">module</span><span class="p">()(</span><span class="n">pixels</span><span class="o">=</span><span class="n">pixels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Exported module output&quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>Further details on exporting <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorDictModule</span></code> instances can be found in the tensordict
<a class="reference external" href="https://pytorch.org/tensordict/stable/tutorials/export.html">documentation</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Exporting modules that take and output nested keys is perfectly fine.
The corresponding kwargs will be the <cite>“_”.join(key)</cite> version of the key, i.e., the <cite>(“group0”, “agent0”, “obs”)</cite>
key will correspond to the <cite>“group0_agent0_obs”</cite> keyword argument. Colliding keys (e.g., <cite>(“group0_agent0”, “obs”)</cite>
and <cite>(“group0”, “agent0_obs”)</cite> may lead to undefined behaviours and should be avoided at all cost.
Obviously, key names should also always produce valid keyword arguments, i.e., they should not contain special
characters such as spaces or commas.</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch.export</span></code> has many other features that we will explore further below. Before this, let us just do a small
digression on exploration and stochastic policies in the context of test-time inference, as well as recurrent
policies.</p>
</section>
<section id="working-with-stochastic-policies">
<h2>Working with stochastic policies<a class="headerlink" href="#working-with-stochastic-policies" title="Permalink to this heading">¶</a></h2>
<p>As you probably noted, above we used the <a class="reference internal" href="../reference/generated/torchrl.envs.set_exploration_type.html#torchrl.envs.set_exploration_type" title="torchrl.envs.set_exploration_type"><code class="xref py py-class docutils literal notranslate"><span class="pre">set_exploration_type</span></code></a> context manager to control
the behaviour of the policy. If the policy is stochastic (e.g., the policy outputs a distribution over the action
space like it is the case in PPO or other similar on-policy algorithms) or explorative (with an exploration module
appended like E-Greedy, additive gaussian or Ornstein-Uhlenbeck) we may want or not want to use that exploration
strategy in its exported version.
Fortunately, export utils can understand that context manager and as long as the exportation occurs within the right
context manager, the behaviour of the policy should match what is indicated. To demonstrate this, let us try with
another exploration type:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">set_exploration_type</span><span class="p">(</span><span class="s2">&quot;RANDOM&quot;</span><span class="p">):</span>
    <span class="n">exported_stochastic_policy</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a><span class="p">(</span>
        <span class="n">policy_transform</span><span class="o">.</span><span class="n">select_out_keys</span><span class="p">(</span><span class="s2">&quot;action&quot;</span><span class="p">),</span>
        <span class="n">args</span><span class="o">=</span><span class="p">(),</span>
        <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;pixels&quot;</span><span class="p">:</span> <span class="n">pixels</span><span class="p">},</span>
        <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Our exported policy should now have a random module at the end of the call stack, unlike the previous version.
Indeed, the last three operations are: generate a random integer between 0 and 6, use a random mask and select
the network output or the random action based on the value in the mask.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stochastic policy&quot;</span><span class="p">)</span>
<span class="n">exported_stochastic_policy</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="working-with-recurrent-policies">
<h2>Working with recurrent policies<a class="headerlink" href="#working-with-recurrent-policies" title="Permalink to this heading">¶</a></h2>
<p>Another typical use case is a recurrent policy that will output an action as well as a one or more recurrent state.
LSTM and GRU are CuDNN-based modules, which means that they will behave differently than regular
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> instances (export utils may not trace them well). Fortunately, TorchRL provides a python
implementation of these modules that can be swapped with the CuDNN version when desired.</p>
<p>To show this, let us write a prototypical policy that relies on an RNN:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensordict.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDictModule</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs</span><span class="w"> </span><span class="kn">import</span> <span class="n">BatchSizeTransform</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules</span><span class="w"> </span><span class="kn">import</span> <span class="n">LSTMModule</span><span class="p">,</span> <span class="n">MLP</span>

<span class="n">lstm</span> <span class="o">=</span> <span class="n">LSTMModule</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;observation&quot;</span><span class="p">,</span> <span class="s2">&quot;hidden0&quot;</span><span class="p">,</span> <span class="s2">&quot;hidden1&quot;</span><span class="p">],</span>
    <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;intermediate&quot;</span><span class="p">,</span> <span class="s2">&quot;hidden0&quot;</span><span class="p">,</span> <span class="s2">&quot;hidden1&quot;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>If the LSTM module is not python based but CuDNN (<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM" title="(in PyTorch v2.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">LSTM</span></code></a>), the <a class="reference internal" href="../reference/generated/torchrl.modules.LSTMModule.html#torchrl.modules.LSTMModule.make_python_based" title="torchrl.modules.LSTMModule.make_python_based"><code class="xref py py-meth docutils literal notranslate"><span class="pre">make_python_based()</span></code></a>
method can be used to use the python version.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">lstm</span> <span class="o">=</span> <span class="n">lstm</span><span class="o">.</span><span class="n">make_python_based</span><span class="p">()</span>
</pre></div>
</div>
<p>Let’s now create the policy. We combine two layers that modify the shape of the input (unsqueeze/squeeze operations)
with the LSTM and an MLP.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">recurrent_policy</span> <span class="o">=</span> <span class="n">TensorDictSequential</span><span class="p">(</span>
    <span class="c1"># Unsqueeze the first dim of all tensors to make LSTMCell happy</span>
    <span class="n">BatchSizeTransform</span><span class="p">(</span><span class="n">reshape_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span>
    <span class="n">lstm</span><span class="p">,</span>
    <span class="n">TensorDictModule</span><span class="p">(</span>
        <span class="n">MLP</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_cells</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span>
        <span class="n">in_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;intermediate&quot;</span><span class="p">],</span>
        <span class="n">out_keys</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;action&quot;</span><span class="p">],</span>
    <span class="p">),</span>
    <span class="c1"># Squeeze the first dim of all tensors to get the original shape back</span>
    <span class="n">BatchSizeTransform</span><span class="p">(</span><span class="n">reshape_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>As before, we select the relevant keys:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">recurrent_policy</span><span class="o">.</span><span class="n">select_out_keys</span><span class="p">(</span><span class="s2">&quot;action&quot;</span><span class="p">,</span> <span class="s2">&quot;hidden0&quot;</span><span class="p">,</span> <span class="s2">&quot;hidden1&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;recurrent policy input keys:&quot;</span><span class="p">,</span> <span class="n">recurrent_policy</span><span class="o">.</span><span class="n">in_keys</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;recurrent policy output keys:&quot;</span><span class="p">,</span> <span class="n">recurrent_policy</span><span class="o">.</span><span class="n">out_keys</span><span class="p">)</span>
</pre></div>
</div>
<p>We are now ready to export. To do this, we build fake inputs and pass them to <a class="reference external" href="https://pytorch.org/docs/stable/export.html#torch.export.export" title="(in PyTorch v2.6)"><code class="xref py py-func docutils literal notranslate"><span class="pre">export()</span></code></a>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">fake_obs</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">fake_hidden0</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">fake_hidden1</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>

<span class="c1"># Tensor indicating whether the state is the first of a sequence</span>
<span class="n">fake_is_init</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros" title="torch.zeros" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span></a><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>

<span class="n">exported_recurrent_policy</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/export.html#torch.export.export" title="torch.export.export" class="sphx-glr-backref-module-torch-export sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">export</span></a><span class="p">(</span>
    <span class="n">recurrent_policy</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="p">(),</span>
    <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;observation&quot;</span><span class="p">:</span> <span class="n">fake_obs</span><span class="p">,</span>
        <span class="s2">&quot;hidden0&quot;</span><span class="p">:</span> <span class="n">fake_hidden0</span><span class="p">,</span>
        <span class="s2">&quot;hidden1&quot;</span><span class="p">:</span> <span class="n">fake_hidden1</span><span class="p">,</span>
        <span class="s2">&quot;is_init&quot;</span><span class="p">:</span> <span class="n">fake_is_init</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Recurrent policy graph:&quot;</span><span class="p">)</span>
<span class="n">exported_recurrent_policy</span><span class="o">.</span><span class="n">graph_module</span><span class="o">.</span><span class="n">print_readable</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="aotinductor-export-your-policy-to-pytorch-free-c-binaries">
<h2>AOTInductor: Export your policy to pytorch-free C++ binaries<a class="headerlink" href="#aotinductor-export-your-policy-to-pytorch-free-c-binaries" title="Permalink to this heading">¶</a></h2>
<p>AOTInductor is a PyTorch module that allows you to export your model (policy or other) to pytorch-free C++ binaries.
This is particularly useful when you need to deploy your model on devices or platforms where PyTorch is not available.</p>
<p>Here’s an example of how you can use AOTInductor to export your policy, inspired by the
<a class="reference external" href="https://pytorch.org/docs/main/torch.compiler_aot_inductor.html">AOTI documentation</a>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tempfile</span><span class="w"> </span><span class="kn">import</span> <span class="n">TemporaryDirectory</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch._inductor</span><span class="w"> </span><span class="kn">import</span> <span class="n">aoti_compile_and_package</span><span class="p">,</span> <span class="n">aoti_load_package</span>

<span class="k">with</span> <span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmpdir</span><span class="p">:</span>
    <span class="n">path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;model.pt2&quot;</span><span class="p">)</span>
    <span class="k">with</span> <a href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
        <span class="n">pkg_path</span> <span class="o">=</span> <span class="n">aoti_compile_and_package</span><span class="p">(</span>
            <span class="n">exported_policy</span><span class="p">,</span>
            <span class="c1"># Specify the generated shared library path</span>
            <span class="n">package_path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;pkg_path&quot;</span><span class="p">,</span> <span class="n">pkg_path</span><span class="p">)</span>

    <span class="n">compiled_module</span> <span class="o">=</span> <span class="n">aoti_load_package</span><span class="p">(</span><span class="n">pkg_path</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">compiled_module</span><span class="p">(</span><span class="n">pixels</span><span class="o">=</span><span class="n">pixels</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="exporting-torchrl-models-with-onnx">
<h2>Exporting TorchRL models with ONNX<a class="headerlink" href="#exporting-torchrl-models-with-onnx" title="Permalink to this heading">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To execute this part of the script, make sure pytorch onnx is installed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install onnx-pytorch
!pip install onnxruntime
</pre></div>
</div>
</div>
<p>You can also find more information about using ONNX in the PyTorch ecosystem
<a class="reference external" href="https://pytorch.org/tutorials/beginner/onnx/intro_onnx.html">here</a>. The following example is based on this
documentation.</p>
<p>In this section, we are going to showcase how we can export our model in such a way that it can be
executed on a pytorch-free setting.</p>
<p>There are plenty of resources on the web explaining how ONNX can be used to deploy PyTorch models on various
hardwares and devices, including <a class="reference external" href="https://qengineering.eu/install-pytorch-on-raspberry-pi-4.html">Raspberry Pi</a>,
<a class="reference external" href="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html">NVIDIA TensorRT</a>,
<a class="reference external" href="https://apple.github.io/coremltools/docs-guides/source/convert-pytorch.html">iOS</a> and
<a class="reference external" href="https://onnxruntime.ai/docs/tutorials/mobile/">Android</a>.</p>
<p>The Atari game we trained on can be isolated without TorchRL or gymnasium with the
<a class="reference external" href="https://github.com/Farama-Foundation/Arcade-Learning-Environment">ALE library</a> and therefore provides us with
a good example of what we can achieve with ONNX.</p>
<p>Let us see what this API looks like:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ale_py</span><span class="w"> </span><span class="kn">import</span> <span class="n">ALEInterface</span><span class="p">,</span> <span class="n">roms</span>

<span class="c1"># Create the interface</span>
<span class="n">ale</span> <span class="o">=</span> <span class="n">ALEInterface</span><span class="p">()</span>
<span class="c1"># Load the pong environment</span>
<span class="n">ale</span><span class="o">.</span><span class="n">loadROM</span><span class="p">(</span><span class="n">roms</span><span class="o">.</span><span class="n">Pong</span><span class="p">)</span>
<span class="n">ale</span><span class="o">.</span><span class="n">reset_game</span><span class="p">()</span>

<span class="c1"># Make a step in the simulator</span>
<span class="n">action</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">reward</span> <span class="o">=</span> <span class="n">ale</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="n">screen_obs</span> <span class="o">=</span> <span class="n">ale</span><span class="o">.</span><span class="n">getScreenRGB</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Observation from ALE simulator:&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">screen_obs</span><span class="p">),</span> <span class="n">screen_obs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">screen_obs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Screen rendering of Pong game.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Exporting to ONNX is quite similar the Export/AOTI above:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">onnxruntime</span>

<span class="k">with</span> <span class="n">set_exploration_type</span><span class="p">(</span><span class="s2">&quot;DETERMINISTIC&quot;</span><span class="p">):</span>
    <span class="c1"># We use torch.onnx.dynamo_export to capture the computation graph from our policy_explore model</span>
    <span class="n">pixels</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.as_tensor.html#torch.as_tensor" title="torch.as_tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span></a><span class="p">(</span><span class="n">screen_obs</span><span class="p">)</span>
    <span class="n">onnx_policy_export</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/onnx_dynamo.html#torch.onnx.dynamo_export" title="torch.onnx.dynamo_export" class="sphx-glr-backref-module-torch-onnx sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">onnx</span><span class="o">.</span><span class="n">dynamo_export</span></a><span class="p">(</span><span class="n">policy_transform</span><span class="p">,</span> <span class="n">pixels</span><span class="o">=</span><span class="n">pixels</span><span class="p">)</span>
</pre></div>
</div>
<p>We can now save the program on disk and load it:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">TemporaryDirectory</span><span class="p">()</span> <span class="k">as</span> <span class="n">tmpdir</span><span class="p">:</span>
    <span class="n">onnx_file_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">tmpdir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;policy.onnx&quot;</span><span class="p">)</span>
    <span class="n">onnx_policy_export</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">onnx_file_path</span><span class="p">)</span>

    <span class="n">ort_session</span> <span class="o">=</span> <span class="n">onnxruntime</span><span class="o">.</span><span class="n">InferenceSession</span><span class="p">(</span>
        <span class="n">onnx_file_path</span><span class="p">,</span> <span class="n">providers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;CPUExecutionProvider&quot;</span><span class="p">]</span>
    <span class="p">)</span>

<span class="n">onnxruntime_input</span> <span class="o">=</span> <span class="p">{</span><span class="n">ort_session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">screen_obs</span><span class="p">}</span>
<span class="n">onnx_policy</span> <span class="o">=</span> <span class="n">ort_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">onnxruntime_input</span><span class="p">)</span>
</pre></div>
</div>
<section id="running-a-rollout-with-onnx">
<h3>Running a rollout with ONNX<a class="headerlink" href="#running-a-rollout-with-onnx" title="Permalink to this heading">¶</a></h3>
<p>We now have an ONNX model that runs our policy. Let’s compare it to the original TorchRL instance: because it is
more lightweight, the ONNX version should be faster than the TorchRL one.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">onnx_policy</span><span class="p">(</span><span class="n">screen_obs</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">onnxruntime_input</span> <span class="o">=</span> <span class="p">{</span><span class="n">ort_session</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">screen_obs</span><span class="p">}</span>
    <span class="n">onnxruntime_outputs</span> <span class="o">=</span> <span class="n">ort_session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">onnxruntime_input</span><span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">onnxruntime_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">action</span>


<span class="k">with</span> <span class="n">timeit</span><span class="p">(</span><span class="s2">&quot;ONNX rollout&quot;</span><span class="p">):</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">ale</span><span class="o">.</span><span class="n">reset_game</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="n">screen_obs</span> <span class="o">=</span> <span class="n">ale</span><span class="o">.</span><span class="n">getScreenRGB</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">onnx_policy</span><span class="p">(</span><span class="n">screen_obs</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">ale</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

<span class="k">with</span> <span class="n">timeit</span><span class="p">(</span><span class="s2">&quot;TorchRL version&quot;</span><span class="p">),</span> <a href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">(),</span> <span class="n">set_exploration_type</span><span class="p">(</span><span class="s2">&quot;DETERMINISTIC&quot;</span><span class="p">):</span>
    <span class="n">env</span><span class="o">.</span><span class="n">rollout</span><span class="p">(</span><span class="n">num_steps</span><span class="p">,</span> <span class="n">policy_explore</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">timeit</span><span class="o">.</span><span class="n">print</span><span class="p">())</span>
</pre></div>
</div>
<p>Note that ONNX also offers the possibility of optimizing models directly, but this is beyond the scope of this
tutorial.</p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>In this tutorial, we learned how to export TorchRL modules using various backends such as PyTorch’s built-in export
functionality, <code class="docutils literal notranslate"><span class="pre">AOTInductor</span></code>, and <code class="docutils literal notranslate"><span class="pre">ONNX</span></code>.
We demonstrated how to export a policy trained on an Atari game and run it on a pytorch-free setting using the <code class="docutils literal notranslate"><span class="pre">ALE</span></code>
library. We also compared the performance of the original TorchRL instance with the exported ONNX model.</p>
<p>Key takeaways:</p>
<ul class="simple">
<li><p>Exporting TorchRL modules allows for deployment on devices without PyTorch installed.</p></li>
<li><p>AOTInductor and ONNX provide alternative backends for exporting models.</p></li>
<li><p>Optimizing ONNX models can improve performance.</p></li>
</ul>
<p>Further reading and learning steps:</p>
<ul class="simple">
<li><p>Check out the official documentation for PyTorch’s <a class="reference external" href="https://pytorch.org/docs/stable/export.html">export functionality</a>,
<a class="reference external" href="https://pytorch.org/tutorials/recipes/torch_export_aoti_python.html">AOTInductor</a>, and
<a class="reference external" href="https://pytorch.org/tutorials/beginner/onnx/intro_onnx.html">ONNX</a> for more
information.</p></li>
<li><p>Experiment with deploying exported models on different devices.</p></li>
<li><p>Explore optimization techniques for ONNX models to improve performance.</p></li>
</ul>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-export-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/4e8ac58ef63f1e596d49d1b7366ef9bc/export.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">export.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/bbb4a09f4b0d139b93b8e77c84568b01/export.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">export.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/150528e38f6816824f1e81ed67476a9f/export.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">export.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="multiagent_competitive_ddpg.html" class="btn btn-neutral float-right" title="Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="rb_tutorial.html" class="btn btn-neutral" title="Using Replay Buffers" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Exporting TorchRL modules</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#fast-recap-a-simple-torchrl-training-loop">Fast recap: a simple TorchRL training loop</a></li>
<li><a class="reference internal" href="#exporting-a-tensordictmodule-based-policy">Exporting a TensorDictModule-based policy</a></li>
<li><a class="reference internal" href="#working-with-stochastic-policies">Working with stochastic policies</a></li>
<li><a class="reference internal" href="#working-with-recurrent-policies">Working with recurrent policies</a></li>
<li><a class="reference internal" href="#aotinductor-export-your-policy-to-pytorch-free-c-binaries">AOTInductor: Export your policy to pytorch-free C++ binaries</a></li>
<li><a class="reference internal" href="#exporting-torchrl-models-with-onnx">Exporting TorchRL models with ONNX</a><ul>
<li><a class="reference internal" href="#running-a-rollout-with-onnx">Running a rollout with ONNX</a></li>
</ul>
</li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-tutorials/"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>