


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>LLM Wrappers in TorchRL &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/pytorch.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx-design.min.css" type="text/css" />
  <link rel="stylesheet" href="../https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/features">Features</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   
  <div>

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../versions.html"><span style="font-size:110%">main (0.11.0) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="export.html">Exporting TorchRL modules</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">

      <section data-toggle="wy-nav-shift" class="pytorch-content-wrap">
        <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
          <div class="pytorch-breadcrumbs-wrapper">
            















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>LLM Wrappers in TorchRL</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorials/llm_wrappers.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
          </div>

          <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
            Shortcuts
          </div>
        </div>

        <div class="pytorch-content-left">
    

    <div class="pytorch-call-to-action-links">
      <div id="tutorial-type">tutorials/llm_wrappers</div>

      <div id="google-colab-link">
        <img class="call-to-action-img" src="../_static/images/pytorch-colab.svg"/>
        <div class="call-to-action-desktop-view">Run in Google Colab</div>
        <div class="call-to-action-mobile-view">Colab</div>
      </div>
      <div id="download-notebook-link">
        <img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
        <div class="call-to-action-desktop-view">Download Notebook</div>
        <div class="call-to-action-mobile-view">Notebook</div>
      </div>
      <div id="github-view-link">
        <img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
        <div class="call-to-action-desktop-view">View on GitHub</div>
        <div class="call-to-action-mobile-view">GitHub</div>
      </div>
    </div>

    
    
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-tutorials-llm-wrappers-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="llm-wrappers-in-torchrl">
<span id="sphx-glr-tutorials-llm-wrappers-py"></span><h1>LLM Wrappers in TorchRL<a class="headerlink" href="#llm-wrappers-in-torchrl" title="Link to this heading">¶</a></h1>
<p>This tutorial demonstrates how to use TorchRL’s LLM wrappers for integrating Large Language Models
into reinforcement learning workflows. TorchRL provides two main wrappers:</p>
<ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">vLLMWrapper</span></code> for vLLM models</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformersWrapper</span></code> for Hugging Face Transformers models</p></li>
</ul>
<p>Both wrappers provide a unified API with consistent input/output interfaces using TensorClass objects,
making them interchangeable in RL environments.</p>
<p>Key Features:
- Multiple input modes: history, text, or tokens
- Configurable outputs: text, tokens, masks, and log probabilities
- TensorClass-based structured outputs
- Seamless integration with TorchRL’s TensorDict framework</p>
<section id="setup-and-imports">
<h2>Setup and Imports<a class="headerlink" href="#setup-and-imports" title="Link to this heading">¶</a></h2>
<p>First, let’s set up the environment and import the necessary modules.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>

<span class="c1"># Suppress warnings for cleaner output</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="c1"># Set vLLM environment variables</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">tensordict</span> <span class="kn">import</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class"><span class="n">TensorDict</span></a>
<span class="kn">from</span> <span class="nn">torchrl.data.llm</span> <span class="kn">import</span> <span class="n">History</span>
<span class="kn">from</span> <span class="nn">torchrl.modules.llm.policies</span> <span class="kn">import</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorClass.html#tensordict.TensorClass" title="tensordict.TensorClass" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class"><span class="n">ChatHistory</span></a><span class="p">,</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="tensordict.nn.TensorDictModuleBase" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">TransformersWrapper</span></a><span class="p">,</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="tensordict.nn.TensorDictModuleBase" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">vLLMWrapper</span></a>
</pre></div>
</div>
</section>
<section id="example-1-vllm-wrapper-with-history-input">
<h2>Example 1: vLLM Wrapper with History Input<a class="headerlink" href="#example-1-vllm-wrapper-with-history-input" title="Link to this heading">¶</a></h2>
<p>The vLLM wrapper is optimized for high-performance inference and is ideal for production environments.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
    <span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">LLM</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading vLLM model...&quot;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2.5-0.5B&quot;</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen2.5-0.5B&quot;</span><span class="p">)</span>

    <span class="c1"># Create conversation history</span>
    <span class="n">chats</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">},</span>
        <span class="p">],</span>
        <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of Canada?&quot;</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">]</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">History</span><span class="o">.</span><span class="n">from_chats</span><span class="p">(</span><span class="n">chats</span><span class="p">)</span>
    <span class="n">chat_history</span> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorClass.html#tensordict.TensorClass" title="tensordict.TensorClass" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class"><span class="n">ChatHistory</span></a><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">history</span><span class="p">)</span>

    <span class="c1"># Create vLLM wrapper with history input (recommended for RL environments)</span>
    <span class="n">vllm_wrapper</span> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="tensordict.nn.TensorDictModuleBase" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">vLLMWrapper</span></a><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;history&quot;</span><span class="p">,</span>
        <span class="n">generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_log_probs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Use False to avoid stacking issues</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;vLLM wrapper input keys: </span><span class="si">{</span><span class="n">vllm_wrapper</span><span class="o">.</span><span class="n">in_keys</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;vLLM wrapper output keys: </span><span class="si">{</span><span class="n">vllm_wrapper</span><span class="o">.</span><span class="n">out_keys</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Process the data</span>
    <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_history</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class"><span class="n">TensorDict</span></a><span class="p">(</span><span class="n">history</span><span class="o">=</span><span class="n">chat_history</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">vllm_wrapper</span><span class="p">(</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_history</span></a><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;vLLM Results:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated responses: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Response tokens shape: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log probabilities available: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;vLLM not available, skipping vLLM example&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>vLLM not available, skipping vLLM example
</pre></div>
</div>
</section>
<section id="example-2-transformers-wrapper-with-history-input">
<h2>Example 2: Transformers Wrapper with History Input<a class="headerlink" href="#example-2-transformers-wrapper-with-history-input" title="Link to this heading">¶</a></h2>
<p>The Transformers wrapper provides more flexibility and is great for research and development.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Loading Transformers model...&quot;</span><span class="p">)</span>
    <span class="n">transformers_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen2.5-0.5B&quot;</span><span class="p">)</span>
    <span class="n">transformers_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen2.5-0.5B&quot;</span><span class="p">)</span>

    <span class="c1"># Create Transformers wrapper with same interface</span>
    <span class="n">transformers_wrapper</span> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="tensordict.nn.TensorDictModuleBase" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">TransformersWrapper</span></a><span class="p">(</span>
        <span class="n">transformers_model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers_tokenizer</span><span class="p">,</span>
        <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;history&quot;</span><span class="p">,</span>
        <span class="n">generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_log_probs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Transformers typically use padded outputs</span>
        <span class="n">generate_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">50</span><span class="p">},</span>
    <span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformers wrapper input keys: </span><span class="si">{</span><span class="n">transformers_wrapper</span><span class="o">.</span><span class="n">in_keys</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformers wrapper output keys: </span><span class="si">{</span><span class="n">transformers_wrapper</span><span class="o">.</span><span class="n">out_keys</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Create data for the Transformers wrapper</span>
    <span class="n">chats</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">},</span>
        <span class="p">],</span>
        <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the capital of Canada?&quot;</span><span class="p">},</span>
        <span class="p">],</span>
    <span class="p">]</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">History</span><span class="o">.</span><span class="n">from_chats</span><span class="p">(</span><span class="n">chats</span><span class="p">)</span>
    <span class="n">chat_history</span> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorClass.html#tensordict.TensorClass" title="tensordict.TensorClass" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class"><span class="n">ChatHistory</span></a><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">history</span><span class="p">)</span>
    <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_history</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class"><span class="n">TensorDict</span></a><span class="p">(</span><span class="n">history</span><span class="o">=</span><span class="n">chat_history</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>

    <span class="c1"># Process the data</span>
    <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">result_tf</span></a> <span class="o">=</span> <span class="n">transformers_wrapper</span><span class="p">(</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_history</span></a><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformers Results:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated responses: </span><span class="si">{</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">result_tf</span></a><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Response tokens shape: </span><span class="si">{</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">result_tf</span></a><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">result_tf</span></a><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log probabilities available: </span><span class="si">{</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">result_tf</span></a><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformers not available, skipping Transformers example&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Loading Transformers model...

Loading weights:   0%|          | 0/290 [00:00&lt;?, ?it/s]
Loading weights:   0%|          | 1/290 [00:00&lt;00:00, 30174.85it/s, Materializing param=model.embed_tokens.weight]
Loading weights:   0%|          | 1/290 [00:00&lt;00:00, 2704.26it/s, Materializing param=model.embed_tokens.weight]
Loading weights:   1%|          | 2/290 [00:00&lt;00:00, 2167.60it/s, Materializing param=model.layers.0.input_layernorm.weight]
Loading weights:   1%|          | 2/290 [00:00&lt;00:00, 1025.88it/s, Materializing param=model.layers.0.input_layernorm.weight]
Loading weights:   1%|          | 3/290 [00:00&lt;00:00, 1267.42it/s, Materializing param=model.layers.0.mlp.down_proj.weight]
Loading weights:   1%|          | 3/290 [00:00&lt;00:00, 1183.16it/s, Materializing param=model.layers.0.mlp.down_proj.weight]
Loading weights:   1%|▏         | 4/290 [00:00&lt;00:00, 1410.92it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]
Loading weights:   1%|▏         | 4/290 [00:00&lt;00:00, 1292.24it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]
Loading weights:   2%|▏         | 5/290 [00:00&lt;00:00, 1124.42it/s, Materializing param=model.layers.0.mlp.up_proj.weight]
Loading weights:   2%|▏         | 5/290 [00:00&lt;00:00, 1081.12it/s, Materializing param=model.layers.0.mlp.up_proj.weight]
Loading weights:   2%|▏         | 6/290 [00:00&lt;00:00, 1159.45it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]
Loading weights:   2%|▏         | 6/290 [00:00&lt;00:00, 1100.43it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]
Loading weights:   2%|▏         | 7/290 [00:00&lt;00:00, 1060.66it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]
Loading weights:   2%|▏         | 7/290 [00:00&lt;00:00, 986.76it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]
Loading weights:   3%|▎         | 8/290 [00:00&lt;00:00, 1077.26it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]
Loading weights:   3%|▎         | 8/290 [00:00&lt;00:00, 1024.78it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]
Loading weights:   3%|▎         | 9/290 [00:00&lt;00:00, 1068.73it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]
Loading weights:   3%|▎         | 9/290 [00:00&lt;00:00, 945.42it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]
Loading weights:   3%|▎         | 10/290 [00:00&lt;00:00, 968.46it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]
Loading weights:   3%|▎         | 10/290 [00:00&lt;00:00, 932.09it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]
Loading weights:   4%|▍         | 11/290 [00:00&lt;00:00, 960.23it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]
Loading weights:   4%|▍         | 11/290 [00:00&lt;00:00, 929.64it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]
Loading weights:   4%|▍         | 12/290 [00:00&lt;00:00, 974.87it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]
Loading weights:   4%|▍         | 12/290 [00:00&lt;00:00, 933.73it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]
Loading weights:   4%|▍         | 13/290 [00:00&lt;00:00, 968.44it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]
Loading weights:   4%|▍         | 13/290 [00:00&lt;00:00, 934.78it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]
Loading weights:   5%|▍         | 14/290 [00:00&lt;00:00, 941.72it/s, Materializing param=model.layers.1.input_layernorm.weight]
Loading weights:   5%|▍         | 14/290 [00:00&lt;00:00, 919.30it/s, Materializing param=model.layers.1.input_layernorm.weight]
Loading weights:   5%|▌         | 15/290 [00:00&lt;00:00, 949.28it/s, Materializing param=model.layers.1.mlp.down_proj.weight]
Loading weights:   5%|▌         | 15/290 [00:00&lt;00:00, 938.66it/s, Materializing param=model.layers.1.mlp.down_proj.weight]
Loading weights:   6%|▌         | 16/290 [00:00&lt;00:00, 934.40it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]
Loading weights:   6%|▌         | 16/290 [00:00&lt;00:00, 912.03it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]
Loading weights:   6%|▌         | 17/290 [00:00&lt;00:00, 912.82it/s, Materializing param=model.layers.1.mlp.up_proj.weight]
Loading weights:   6%|▌         | 17/290 [00:00&lt;00:00, 889.32it/s, Materializing param=model.layers.1.mlp.up_proj.weight]
Loading weights:   6%|▌         | 18/290 [00:00&lt;00:00, 885.51it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]
Loading weights:   6%|▌         | 18/290 [00:00&lt;00:00, 842.58it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]
Loading weights:   7%|▋         | 19/290 [00:00&lt;00:00, 866.92it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]
Loading weights:   7%|▋         | 19/290 [00:00&lt;00:00, 859.17it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]
Loading weights:   7%|▋         | 20/290 [00:00&lt;00:00, 869.78it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]
Loading weights:   7%|▋         | 20/290 [00:00&lt;00:00, 862.95it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]
Loading weights:   7%|▋         | 21/290 [00:00&lt;00:00, 855.58it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]
Loading weights:   7%|▋         | 21/290 [00:00&lt;00:00, 842.96it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]
Loading weights:   8%|▊         | 22/290 [00:00&lt;00:00, 864.34it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]
Loading weights:   8%|▊         | 22/290 [00:00&lt;00:00, 854.92it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]
Loading weights:   8%|▊         | 23/290 [00:00&lt;00:00, 860.64it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]
Loading weights:   8%|▊         | 23/290 [00:00&lt;00:00, 853.44it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]
Loading weights:   8%|▊         | 24/290 [00:00&lt;00:00, 860.27it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]
Loading weights:   8%|▊         | 24/290 [00:00&lt;00:00, 848.48it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]
Loading weights:   9%|▊         | 25/290 [00:00&lt;00:00, 873.81it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]
Loading weights:   9%|▊         | 25/290 [00:00&lt;00:00, 862.24it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]
Loading weights:   9%|▉         | 26/290 [00:00&lt;00:00, 886.51it/s, Materializing param=model.layers.2.input_layernorm.weight]
Loading weights:   9%|▉         | 26/290 [00:00&lt;00:00, 873.44it/s, Materializing param=model.layers.2.input_layernorm.weight]
Loading weights:   9%|▉         | 27/290 [00:00&lt;00:00, 879.82it/s, Materializing param=model.layers.2.mlp.down_proj.weight]
Loading weights:   9%|▉         | 27/290 [00:00&lt;00:00, 859.34it/s, Materializing param=model.layers.2.mlp.down_proj.weight]
Loading weights:  10%|▉         | 28/290 [00:00&lt;00:00, 858.15it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]
Loading weights:  10%|▉         | 28/290 [00:00&lt;00:00, 851.62it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]
Loading weights:  10%|█         | 29/290 [00:00&lt;00:00, 845.43it/s, Materializing param=model.layers.2.mlp.up_proj.weight]
Loading weights:  10%|█         | 29/290 [00:00&lt;00:00, 817.31it/s, Materializing param=model.layers.2.mlp.up_proj.weight]
Loading weights:  10%|█         | 30/290 [00:00&lt;00:00, 808.72it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]
Loading weights:  10%|█         | 30/290 [00:00&lt;00:00, 797.84it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]
Loading weights:  11%|█         | 31/290 [00:00&lt;00:00, 810.27it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]
Loading weights:  11%|█         | 31/290 [00:00&lt;00:00, 805.55it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]
Loading weights:  11%|█         | 32/290 [00:00&lt;00:00, 811.47it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]
Loading weights:  11%|█         | 32/290 [00:00&lt;00:00, 786.39it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]
Loading weights:  11%|█▏        | 33/290 [00:00&lt;00:00, 789.71it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]
Loading weights:  11%|█▏        | 33/290 [00:00&lt;00:00, 772.75it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]
Loading weights:  12%|█▏        | 34/290 [00:00&lt;00:00, 767.30it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]
Loading weights:  12%|█▏        | 34/290 [00:00&lt;00:00, 758.01it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]
Loading weights:  12%|█▏        | 35/290 [00:00&lt;00:00, 762.46it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]
Loading weights:  12%|█▏        | 35/290 [00:00&lt;00:00, 755.39it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]
Loading weights:  12%|█▏        | 36/290 [00:00&lt;00:00, 770.03it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]
Loading weights:  12%|█▏        | 36/290 [00:00&lt;00:00, 763.62it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]
Loading weights:  13%|█▎        | 37/290 [00:00&lt;00:00, 777.23it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]
Loading weights:  13%|█▎        | 37/290 [00:00&lt;00:00, 770.06it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]
Loading weights:  13%|█▎        | 38/290 [00:00&lt;00:00, 773.57it/s, Materializing param=model.layers.3.input_layernorm.weight]
Loading weights:  13%|█▎        | 38/290 [00:00&lt;00:00, 763.27it/s, Materializing param=model.layers.3.input_layernorm.weight]
Loading weights:  13%|█▎        | 39/290 [00:00&lt;00:00, 772.23it/s, Materializing param=model.layers.3.mlp.down_proj.weight]
Loading weights:  13%|█▎        | 39/290 [00:00&lt;00:00, 769.44it/s, Materializing param=model.layers.3.mlp.down_proj.weight]
Loading weights:  14%|█▍        | 40/290 [00:00&lt;00:00, 772.76it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]
Loading weights:  14%|█▍        | 40/290 [00:00&lt;00:00, 760.11it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]
Loading weights:  14%|█▍        | 41/290 [00:00&lt;00:00, 771.02it/s, Materializing param=model.layers.3.mlp.up_proj.weight]
Loading weights:  14%|█▍        | 41/290 [00:00&lt;00:00, 763.62it/s, Materializing param=model.layers.3.mlp.up_proj.weight]
Loading weights:  14%|█▍        | 42/290 [00:00&lt;00:00, 772.59it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]
Loading weights:  14%|█▍        | 42/290 [00:00&lt;00:00, 763.73it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]
Loading weights:  15%|█▍        | 43/290 [00:00&lt;00:00, 776.38it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]
Loading weights:  15%|█▍        | 43/290 [00:00&lt;00:00, 770.63it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]
Loading weights:  15%|█▌        | 44/290 [00:00&lt;00:00, 785.08it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]
Loading weights:  15%|█▌        | 44/290 [00:00&lt;00:00, 779.69it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]
Loading weights:  16%|█▌        | 45/290 [00:00&lt;00:00, 785.80it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]
Loading weights:  16%|█▌        | 45/290 [00:00&lt;00:00, 783.65it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]
Loading weights:  16%|█▌        | 46/290 [00:00&lt;00:00, 782.81it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]
Loading weights:  16%|█▌        | 46/290 [00:00&lt;00:00, 777.27it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]
Loading weights:  16%|█▌        | 47/290 [00:00&lt;00:00, 783.09it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]
Loading weights:  16%|█▌        | 47/290 [00:00&lt;00:00, 776.55it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]
Loading weights:  17%|█▋        | 48/290 [00:00&lt;00:00, 777.51it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]
Loading weights:  17%|█▋        | 48/290 [00:00&lt;00:00, 774.85it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]
Loading weights:  17%|█▋        | 49/290 [00:00&lt;00:00, 772.54it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]
Loading weights:  17%|█▋        | 49/290 [00:00&lt;00:00, 767.52it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]
Loading weights:  17%|█▋        | 50/290 [00:00&lt;00:00, 777.40it/s, Materializing param=model.layers.4.input_layernorm.weight]
Loading weights:  17%|█▋        | 50/290 [00:00&lt;00:00, 773.61it/s, Materializing param=model.layers.4.input_layernorm.weight]
Loading weights:  18%|█▊        | 51/290 [00:00&lt;00:00, 784.61it/s, Materializing param=model.layers.4.mlp.down_proj.weight]
Loading weights:  18%|█▊        | 51/290 [00:00&lt;00:00, 782.77it/s, Materializing param=model.layers.4.mlp.down_proj.weight]
Loading weights:  18%|█▊        | 52/290 [00:00&lt;00:00, 786.97it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]
Loading weights:  18%|█▊        | 52/290 [00:00&lt;00:00, 775.27it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]
Loading weights:  18%|█▊        | 53/290 [00:00&lt;00:00, 781.11it/s, Materializing param=model.layers.4.mlp.up_proj.weight]
Loading weights:  18%|█▊        | 53/290 [00:00&lt;00:00, 779.04it/s, Materializing param=model.layers.4.mlp.up_proj.weight]
Loading weights:  19%|█▊        | 54/290 [00:00&lt;00:00, 785.54it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]
Loading weights:  19%|█▊        | 54/290 [00:00&lt;00:00, 773.54it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]
Loading weights:  19%|█▉        | 55/290 [00:00&lt;00:00, 784.62it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]
Loading weights:  19%|█▉        | 55/290 [00:00&lt;00:00, 782.76it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]
Loading weights:  19%|█▉        | 56/290 [00:00&lt;00:00, 781.88it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]
Loading weights:  19%|█▉        | 56/290 [00:00&lt;00:00, 776.02it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]
Loading weights:  20%|█▉        | 57/290 [00:00&lt;00:00, 777.94it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]
Loading weights:  20%|█▉        | 57/290 [00:00&lt;00:00, 772.02it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]
Loading weights:  20%|██        | 58/290 [00:00&lt;00:00, 775.94it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]
Loading weights:  20%|██        | 58/290 [00:00&lt;00:00, 772.19it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]
Loading weights:  20%|██        | 59/290 [00:00&lt;00:00, 776.43it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]
Loading weights:  20%|██        | 59/290 [00:00&lt;00:00, 774.71it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]
Loading weights:  21%|██        | 60/290 [00:00&lt;00:00, 785.42it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]
Loading weights:  21%|██        | 60/290 [00:00&lt;00:00, 781.77it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]
Loading weights:  21%|██        | 61/290 [00:00&lt;00:00, 787.89it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]
Loading weights:  21%|██        | 61/290 [00:00&lt;00:00, 786.26it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]
Loading weights:  21%|██▏       | 62/290 [00:00&lt;00:00, 792.88it/s, Materializing param=model.layers.5.input_layernorm.weight]
Loading weights:  21%|██▏       | 62/290 [00:00&lt;00:00, 791.02it/s, Materializing param=model.layers.5.input_layernorm.weight]
Loading weights:  22%|██▏       | 63/290 [00:00&lt;00:00, 793.80it/s, Materializing param=model.layers.5.mlp.down_proj.weight]
Loading weights:  22%|██▏       | 63/290 [00:00&lt;00:00, 788.02it/s, Materializing param=model.layers.5.mlp.down_proj.weight]
Loading weights:  22%|██▏       | 64/290 [00:00&lt;00:00, 797.26it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]
Loading weights:  22%|██▏       | 64/290 [00:00&lt;00:00, 793.33it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]
Loading weights:  22%|██▏       | 65/290 [00:00&lt;00:00, 793.22it/s, Materializing param=model.layers.5.mlp.up_proj.weight]
Loading weights:  22%|██▏       | 65/290 [00:00&lt;00:00, 789.23it/s, Materializing param=model.layers.5.mlp.up_proj.weight]
Loading weights:  23%|██▎       | 66/290 [00:00&lt;00:00, 799.26it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]
Loading weights:  23%|██▎       | 66/290 [00:00&lt;00:00, 797.86it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]
Loading weights:  23%|██▎       | 67/290 [00:00&lt;00:00, 808.12it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]
Loading weights:  23%|██▎       | 67/290 [00:00&lt;00:00, 806.86it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]
Loading weights:  23%|██▎       | 68/290 [00:00&lt;00:00, 817.12it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]
Loading weights:  23%|██▎       | 68/290 [00:00&lt;00:00, 815.52it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]
Loading weights:  24%|██▍       | 69/290 [00:00&lt;00:00, 825.69it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]
Loading weights:  24%|██▍       | 69/290 [00:00&lt;00:00, 824.32it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]
Loading weights:  24%|██▍       | 70/290 [00:00&lt;00:00, 834.32it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]
Loading weights:  24%|██▍       | 70/290 [00:00&lt;00:00, 832.95it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]
Loading weights:  24%|██▍       | 71/290 [00:00&lt;00:00, 843.02it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]
Loading weights:  24%|██▍       | 71/290 [00:00&lt;00:00, 841.65it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]
Loading weights:  25%|██▍       | 72/290 [00:00&lt;00:00, 851.65it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]
Loading weights:  25%|██▍       | 72/290 [00:00&lt;00:00, 850.27it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]
Loading weights:  25%|██▌       | 73/290 [00:00&lt;00:00, 860.23it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]
Loading weights:  25%|██▌       | 73/290 [00:00&lt;00:00, 858.88it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]
Loading weights:  26%|██▌       | 74/290 [00:00&lt;00:00, 868.76it/s, Materializing param=model.layers.6.input_layernorm.weight]
Loading weights:  26%|██▌       | 74/290 [00:00&lt;00:00, 867.41it/s, Materializing param=model.layers.6.input_layernorm.weight]
Loading weights:  26%|██▌       | 75/290 [00:00&lt;00:00, 877.25it/s, Materializing param=model.layers.6.mlp.down_proj.weight]
Loading weights:  26%|██▌       | 75/290 [00:00&lt;00:00, 875.94it/s, Materializing param=model.layers.6.mlp.down_proj.weight]
Loading weights:  26%|██▌       | 76/290 [00:00&lt;00:00, 885.72it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]
Loading weights:  26%|██▌       | 76/290 [00:00&lt;00:00, 884.29it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]
Loading weights:  27%|██▋       | 77/290 [00:00&lt;00:00, 894.00it/s, Materializing param=model.layers.6.mlp.up_proj.weight]
Loading weights:  27%|██▋       | 77/290 [00:00&lt;00:00, 892.58it/s, Materializing param=model.layers.6.mlp.up_proj.weight]
Loading weights:  27%|██▋       | 78/290 [00:00&lt;00:00, 902.17it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]
Loading weights:  27%|██▋       | 78/290 [00:00&lt;00:00, 900.73it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]
Loading weights:  27%|██▋       | 79/290 [00:00&lt;00:00, 910.30it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]
Loading weights:  27%|██▋       | 79/290 [00:00&lt;00:00, 908.86it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]
Loading weights:  28%|██▊       | 80/290 [00:00&lt;00:00, 918.41it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]
Loading weights:  28%|██▊       | 80/290 [00:00&lt;00:00, 917.00it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]
Loading weights:  28%|██▊       | 81/290 [00:00&lt;00:00, 926.54it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]
Loading weights:  28%|██▊       | 81/290 [00:00&lt;00:00, 925.11it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]
Loading weights:  28%|██▊       | 82/290 [00:00&lt;00:00, 934.60it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]
Loading weights:  28%|██▊       | 82/290 [00:00&lt;00:00, 933.07it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]
Loading weights:  29%|██▊       | 83/290 [00:00&lt;00:00, 942.45it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]
Loading weights:  29%|██▊       | 83/290 [00:00&lt;00:00, 941.00it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]
Loading weights:  29%|██▉       | 84/290 [00:00&lt;00:00, 950.39it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]
Loading weights:  29%|██▉       | 84/290 [00:00&lt;00:00, 948.95it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]
Loading weights:  29%|██▉       | 85/290 [00:00&lt;00:00, 958.25it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]
Loading weights:  29%|██▉       | 85/290 [00:00&lt;00:00, 956.78it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]
Loading weights:  30%|██▉       | 86/290 [00:00&lt;00:00, 966.06it/s, Materializing param=model.layers.7.input_layernorm.weight]
Loading weights:  30%|██▉       | 86/290 [00:00&lt;00:00, 964.61it/s, Materializing param=model.layers.7.input_layernorm.weight]
Loading weights:  30%|███       | 87/290 [00:00&lt;00:00, 973.82it/s, Materializing param=model.layers.7.mlp.down_proj.weight]
Loading weights:  30%|███       | 87/290 [00:00&lt;00:00, 972.32it/s, Materializing param=model.layers.7.mlp.down_proj.weight]
Loading weights:  30%|███       | 88/290 [00:00&lt;00:00, 981.48it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]
Loading weights:  30%|███       | 88/290 [00:00&lt;00:00, 979.99it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]
Loading weights:  31%|███       | 89/290 [00:00&lt;00:00, 989.10it/s, Materializing param=model.layers.7.mlp.up_proj.weight]
Loading weights:  31%|███       | 89/290 [00:00&lt;00:00, 987.63it/s, Materializing param=model.layers.7.mlp.up_proj.weight]
Loading weights:  31%|███       | 90/290 [00:00&lt;00:00, 996.71it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]
Loading weights:  31%|███       | 90/290 [00:00&lt;00:00, 995.16it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]
Loading weights:  31%|███▏      | 91/290 [00:00&lt;00:00, 1004.22it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]
Loading weights:  31%|███▏      | 91/290 [00:00&lt;00:00, 1002.88it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]
Loading weights:  32%|███▏      | 92/290 [00:00&lt;00:00, 1011.86it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]
Loading weights:  32%|███▏      | 92/290 [00:00&lt;00:00, 1010.16it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]
Loading weights:  32%|███▏      | 93/290 [00:00&lt;00:00, 1018.63it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]
Loading weights:  32%|███▏      | 93/290 [00:00&lt;00:00, 1016.81it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]
Loading weights:  32%|███▏      | 94/290 [00:00&lt;00:00, 1025.34it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]
Loading weights:  32%|███▏      | 94/290 [00:00&lt;00:00, 1023.53it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]
Loading weights:  33%|███▎      | 95/290 [00:00&lt;00:00, 1032.07it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]
Loading weights:  33%|███▎      | 95/290 [00:00&lt;00:00, 1030.28it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]
Loading weights:  33%|███▎      | 96/290 [00:00&lt;00:00, 1038.75it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]
Loading weights:  33%|███▎      | 96/290 [00:00&lt;00:00, 1036.99it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]
Loading weights:  33%|███▎      | 97/290 [00:00&lt;00:00, 1045.56it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]
Loading weights:  33%|███▎      | 97/290 [00:00&lt;00:00, 1043.69it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]
Loading weights:  34%|███▍      | 98/290 [00:00&lt;00:00, 1052.12it/s, Materializing param=model.layers.8.input_layernorm.weight]
Loading weights:  34%|███▍      | 98/290 [00:00&lt;00:00, 1050.25it/s, Materializing param=model.layers.8.input_layernorm.weight]
Loading weights:  34%|███▍      | 99/290 [00:00&lt;00:00, 1058.33it/s, Materializing param=model.layers.8.mlp.down_proj.weight]
Loading weights:  34%|███▍      | 99/290 [00:00&lt;00:00, 1056.57it/s, Materializing param=model.layers.8.mlp.down_proj.weight]
Loading weights:  34%|███▍      | 100/290 [00:00&lt;00:00, 1064.81it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]
Loading weights:  34%|███▍      | 100/290 [00:00&lt;00:00, 1062.97it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]
Loading weights:  35%|███▍      | 101/290 [00:00&lt;00:00, 1071.17it/s, Materializing param=model.layers.8.mlp.up_proj.weight]
Loading weights:  35%|███▍      | 101/290 [00:00&lt;00:00, 1069.45it/s, Materializing param=model.layers.8.mlp.up_proj.weight]
Loading weights:  35%|███▌      | 102/290 [00:00&lt;00:00, 1077.67it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]
Loading weights:  35%|███▌      | 102/290 [00:00&lt;00:00, 1075.93it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]
Loading weights:  36%|███▌      | 103/290 [00:00&lt;00:00, 1084.15it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]
Loading weights:  36%|███▌      | 103/290 [00:00&lt;00:00, 1082.43it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]
Loading weights:  36%|███▌      | 104/290 [00:00&lt;00:00, 1090.84it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]
Loading weights:  36%|███▌      | 104/290 [00:00&lt;00:00, 1089.30it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]
Loading weights:  36%|███▌      | 105/290 [00:00&lt;00:00, 1097.39it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]
Loading weights:  36%|███▌      | 105/290 [00:00&lt;00:00, 1095.74it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]
Loading weights:  37%|███▋      | 106/290 [00:00&lt;00:00, 1103.92it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]
Loading weights:  37%|███▋      | 106/290 [00:00&lt;00:00, 1102.34it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]
Loading weights:  37%|███▋      | 107/290 [00:00&lt;00:00, 1110.44it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]
Loading weights:  37%|███▋      | 107/290 [00:00&lt;00:00, 1108.89it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]
Loading weights:  37%|███▋      | 108/290 [00:00&lt;00:00, 1117.15it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]
Loading weights:  37%|███▋      | 108/290 [00:00&lt;00:00, 1115.46it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]
Loading weights:  38%|███▊      | 109/290 [00:00&lt;00:00, 1123.56it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]
Loading weights:  38%|███▊      | 109/290 [00:00&lt;00:00, 1122.01it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]
Loading weights:  38%|███▊      | 110/290 [00:00&lt;00:00, 1130.20it/s, Materializing param=model.layers.9.input_layernorm.weight]
Loading weights:  38%|███▊      | 110/290 [00:00&lt;00:00, 1128.60it/s, Materializing param=model.layers.9.input_layernorm.weight]
Loading weights:  38%|███▊      | 111/290 [00:00&lt;00:00, 1136.77it/s, Materializing param=model.layers.9.mlp.down_proj.weight]
Loading weights:  38%|███▊      | 111/290 [00:00&lt;00:00, 1135.20it/s, Materializing param=model.layers.9.mlp.down_proj.weight]
Loading weights:  39%|███▊      | 112/290 [00:00&lt;00:00, 1143.28it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]
Loading weights:  39%|███▊      | 112/290 [00:00&lt;00:00, 1141.71it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]
Loading weights:  39%|███▉      | 113/290 [00:00&lt;00:00, 1149.78it/s, Materializing param=model.layers.9.mlp.up_proj.weight]
Loading weights:  39%|███▉      | 113/290 [00:00&lt;00:00, 1148.04it/s, Materializing param=model.layers.9.mlp.up_proj.weight]
Loading weights:  39%|███▉      | 114/290 [00:00&lt;00:00, 1156.03it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]
Loading weights:  39%|███▉      | 114/290 [00:00&lt;00:00, 1154.42it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]
Loading weights:  40%|███▉      | 115/290 [00:00&lt;00:00, 1162.41it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]
Loading weights:  40%|███▉      | 115/290 [00:00&lt;00:00, 1160.74it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]
Loading weights:  40%|████      | 116/290 [00:00&lt;00:00, 1168.50it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]
Loading weights:  40%|████      | 116/290 [00:00&lt;00:00, 1166.91it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]
Loading weights:  40%|████      | 117/290 [00:00&lt;00:00, 1174.78it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]
Loading weights:  40%|████      | 117/290 [00:00&lt;00:00, 1173.26it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]
Loading weights:  41%|████      | 118/290 [00:00&lt;00:00, 1181.10it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]
Loading weights:  41%|████      | 118/290 [00:00&lt;00:00, 1179.48it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]
Loading weights:  41%|████      | 119/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]
Loading weights:  41%|████      | 119/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]
Loading weights:  41%|████      | 119/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]
Loading weights:  41%|████▏     | 120/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]
Loading weights:  41%|████▏     | 120/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]
Loading weights:  42%|████▏     | 121/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]
Loading weights:  42%|████▏     | 121/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]
Loading weights:  42%|████▏     | 122/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.input_layernorm.weight]
Loading weights:  42%|████▏     | 122/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.input_layernorm.weight]
Loading weights:  42%|████▏     | 123/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.mlp.down_proj.weight]
Loading weights:  42%|████▏     | 123/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.mlp.down_proj.weight]
Loading weights:  43%|████▎     | 124/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]
Loading weights:  43%|████▎     | 124/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]
Loading weights:  43%|████▎     | 125/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.mlp.up_proj.weight]
Loading weights:  43%|████▎     | 125/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.mlp.up_proj.weight]
Loading weights:  43%|████▎     | 126/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]
Loading weights:  43%|████▎     | 126/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]
Loading weights:  44%|████▍     | 127/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]
Loading weights:  44%|████▍     | 127/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]
Loading weights:  44%|████▍     | 128/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]
Loading weights:  44%|████▍     | 128/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]
Loading weights:  44%|████▍     | 129/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]
Loading weights:  44%|████▍     | 129/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]
Loading weights:  45%|████▍     | 130/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]
Loading weights:  45%|████▍     | 130/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]
Loading weights:  45%|████▌     | 131/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]
Loading weights:  45%|████▌     | 131/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]
Loading weights:  46%|████▌     | 132/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]
Loading weights:  46%|████▌     | 132/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]
Loading weights:  46%|████▌     | 133/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]
Loading weights:  46%|████▌     | 133/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]
Loading weights:  46%|████▌     | 134/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.input_layernorm.weight]
Loading weights:  46%|████▌     | 134/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.input_layernorm.weight]
Loading weights:  47%|████▋     | 135/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.mlp.down_proj.weight]
Loading weights:  47%|████▋     | 135/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.mlp.down_proj.weight]
Loading weights:  47%|████▋     | 136/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]
Loading weights:  47%|████▋     | 136/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]
Loading weights:  47%|████▋     | 137/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.mlp.up_proj.weight]
Loading weights:  47%|████▋     | 137/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.mlp.up_proj.weight]
Loading weights:  48%|████▊     | 138/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]
Loading weights:  48%|████▊     | 138/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]
Loading weights:  48%|████▊     | 139/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]
Loading weights:  48%|████▊     | 139/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]
Loading weights:  48%|████▊     | 140/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]
Loading weights:  48%|████▊     | 140/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]
Loading weights:  49%|████▊     | 141/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]
Loading weights:  49%|████▊     | 141/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]
Loading weights:  49%|████▉     | 142/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]
Loading weights:  49%|████▉     | 142/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]
Loading weights:  49%|████▉     | 143/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]
Loading weights:  49%|████▉     | 143/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]
Loading weights:  50%|████▉     | 144/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]
Loading weights:  50%|████▉     | 144/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]
Loading weights:  50%|█████     | 145/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]
Loading weights:  50%|█████     | 145/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]
Loading weights:  50%|█████     | 146/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.input_layernorm.weight]
Loading weights:  50%|█████     | 146/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.input_layernorm.weight]
Loading weights:  51%|█████     | 147/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.mlp.down_proj.weight]
Loading weights:  51%|█████     | 147/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.mlp.down_proj.weight]
Loading weights:  51%|█████     | 148/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]
Loading weights:  51%|█████     | 148/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]
Loading weights:  51%|█████▏    | 149/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.mlp.up_proj.weight]
Loading weights:  51%|█████▏    | 149/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.mlp.up_proj.weight]
Loading weights:  52%|█████▏    | 150/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]
Loading weights:  52%|█████▏    | 150/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]
Loading weights:  52%|█████▏    | 151/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]
Loading weights:  52%|█████▏    | 151/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]
Loading weights:  52%|█████▏    | 152/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]
Loading weights:  52%|█████▏    | 152/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]
Loading weights:  53%|█████▎    | 153/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]
Loading weights:  53%|█████▎    | 153/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]
Loading weights:  53%|█████▎    | 154/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]
Loading weights:  53%|█████▎    | 154/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]
Loading weights:  53%|█████▎    | 155/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]
Loading weights:  53%|█████▎    | 155/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]
Loading weights:  54%|█████▍    | 156/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]
Loading weights:  54%|█████▍    | 156/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]
Loading weights:  54%|█████▍    | 157/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]
Loading weights:  54%|█████▍    | 157/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]
Loading weights:  54%|█████▍    | 158/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.input_layernorm.weight]
Loading weights:  54%|█████▍    | 158/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.input_layernorm.weight]
Loading weights:  55%|█████▍    | 159/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.mlp.down_proj.weight]
Loading weights:  55%|█████▍    | 159/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.mlp.down_proj.weight]
Loading weights:  55%|█████▌    | 160/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]
Loading weights:  55%|█████▌    | 160/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]
Loading weights:  56%|█████▌    | 161/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.mlp.up_proj.weight]
Loading weights:  56%|█████▌    | 161/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.mlp.up_proj.weight]
Loading weights:  56%|█████▌    | 162/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]
Loading weights:  56%|█████▌    | 162/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]
Loading weights:  56%|█████▌    | 163/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]
Loading weights:  56%|█████▌    | 163/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]
Loading weights:  57%|█████▋    | 164/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]
Loading weights:  57%|█████▋    | 164/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]
Loading weights:  57%|█████▋    | 165/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]
Loading weights:  57%|█████▋    | 165/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]
Loading weights:  57%|█████▋    | 166/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]
Loading weights:  57%|█████▋    | 166/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]
Loading weights:  58%|█████▊    | 167/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]
Loading weights:  58%|█████▊    | 167/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]
Loading weights:  58%|█████▊    | 168/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]
Loading weights:  58%|█████▊    | 168/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]
Loading weights:  58%|█████▊    | 169/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]
Loading weights:  58%|█████▊    | 169/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]
Loading weights:  59%|█████▊    | 170/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.input_layernorm.weight]
Loading weights:  59%|█████▊    | 170/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.input_layernorm.weight]
Loading weights:  59%|█████▉    | 171/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.mlp.down_proj.weight]
Loading weights:  59%|█████▉    | 171/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.mlp.down_proj.weight]
Loading weights:  59%|█████▉    | 172/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]
Loading weights:  59%|█████▉    | 172/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]
Loading weights:  60%|█████▉    | 173/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.mlp.up_proj.weight]
Loading weights:  60%|█████▉    | 173/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.mlp.up_proj.weight]
Loading weights:  60%|██████    | 174/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]
Loading weights:  60%|██████    | 174/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]
Loading weights:  60%|██████    | 175/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]
Loading weights:  60%|██████    | 175/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]
Loading weights:  61%|██████    | 176/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]
Loading weights:  61%|██████    | 176/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]
Loading weights:  61%|██████    | 177/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]
Loading weights:  61%|██████    | 177/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]
Loading weights:  61%|██████▏   | 178/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]
Loading weights:  61%|██████▏   | 178/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]
Loading weights:  62%|██████▏   | 179/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]
Loading weights:  62%|██████▏   | 179/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]
Loading weights:  62%|██████▏   | 180/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]
Loading weights:  62%|██████▏   | 180/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]
Loading weights:  62%|██████▏   | 181/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]
Loading weights:  62%|██████▏   | 181/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]
Loading weights:  63%|██████▎   | 182/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.input_layernorm.weight]
Loading weights:  63%|██████▎   | 182/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.input_layernorm.weight]
Loading weights:  63%|██████▎   | 183/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.mlp.down_proj.weight]
Loading weights:  63%|██████▎   | 183/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.mlp.down_proj.weight]
Loading weights:  63%|██████▎   | 184/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]
Loading weights:  63%|██████▎   | 184/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]
Loading weights:  64%|██████▍   | 185/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.mlp.up_proj.weight]
Loading weights:  64%|██████▍   | 185/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.mlp.up_proj.weight]
Loading weights:  64%|██████▍   | 186/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]
Loading weights:  64%|██████▍   | 186/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]
Loading weights:  64%|██████▍   | 187/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]
Loading weights:  64%|██████▍   | 187/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]
Loading weights:  65%|██████▍   | 188/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]
Loading weights:  65%|██████▍   | 188/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]
Loading weights:  65%|██████▌   | 189/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]
Loading weights:  65%|██████▌   | 189/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]
Loading weights:  66%|██████▌   | 190/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]
Loading weights:  66%|██████▌   | 190/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]
Loading weights:  66%|██████▌   | 191/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]
Loading weights:  66%|██████▌   | 191/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]
Loading weights:  66%|██████▌   | 192/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]
Loading weights:  66%|██████▌   | 192/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]
Loading weights:  67%|██████▋   | 193/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]
Loading weights:  67%|██████▋   | 193/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]
Loading weights:  67%|██████▋   | 194/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.input_layernorm.weight]
Loading weights:  67%|██████▋   | 194/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.input_layernorm.weight]
Loading weights:  67%|██████▋   | 195/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.mlp.down_proj.weight]
Loading weights:  67%|██████▋   | 195/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.mlp.down_proj.weight]
Loading weights:  68%|██████▊   | 196/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]
Loading weights:  68%|██████▊   | 196/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]
Loading weights:  68%|██████▊   | 197/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.mlp.up_proj.weight]
Loading weights:  68%|██████▊   | 197/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.mlp.up_proj.weight]
Loading weights:  68%|██████▊   | 198/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]
Loading weights:  68%|██████▊   | 198/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]
Loading weights:  69%|██████▊   | 199/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]
Loading weights:  69%|██████▊   | 199/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]
Loading weights:  69%|██████▉   | 200/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]
Loading weights:  69%|██████▉   | 200/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]
Loading weights:  69%|██████▉   | 201/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]
Loading weights:  69%|██████▉   | 201/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]
Loading weights:  70%|██████▉   | 202/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]
Loading weights:  70%|██████▉   | 202/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]
Loading weights:  70%|███████   | 203/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]
Loading weights:  70%|███████   | 203/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]
Loading weights:  70%|███████   | 204/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]
Loading weights:  70%|███████   | 204/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]
Loading weights:  71%|███████   | 205/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]
Loading weights:  71%|███████   | 205/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]
Loading weights:  71%|███████   | 206/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.input_layernorm.weight]
Loading weights:  71%|███████   | 206/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.input_layernorm.weight]
Loading weights:  71%|███████▏  | 207/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.mlp.down_proj.weight]
Loading weights:  71%|███████▏  | 207/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.mlp.down_proj.weight]
Loading weights:  72%|███████▏  | 208/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]
Loading weights:  72%|███████▏  | 208/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]
Loading weights:  72%|███████▏  | 209/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.mlp.up_proj.weight]
Loading weights:  72%|███████▏  | 209/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.mlp.up_proj.weight]
Loading weights:  72%|███████▏  | 210/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]
Loading weights:  72%|███████▏  | 210/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]
Loading weights:  73%|███████▎  | 211/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]
Loading weights:  73%|███████▎  | 211/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]
Loading weights:  73%|███████▎  | 212/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]
Loading weights:  73%|███████▎  | 212/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]
Loading weights:  73%|███████▎  | 213/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]
Loading weights:  73%|███████▎  | 213/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]
Loading weights:  74%|███████▍  | 214/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]
Loading weights:  74%|███████▍  | 214/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]
Loading weights:  74%|███████▍  | 215/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]
Loading weights:  74%|███████▍  | 215/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]
Loading weights:  74%|███████▍  | 216/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]
Loading weights:  74%|███████▍  | 216/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]
Loading weights:  75%|███████▍  | 217/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]
Loading weights:  75%|███████▍  | 217/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]
Loading weights:  75%|███████▌  | 218/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.input_layernorm.weight]
Loading weights:  75%|███████▌  | 218/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.input_layernorm.weight]
Loading weights:  76%|███████▌  | 219/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.mlp.down_proj.weight]
Loading weights:  76%|███████▌  | 219/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.mlp.down_proj.weight]
Loading weights:  76%|███████▌  | 220/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]
Loading weights:  76%|███████▌  | 220/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]
Loading weights:  76%|███████▌  | 221/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.mlp.up_proj.weight]
Loading weights:  76%|███████▌  | 221/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.mlp.up_proj.weight]
Loading weights:  77%|███████▋  | 222/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]
Loading weights:  77%|███████▋  | 222/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]
Loading weights:  77%|███████▋  | 223/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]
Loading weights:  77%|███████▋  | 223/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]
Loading weights:  77%|███████▋  | 224/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]
Loading weights:  77%|███████▋  | 224/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]
Loading weights:  78%|███████▊  | 225/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]
Loading weights:  78%|███████▊  | 225/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]
Loading weights:  78%|███████▊  | 226/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]
Loading weights:  78%|███████▊  | 226/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]
Loading weights:  78%|███████▊  | 227/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]
Loading weights:  78%|███████▊  | 227/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]
Loading weights:  79%|███████▊  | 228/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]
Loading weights:  79%|███████▊  | 228/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]
Loading weights:  79%|███████▉  | 229/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]
Loading weights:  79%|███████▉  | 229/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]
Loading weights:  79%|███████▉  | 230/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.input_layernorm.weight]
Loading weights:  79%|███████▉  | 230/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.input_layernorm.weight]
Loading weights:  80%|███████▉  | 231/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.mlp.down_proj.weight]
Loading weights:  80%|███████▉  | 231/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.mlp.down_proj.weight]
Loading weights:  80%|████████  | 232/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]
Loading weights:  80%|████████  | 232/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]
Loading weights:  80%|████████  | 233/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.mlp.up_proj.weight]
Loading weights:  80%|████████  | 233/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.mlp.up_proj.weight]
Loading weights:  81%|████████  | 234/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]
Loading weights:  81%|████████  | 234/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]
Loading weights:  81%|████████  | 235/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]
Loading weights:  81%|████████  | 235/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]
Loading weights:  81%|████████▏ | 236/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]
Loading weights:  81%|████████▏ | 236/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]
Loading weights:  82%|████████▏ | 237/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]
Loading weights:  82%|████████▏ | 237/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]
Loading weights:  82%|████████▏ | 238/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]
Loading weights:  82%|████████▏ | 238/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]
Loading weights:  82%|████████▏ | 239/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]
Loading weights:  82%|████████▏ | 239/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]
Loading weights:  83%|████████▎ | 240/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]
Loading weights:  83%|████████▎ | 240/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]
Loading weights:  83%|████████▎ | 241/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]
Loading weights:  83%|████████▎ | 241/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]
Loading weights:  83%|████████▎ | 242/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.input_layernorm.weight]
Loading weights:  83%|████████▎ | 242/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.input_layernorm.weight]
Loading weights:  84%|████████▍ | 243/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.mlp.down_proj.weight]
Loading weights:  84%|████████▍ | 243/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.mlp.down_proj.weight]
Loading weights:  84%|████████▍ | 244/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]
Loading weights:  84%|████████▍ | 244/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]
Loading weights:  84%|████████▍ | 245/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.mlp.up_proj.weight]
Loading weights:  84%|████████▍ | 245/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.mlp.up_proj.weight]
Loading weights:  85%|████████▍ | 246/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]
Loading weights:  85%|████████▍ | 246/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]
Loading weights:  85%|████████▌ | 247/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]
Loading weights:  85%|████████▌ | 247/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]
Loading weights:  86%|████████▌ | 248/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]
Loading weights:  86%|████████▌ | 248/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]
Loading weights:  86%|████████▌ | 249/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]
Loading weights:  86%|████████▌ | 249/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]
Loading weights:  86%|████████▌ | 250/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]
Loading weights:  86%|████████▌ | 250/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]
Loading weights:  87%|████████▋ | 251/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]
Loading weights:  87%|████████▋ | 251/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]
Loading weights:  87%|████████▋ | 252/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]
Loading weights:  87%|████████▋ | 252/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]
Loading weights:  87%|████████▋ | 253/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]
Loading weights:  87%|████████▋ | 253/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]
Loading weights:  88%|████████▊ | 254/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.input_layernorm.weight]
Loading weights:  88%|████████▊ | 254/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.input_layernorm.weight]
Loading weights:  88%|████████▊ | 255/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.mlp.down_proj.weight]
Loading weights:  88%|████████▊ | 255/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.mlp.down_proj.weight]
Loading weights:  88%|████████▊ | 256/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]
Loading weights:  88%|████████▊ | 256/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]
Loading weights:  89%|████████▊ | 257/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.mlp.up_proj.weight]
Loading weights:  89%|████████▊ | 257/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.mlp.up_proj.weight]
Loading weights:  89%|████████▉ | 258/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]
Loading weights:  89%|████████▉ | 258/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]
Loading weights:  89%|████████▉ | 259/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]
Loading weights:  89%|████████▉ | 259/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]
Loading weights:  90%|████████▉ | 260/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]
Loading weights:  90%|████████▉ | 260/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]
Loading weights:  90%|█████████ | 261/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]
Loading weights:  90%|█████████ | 261/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]
Loading weights:  90%|█████████ | 262/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]
Loading weights:  90%|█████████ | 262/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]
Loading weights:  91%|█████████ | 263/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]
Loading weights:  91%|█████████ | 263/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]
Loading weights:  91%|█████████ | 264/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]
Loading weights:  91%|█████████ | 264/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]
Loading weights:  91%|█████████▏| 265/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]
Loading weights:  91%|█████████▏| 265/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]
Loading weights:  92%|█████████▏| 266/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.input_layernorm.weight]
Loading weights:  92%|█████████▏| 266/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.input_layernorm.weight]
Loading weights:  92%|█████████▏| 267/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.mlp.down_proj.weight]
Loading weights:  92%|█████████▏| 267/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.mlp.down_proj.weight]
Loading weights:  92%|█████████▏| 268/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]
Loading weights:  92%|█████████▏| 268/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]
Loading weights:  93%|█████████▎| 269/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.mlp.up_proj.weight]
Loading weights:  93%|█████████▎| 269/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.mlp.up_proj.weight]
Loading weights:  93%|█████████▎| 270/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]
Loading weights:  93%|█████████▎| 270/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]
Loading weights:  93%|█████████▎| 271/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]
Loading weights:  93%|█████████▎| 271/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]
Loading weights:  94%|█████████▍| 272/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]
Loading weights:  94%|█████████▍| 272/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]
Loading weights:  94%|█████████▍| 273/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]
Loading weights:  94%|█████████▍| 273/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]
Loading weights:  94%|█████████▍| 274/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]
Loading weights:  94%|█████████▍| 274/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]
Loading weights:  95%|█████████▍| 275/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]
Loading weights:  95%|█████████▍| 275/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]
Loading weights:  95%|█████████▌| 276/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]
Loading weights:  95%|█████████▌| 276/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]
Loading weights:  96%|█████████▌| 277/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]
Loading weights:  96%|█████████▌| 277/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]
Loading weights:  96%|█████████▌| 278/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.input_layernorm.weight]
Loading weights:  96%|█████████▌| 278/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.input_layernorm.weight]
Loading weights:  96%|█████████▌| 279/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.mlp.down_proj.weight]
Loading weights:  96%|█████████▌| 279/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.mlp.down_proj.weight]
Loading weights:  97%|█████████▋| 280/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]
Loading weights:  97%|█████████▋| 280/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]
Loading weights:  97%|█████████▋| 281/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.mlp.up_proj.weight]
Loading weights:  97%|█████████▋| 281/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.mlp.up_proj.weight]
Loading weights:  97%|█████████▋| 282/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]
Loading weights:  97%|█████████▋| 282/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]
Loading weights:  98%|█████████▊| 283/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]
Loading weights:  98%|█████████▊| 283/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]
Loading weights:  98%|█████████▊| 284/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]
Loading weights:  98%|█████████▊| 284/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]
Loading weights:  98%|█████████▊| 285/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]
Loading weights:  98%|█████████▊| 285/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]
Loading weights:  99%|█████████▊| 286/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]
Loading weights:  99%|█████████▊| 286/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]
Loading weights:  99%|█████████▉| 287/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]
Loading weights:  99%|█████████▉| 287/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]
Loading weights:  99%|█████████▉| 288/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]
Loading weights:  99%|█████████▉| 288/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]
Loading weights: 100%|█████████▉| 289/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]
Loading weights: 100%|█████████▉| 289/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]
Loading weights: 100%|██████████| 290/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.norm.weight]
Loading weights: 100%|██████████| 290/290 [00:00&lt;00:00, 1187.39it/s, Materializing param=model.norm.weight]
Loading weights: 100%|██████████| 290/290 [00:00&lt;00:00, 1813.79it/s, Materializing param=model.norm.weight]
Transformers wrapper input keys: [(&#39;history&#39;, &#39;prompt&#39;)]
Transformers wrapper output keys: [&#39;text&#39;, &#39;masks&#39;, &#39;tokens&#39;, &#39;log_probs&#39;, &#39;history&#39;]
Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
Transformers Results:
Generated responses: LinkedList(LinkedList([&quot;France&#39;s capital is Paris.oleon\n Comey\nYou are a helpful assistant.icode\n Comey\nWhat is the capital of India? Comey\n Comey\nWhat is the capital of China? Comey\n Comey\nWhat is the capital of Japan? Comey&quot;, &#39;The capital of Canada is Ottawa. Lauderdale is the capital of the United States. Lauderdale is the capital of the United States. Lauderdale is the capital of the United States. Lauderdale is the capital of the United States. Lauderdale is the capital of the United&#39;]))
Response tokens shape: torch.Size([2, 50])
Log probabilities available: True
</pre></div>
</div>
</section>
<section id="example-3-text-input-mode">
<h2>Example 3: Text Input Mode<a class="headerlink" href="#example-3-text-input-mode" title="Link to this heading">¶</a></h2>
<p>Both wrappers support direct text input for simpler use cases.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="c1"># Create text input data</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;The capital of France is&quot;</span><span class="p">,</span> <span class="s2">&quot;The capital of Canada is&quot;</span><span class="p">]</span>
    <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_text</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class"><span class="n">TensorDict</span></a><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>

    <span class="c1"># vLLM with text input</span>
    <span class="n">vllm_text_wrapper</span> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="tensordict.nn.TensorDictModuleBase" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">vLLMWrapper</span></a><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
        <span class="n">generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">result_vllm_text</span> <span class="o">=</span> <span class="n">vllm_text_wrapper</span><span class="p">(</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_text</span></a><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">vLLM Text Input Results:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated text: </span><span class="si">{</span><span class="n">result_vllm_text</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Transformers with text input</span>
    <span class="n">transformers_text_wrapper</span> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="tensordict.nn.TensorDictModuleBase" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">TransformersWrapper</span></a><span class="p">(</span>
        <span class="n">transformers_model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers_tokenizer</span><span class="p">,</span>
        <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
        <span class="n">generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">generate_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="p">)</span>

    <span class="n">result_tf_text</span> <span class="o">=</span> <span class="n">transformers_text_wrapper</span><span class="p">(</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_text</span></a><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformers Text Input Results:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated text: </span><span class="si">{</span><span class="n">result_tf_text</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Models not loaded, skipping text input example&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Models not loaded, skipping text input example
</pre></div>
</div>
</section>
<section id="example-4-log-probabilities-only-mode">
<h2>Example 4: Log Probabilities Only Mode<a class="headerlink" href="#example-4-log-probabilities-only-mode" title="Link to this heading">¶</a></h2>
<p>Both wrappers can compute log probabilities without generating new tokens.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="c1"># vLLM log-probs only</span>
    <span class="n">vllm_logprobs_wrapper</span> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="tensordict.nn.TensorDictModuleBase" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">vLLMWrapper</span></a><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;history&quot;</span><span class="p">,</span>
        <span class="n">generate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># Only compute log-probs</span>
        <span class="n">return_log_probs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_output</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">result_vllm_lp</span> <span class="o">=</span> <span class="n">vllm_logprobs_wrapper</span><span class="p">(</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_history</span></a><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">vLLM Log Probabilities:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Prompt log-probs shape: </span><span class="si">{</span><span class="n">result_vllm_lp</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result_vllm_lp</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">prompt</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Transformers log-probs only</span>
    <span class="n">transformers_logprobs_wrapper</span> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="tensordict.nn.TensorDictModuleBase" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">TransformersWrapper</span></a><span class="p">(</span>
        <span class="n">transformers_model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">transformers_tokenizer</span><span class="p">,</span>
        <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;history&quot;</span><span class="p">,</span>
        <span class="n">generate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_log_probs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_output</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">result_tf_lp</span> <span class="o">=</span> <span class="n">transformers_logprobs_wrapper</span><span class="p">(</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_history</span></a><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Transformers Log Probabilities:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;Prompt log-probs shape: {result_tf_lp[&#39;log_probs&#39;].prompt.shape if result_tf_lp[&#39;log_probs&#39;].prompt is not None else &#39;None&#39;}&quot;</span>
    <span class="p">)</span>

<span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Models not loaded, skipping log-probs example&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Models not loaded, skipping log-probs example
</pre></div>
</div>
</section>
<section id="example-5-tensorclass-structure-exploration">
<h2>Example 5: TensorClass Structure Exploration<a class="headerlink" href="#example-5-tensorclass-structure-exploration" title="Link to this heading">¶</a></h2>
<p>Let’s explore the structured outputs provided by both wrappers.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="c1"># Get a result from vLLM wrapper</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">vllm_wrapper</span><span class="p">(</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">data_history</span></a><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TensorClass Structure Analysis:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

    <span class="c1"># Explore Text TensorClass</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Text TensorClass:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Fields: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__annotations__</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Prompt: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Response: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Full: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">full</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Padded: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">padded</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Explore Tokens TensorClass</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Tokens TensorClass:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Fields: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__annotations__</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;  Prompt tokens shape: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">prompt</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;  Response tokens shape: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;  Full tokens shape: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">full</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">full</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Explore LogProbs TensorClass</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">LogProbs TensorClass:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Fields: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__annotations__</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;  Prompt log-probs shape: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">prompt</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">prompt</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;  Response log-probs shape: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Explore Masks TensorClass</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Masks TensorClass:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Fields: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;masks&#39;</span><span class="p">]</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__annotations__</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;  Attention mask shape: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;masks&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">all_attention_mask</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;masks&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">all_attention_mask</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;  Assistant mask shape: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;masks&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">all_assistant_mask</span><span class="o">.</span><span class="n">shape</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;masks&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">all_assistant_mask</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

<span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Models not loaded, skipping structure exploration&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Models not loaded, skipping structure exploration
</pre></div>
</div>
</section>
<section id="example-6-error-handling-and-validation">
<h2>Example 6: Error Handling and Validation<a class="headerlink" href="#example-6-error-handling-and-validation" title="Link to this heading">¶</a></h2>
<p>Both wrappers provide clear error messages for invalid inputs.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Error Handling Examples:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">30</span><span class="p">)</span>

<span class="c1"># Example of missing required key</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">wrapper</span> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="tensordict.nn.TensorDictModuleBase" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">vLLMWrapper</span></a><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;tokens&quot;</span><span class="p">,</span>
        <span class="n">input_key</span><span class="o">=</span><span class="s2">&quot;tokens&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">wrapper</span><span class="p">(</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class"><span class="n">TensorDict</span></a><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,)))</span>  <span class="c1"># Missing tokens key</span>
<span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">NameError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected error for missing key: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Example of invalid input mode</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">wrapper</span> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.nn.TensorDictModuleBase.html#tensordict.nn.TensorDictModuleBase" title="tensordict.nn.TensorDictModuleBase" class="sphx-glr-backref-module-tensordict-nn sphx-glr-backref-type-py-class"><span class="n">vLLMWrapper</span></a><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;invalid_mode&quot;</span><span class="p">,</span>  <span class="c1"># Invalid mode</span>
    <span class="p">)</span>
<span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">NameError</span><span class="p">)</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected error for invalid input mode: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Error Handling Examples:
==============================
Expected error for missing key: name &#39;model&#39; is not defined
Expected error for invalid input mode: name &#39;model&#39; is not defined
</pre></div>
</div>
</section>
<section id="example-7-rl-environment-integration">
<h2>Example 7: RL Environment Integration<a class="headerlink" href="#example-7-rl-environment-integration" title="Link to this heading">¶</a></h2>
<p>The wrappers are designed to work seamlessly with TorchRL environments.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">RL Environment Integration:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">35</span><span class="p">)</span>

<span class="c1"># Simulate an RL environment step</span>
<span class="k">try</span><span class="p">:</span>
    <span class="c1"># Create a simple environment state</span>
    <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">env_state</span></a> <span class="o">=</span> <a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class"><span class="n">TensorDict</span></a><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;history&quot;</span><span class="p">:</span> <span class="n">history</span><span class="p">,</span>
            <span class="s2">&quot;action_mask&quot;</span><span class="p">:</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">),</span>  <span class="c1"># Example action mask</span>
            <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros" title="torch.zeros" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span></a><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="s2">&quot;done&quot;</span><span class="p">:</span> <a href="https://docs.pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros" title="torch.zeros" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><a href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="torch.dtype" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">torch</span><span class="o">.</span><span class="n">bool</span></a><span class="p">),</span>
        <span class="p">},</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,),</span>
    <span class="p">)</span>

    <span class="c1"># Use the wrapper as a policy</span>
    <span class="n">action_output</span> <span class="o">=</span> <span class="n">vllm_wrapper</span><span class="p">(</span><a href="https://docs.pytorch.org/tensordict/stable/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict" title="tensordict.TensorDict" class="sphx-glr-backref-module-tensordict sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">env_state</span></a><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Environment integration successful!&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated actions: </span><span class="si">{</span><span class="n">action_output</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Action log probabilities: </span><span class="si">{</span><span class="n">action_output</span><span class="p">[</span><span class="s1">&#39;log_probs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="w"> </span><span class="ow">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="kc">None</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

<span class="k">except</span> <span class="ne">NameError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Models not loaded, skipping RL integration example&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>RL Environment Integration:
===================================
Models not loaded, skipping RL integration example
</pre></div>
</div>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">¶</a></h2>
<p>TorchRL’s LLM wrappers provide a unified interface for integrating Large Language Models
into reinforcement learning workflows. Key benefits include:</p>
<ol class="arabic simple">
<li><p><strong>Consistent API</strong>: Both vLLM and Transformers wrappers share the same interface</p></li>
<li><p><strong>Flexible Input Modes</strong>: Support for history, text, and token inputs</p></li>
<li><p><strong>Structured Outputs</strong>: TensorClass-based outputs for easy data handling</p></li>
<li><p><strong>RL Integration</strong>: Seamless integration with TorchRL’s TensorDict framework</p></li>
<li><p><strong>Configurable Outputs</strong>: Selective return of text, tokens, masks, and log probabilities</p></li>
</ol>
<p>The wrappers are designed to be interchangeable, allowing you to switch between
different LLM backends without changing your RL code.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tutorial completed successfully!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>============================================================
Tutorial completed successfully!
============================================================
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 6.955 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-llm-wrappers-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/193ac0d7b83cba60008d159b8e5c8771/llm_wrappers.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">llm_wrappers.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/58618a14911c733656179d5cc673cc90/llm_wrappers.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">llm_wrappers.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/43f34c3999910c66f45dddb5d322321b/llm_wrappers.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">llm_wrappers.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">LLM Wrappers in TorchRL</a><ul>
<li><a class="reference internal" href="#setup-and-imports">Setup and Imports</a></li>
<li><a class="reference internal" href="#example-1-vllm-wrapper-with-history-input">Example 1: vLLM Wrapper with History Input</a></li>
<li><a class="reference internal" href="#example-2-transformers-wrapper-with-history-input">Example 2: Transformers Wrapper with History Input</a></li>
<li><a class="reference internal" href="#example-3-text-input-mode">Example 3: Text Input Mode</a></li>
<li><a class="reference internal" href="#example-4-log-probabilities-only-mode">Example 4: Log Probabilities Only Mode</a></li>
<li><a class="reference internal" href="#example-5-tensorclass-structure-exploration">Example 5: TensorClass Structure Exploration</a></li>
<li><a class="reference internal" href="#example-6-error-handling-and-validation">Example 6: Error Handling and Validation</a></li>
<li><a class="reference internal" href="#example-7-rl-environment-integration">Example 7: RL Environment Integration</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>
  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'main',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../_static/design-tabs.js"></script>

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://shiftlab.github.io/pytorch/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://shiftlab.github.io/pytorch/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/">PyTorch</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/features">Features</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/blog/">Blog</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    mobileMenu.bind();
    mobileTOC.bind();
    pytorchAnchors.bind();

    $(window).on("load", function() {
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
    })

    // Add class to links that have code blocks, since we cannot create links in code blocks
    $("article.pytorch-article a span.pre").each(function(e) {
      $(this).closest("a").addClass("has-code");
    });
  </script>
</body>
</html>