


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchrl.trainers.trainers &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../../../versions.html"><span style="font-size:110%">main (0.10.0+g807e9fe) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-1.html">Get started with TorchRLâ€™s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/export.html">Exporting TorchRL modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/llm_browser.html">TorchRL LLM: Building Tool-Enabled Environments</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>torchrl.trainers.trainers</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
    
    
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchrl.trainers.trainers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the MIT license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">abc</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">itertools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pathlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">defaultdict</span><span class="p">,</span> <span class="n">OrderedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">textwrap</span><span class="w"> </span><span class="kn">import</span> <span class="n">indent</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Literal</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict</span><span class="w"> </span><span class="kn">import</span> <span class="n">NestedKey</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">TensorDict</span><span class="p">,</span> <span class="n">TensorDictBase</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict._tensorcollection</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorCollection</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDictModule</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">expand_right</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">optim</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl._utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_CKPT_BACKEND</span><span class="p">,</span>
    <span class="n">KeyDependentDefaultDict</span><span class="p">,</span>
    <span class="n">logger</span> <span class="k">as</span> <span class="n">torchrl_logger</span><span class="p">,</span>
    <span class="n">rl_warnings</span><span class="p">,</span>
    <span class="n">timeit</span><span class="p">,</span>
    <span class="n">VERBOSE</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseCollector</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">split_trajectories</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.replay_buffers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">PrioritizedSampler</span><span class="p">,</span>
    <span class="n">TensorDictPrioritizedReplayBuffer</span><span class="p">,</span>
    <span class="n">TensorDictReplayBuffer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">DEVICE_TYPING</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.common</span><span class="w"> </span><span class="kn">import</span> <span class="n">EnvBase</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">ExplorationType</span><span class="p">,</span> <span class="n">set_exploration_type</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.objectives.common</span><span class="w"> </span><span class="kn">import</span> <span class="n">LossModule</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.objectives.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">TargetNetUpdater</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.record.loggers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Logger</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

    <span class="n">_has_tqdm</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_has_tqdm</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torchsnapshot</span><span class="w"> </span><span class="kn">import</span> <span class="n">Snapshot</span><span class="p">,</span> <span class="n">StateDict</span>

    <span class="n">_has_ts</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">_has_ts</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">REPLAY_BUFFER_CLASS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;prioritized&quot;</span><span class="p">:</span> <span class="n">TensorDictPrioritizedReplayBuffer</span><span class="p">,</span>
    <span class="s2">&quot;circular&quot;</span><span class="p">:</span> <span class="n">TensorDictReplayBuffer</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Mapping of metric names to logger methods - controls how different metrics are logged</span>
<span class="n">LOGGER_METHODS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;grad_norm&quot;</span><span class="p">:</span> <span class="s2">&quot;log_scalar&quot;</span><span class="p">,</span>
    <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="s2">&quot;log_scalar&quot;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Format strings for different data types in progress bar display</span>
<span class="n">TYPE_DESCR</span> <span class="o">=</span> <span class="p">{</span><span class="nb">float</span><span class="p">:</span> <span class="s2">&quot;4.4f&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">}</span>
<span class="n">REWARD_KEY</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="TrainerHookBase"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.TrainerHookBase.html#torchrl.trainers.TrainerHookBase">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">TrainerHookBase</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An abstract hooking class for torchrl Trainer class.&quot;&quot;&quot;</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

<div class="viewcode-block" id="TrainerHookBase.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.TrainerHookBase.html#torchrl.trainers.TrainerHookBase.register">[docs]</a>    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Registers the hook in the trainer at a default location.</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer (Trainer): the trainer where the hook must be registered.</span>
<span class="sd">            name (str): the name of the hook.</span>

<span class="sd">        .. note::</span>
<span class="sd">          To register the hook at another location than the default, use</span>
<span class="sd">          :meth:`~torchrl.trainers.Trainer.register_op`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div></div>


<div class="viewcode-block" id="Trainer"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.Trainer.html#torchrl.trainers.Trainer">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">Trainer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A generic Trainer class.</span>

<span class="sd">    A trainer is responsible for collecting data and training the model.</span>
<span class="sd">    To keep the class as versatile as possible, Trainer does not construct any</span>
<span class="sd">    of its specific operations: they all must be hooked at specific points in</span>
<span class="sd">    the training loop.</span>

<span class="sd">    To build a Trainer, one needs an iterable data source (a :obj:`collector`), a</span>
<span class="sd">    loss module and an optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        collector (Sequence[TensorDictBase]): An iterable returning batches of</span>
<span class="sd">            data in a TensorDict form of shape [batch x time steps].</span>
<span class="sd">        total_frames (int): Total number of frames to be collected during</span>
<span class="sd">            training.</span>
<span class="sd">        loss_module (LossModule): A module that reads TensorDict batches</span>
<span class="sd">            (possibly sampled from a replay buffer) and return a loss</span>
<span class="sd">            TensorDict where every key points to a different loss component.</span>
<span class="sd">        optimizer (optim.Optimizer): An optimizer that trains the parameters</span>
<span class="sd">            of the model.</span>
<span class="sd">        logger (Logger, optional): a Logger that will handle the logging.</span>
<span class="sd">        optim_steps_per_batch (int, optional): number of optimization steps</span>
<span class="sd">            per collection of data. An trainer works as follows: a main loop</span>
<span class="sd">            collects batches of data (epoch loop), and a sub-loop (training</span>
<span class="sd">            loop) performs model updates in between two collections of data.</span>
<span class="sd">            If `None`, the trainer will use the number of workers as the number of optimization steps.</span>
<span class="sd">        clip_grad_norm (bool, optional): If True, the gradients will be clipped</span>
<span class="sd">            based on the total norm of the model parameters. If False,</span>
<span class="sd">            all the partial derivatives will be clamped to</span>
<span class="sd">            (-clip_norm, clip_norm). Default is ``True``.</span>
<span class="sd">        clip_norm (Number, optional): value to be used for clipping gradients.</span>
<span class="sd">            Default is None (no clip norm).</span>
<span class="sd">        progress_bar (bool, optional): If True, a progress bar will be</span>
<span class="sd">            displayed using tqdm. If tqdm is not installed, this option</span>
<span class="sd">            won&#39;t have any effect. Default is ``True``</span>
<span class="sd">        seed (int, optional): Seed to be used for the collector, pytorch and</span>
<span class="sd">            numpy. Default is ``None``.</span>
<span class="sd">        save_trainer_interval (int, optional): How often the trainer should be</span>
<span class="sd">            saved to disk, in frame count. Default is 10000.</span>
<span class="sd">        log_interval (int, optional): How often the values should be logged,</span>
<span class="sd">            in frame count. Default is 10000.</span>
<span class="sd">        save_trainer_file (path, optional): path where to save the trainer.</span>
<span class="sd">            Default is None (no saving)</span>
<span class="sd">        async_collection (bool, optional): Whether to collect data asynchronously.</span>
<span class="sd">            This will only work if the replay buffer is registed within the data collector.</span>
<span class="sd">            If using this, the UTD ratio (Update to Data) will be logged under the key &quot;utd_ratio&quot;.</span>
<span class="sd">            Default is False.</span>
<span class="sd">        log_timings (bool, optional): If True, automatically register a LogTiming hook to log</span>
<span class="sd">            timing information for all hooks to the logger (e.g., wandb, tensorboard).</span>
<span class="sd">            Timing metrics will be logged with prefix &quot;time/&quot; (e.g., &quot;time/hook/UpdateWeights&quot;).</span>
<span class="sd">            Default is False.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Training state trackers (used for logging and checkpointing)</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_optim_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Total number of optimization steps completed</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_collected_frames</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Total number of frames collected (deprecated)</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_last_log</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span>
            <span class="nb">str</span><span class="p">,</span> <span class="n">Any</span>
        <span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># Tracks when each metric was last logged (for log_interval control)</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_last_save</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">0</span>  <span class="c1"># Tracks when trainer was last saved (for save_interval control)</span>
        <span class="p">)</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Total number of frames collected (current)</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_app_state</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Application state for checkpointing</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">collector</span><span class="p">:</span> <span class="n">BaseCollector</span><span class="p">,</span>
        <span class="n">total_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">frame_skip</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">optim_steps_per_batch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">loss_module</span><span class="p">:</span> <span class="n">LossModule</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">TensorDictBase</span><span class="p">],</span> <span class="n">TensorDictBase</span><span class="p">],</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">logger</span><span class="p">:</span> <span class="n">Logger</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clip_grad_norm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">clip_norm</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">progress_bar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_trainer_interval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">log_interval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span>
        <span class="n">save_trainer_file</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">async_collection</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">log_timings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>

        <span class="c1"># objects</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span> <span class="o">=</span> <span class="n">frame_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span> <span class="o">=</span> <span class="n">collector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span> <span class="o">=</span> <span class="n">loss_module</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logger</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">async_collection</span> <span class="o">=</span> <span class="n">async_collection</span>

        <span class="c1"># Logging frequency control - how often to log each metric (in frames)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_log_interval</span> <span class="o">=</span> <span class="n">log_interval</span>

        <span class="c1"># seeding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_seed</span><span class="p">()</span>

        <span class="c1"># constants</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optim_steps_per_batch</span> <span class="o">=</span> <span class="n">optim_steps_per_batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_frames</span> <span class="o">=</span> <span class="n">total_frames</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span> <span class="o">=</span> <span class="n">num_epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_norm</span> <span class="o">=</span> <span class="n">clip_grad_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_norm</span> <span class="o">=</span> <span class="n">clip_norm</span>
        <span class="k">if</span> <span class="n">progress_bar</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_has_tqdm</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;tqdm library not found. &quot;</span>
                <span class="s2">&quot;Consider installing tqdm to use the Trainer progress bar.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span> <span class="o">=</span> <span class="n">progress_bar</span> <span class="ow">and</span> <span class="n">_has_tqdm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_interval</span> <span class="o">=</span> <span class="n">save_trainer_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_file</span> <span class="o">=</span> <span class="n">save_trainer_file</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_log_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

        <span class="c1"># Hook collections for different stages of the training loop</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_process_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># Process collected batches (e.g., reward normalization)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_ops</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># After optimization steps (e.g., weight updates)</span>

        <span class="c1"># Logging hook collections - different points in training loop where logging can occur</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_log_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># After optimization steps (e.g., validation rewards)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_steps_log_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># Before optimization steps (e.g., rewards, frame counts)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_log_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># After each optimization step (e.g., gradient norms)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_epoch_log_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># Before each epoch logging (e.g., epoch-specific metrics)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_epoch_log_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># After each epoch logging (e.g., epoch completion metrics)</span>

        <span class="c1"># Regular hook collections for non-logging operations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_epoch_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># Before each epoch (e.g., epoch setup, cache clearing)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_epoch_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># After each epoch (e.g., epoch cleanup, weight syncing)</span>

        <span class="c1"># Optimization-related hook collections</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_optim_ops</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Before optimization steps (e.g., cache clearing)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_loss_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># After loss computation, operates on batch (e.g., priority updates)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_process_loss_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># Transform loss values before optimizer (e.g., scaling, clipping)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_ops</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># During optimization (e.g., gradient clipping)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_process_optim_batch_ops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[]</span>
        <span class="p">)</span>  <span class="c1"># Process batches for optimization (e.g., subsampling)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_ops</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># After optimization (e.g., weight syncing)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optimizer_hook</span> <span class="o">=</span> <span class="n">OptimizerHook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="n">optimizer_hook</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">log_timings</span><span class="p">:</span>
            <span class="n">log_timing_hook</span> <span class="o">=</span> <span class="n">LogTiming</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;time&quot;</span><span class="p">,</span> <span class="n">percall</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">erase</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">log_timing_hook</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">module_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">module_name</span><span class="si">}</span><span class="s2"> is already registered, choose a different name.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">module_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">module</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_wrap_hook_with_timing</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">hook_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Wrap a hook with timing measurement.</span>

<span class="sd">        Args:</span>
<span class="sd">            op: The hook/operation to wrap</span>
<span class="sd">            hook_name: Optional name for the hook. If not provided, will be inferred from op.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A wrapped version of the hook that measures execution time.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">hook_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">hook_name</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span>
                <span class="s2">&quot;__name__&quot;</span><span class="p">,</span>
                <span class="n">op</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="s2">&quot;__class__&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="s2">&quot;unknown_hook&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">timed_hook</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">timeit</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;hook/</span><span class="si">{</span><span class="n">hook_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">op</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Preserve original attributes for debugging</span>
        <span class="n">timed_hook</span><span class="o">.</span><span class="n">__wrapped__</span> <span class="o">=</span> <span class="n">op</span>
        <span class="n">timed_hook</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="n">hook_name</span>
        <span class="k">return</span> <span class="n">timed_hook</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">_CKPT_BACKEND</span> <span class="o">==</span> <span class="s2">&quot;torchsnapshot&quot;</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">StateDict</span><span class="p">(</span>
                <span class="n">collected_frames</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span><span class="p">,</span>
                <span class="n">_last_log</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_last_log</span><span class="p">,</span>
                <span class="n">_last_save</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_last_save</span><span class="p">,</span>
                <span class="n">_optim_count</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_optim_count</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span>
                <span class="n">collected_frames</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span><span class="p">,</span>
                <span class="n">_last_log</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_last_log</span><span class="p">,</span>
                <span class="n">_last_save</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_last_save</span><span class="p">,</span>
                <span class="n">_optim_count</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_optim_count</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">app_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;state&quot;</span><span class="p">:</span> <span class="n">StateDict</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_state</span><span class="p">()),</span>
            <span class="s2">&quot;collector&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="p">,</span>
            <span class="s2">&quot;loss_module&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span><span class="p">,</span>
            <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">item</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_app_state</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_state</span><span class="p">()</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span>
            <span class="n">collector</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="n">loss_module</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
            <span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">state_dict</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">model_state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;loss_module&quot;</span><span class="p">]</span>
        <span class="n">collector_state_dict</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;collector&quot;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">model_state_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">collector_state_dict</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">item</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">][</span><span class="s2">&quot;collected_frames&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_log</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">][</span><span class="s2">&quot;_last_log&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_save</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">][</span><span class="s2">&quot;_last_save&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optim_count</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">][</span><span class="s2">&quot;_optim_count&quot;</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_save_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">_CKPT_BACKEND</span> <span class="o">==</span> <span class="s2">&quot;torchsnapshot&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_ts</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                    <span class="s2">&quot;torchsnapshot not found. Consider installing torchsnapshot or &quot;</span>
                    <span class="s2">&quot;using the torch checkpointing backend (`CKPT_BACKEND=torch`)&quot;</span>
                <span class="p">)</span>
            <span class="n">Snapshot</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">app_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">app_state</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_file</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_CKPT_BACKEND</span> <span class="o">==</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_file</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;CKPT_BACKEND should be one of </span><span class="si">{</span><span class="n">_CKPT_BACKEND</span><span class="o">.</span><span class="n">backends</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">_CKPT_BACKEND</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">save_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">force_save</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_save</span> <span class="o">=</span> <span class="n">force_save</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_save</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_interval</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_last_save</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span>
                <span class="n">_save</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">_save</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer_file</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_trainer</span><span class="p">()</span>

<div class="viewcode-block" id="Trainer.load_from_file"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.Trainer.html#torchrl.trainers.Trainer.load_from_file">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">load_from_file</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Trainer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads a file and its state-dict in the trainer.</span>

<span class="sd">        Keyword arguments are passed to the :func:`~torch.load` function.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">_CKPT_BACKEND</span> <span class="o">==</span> <span class="s2">&quot;torchsnapshot&quot;</span><span class="p">:</span>
            <span class="n">snapshot</span> <span class="o">=</span> <span class="n">Snapshot</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">file</span><span class="p">)</span>
            <span class="n">snapshot</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">app_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">app_state</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_CKPT_BACKEND</span> <span class="o">==</span> <span class="s2">&quot;torch&quot;</span><span class="p">:</span>
            <span class="n">loaded_dict</span><span class="p">:</span> <span class="n">OrderedDict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">loaded_dict</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">static_seed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">collector</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">BaseCollector</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collector</span>

    <span class="nd">@collector</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">collector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">collector</span><span class="p">:</span> <span class="n">BaseCollector</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collector</span> <span class="o">=</span> <span class="n">collector</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register_op</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dest</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span>
            <span class="s2">&quot;batch_process&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pre_optim_steps&quot;</span><span class="p">,</span>
            <span class="s2">&quot;process_optim_batch&quot;</span><span class="p">,</span>
            <span class="s2">&quot;post_loss&quot;</span><span class="p">,</span>
            <span class="s2">&quot;process_loss&quot;</span><span class="p">,</span>
            <span class="s2">&quot;optimizer&quot;</span><span class="p">,</span>
            <span class="s2">&quot;post_steps&quot;</span><span class="p">,</span>
            <span class="s2">&quot;post_optim&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pre_steps_log&quot;</span><span class="p">,</span>
            <span class="s2">&quot;post_steps_log&quot;</span><span class="p">,</span>
            <span class="s2">&quot;post_optim_log&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pre_epoch_log&quot;</span><span class="p">,</span>
            <span class="s2">&quot;post_epoch_log&quot;</span><span class="p">,</span>
            <span class="s2">&quot;pre_epoch&quot;</span><span class="p">,</span>
            <span class="s2">&quot;post_epoch&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">op</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Wrap hook with timing for performance monitoring</span>
        <span class="c1"># Get hook name from registered modules if available</span>
        <span class="n">hook_name</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">module</span> <span class="ow">is</span> <span class="n">op</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">callable</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="fm">__call__</span> <span class="ow">is</span> <span class="n">op</span><span class="p">):</span>
                <span class="n">hook_name</span> <span class="o">=</span> <span class="n">name</span>
                <span class="k">break</span>

        <span class="n">timed_op</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_wrap_hook_with_timing</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">hook_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;batch_process&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">TensorDictBase</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_batch_process_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;pre_optim_steps&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_optim_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;process_optim_batch&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">TensorDictBase</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_process_optim_batch_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_loss&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">TensorDictBase</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_loss_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;process_loss&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="n">TensorDictBase</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_process_loss_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">output</span><span class="o">=</span><span class="n">TensorDictBase</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_steps&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_optim&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;pre_steps_log&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_steps_log_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_steps_log&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_log_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_optim_log&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_log_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;pre_epoch_log&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_epoch_log_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_epoch_log&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span>
                <span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_epoch_log_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;pre_epoch&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_epoch_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">elif</span> <span class="n">dest</span> <span class="o">==</span> <span class="s2">&quot;post_epoch&quot;</span><span class="p">:</span>
            <span class="n">_check_input_output_typehint</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="nb">input</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_epoch_ops</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">timed_op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The hook collection </span><span class="si">{</span><span class="n">dest</span><span class="si">}</span><span class="s2"> is not recognised. Choose from:&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;(batch_process, pre_optim_steps, process_optim_batch, post_loss, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;process_loss, optimizer, post_steps, post_optim, pre_steps_log, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;post_steps_log, post_optim_log, pre_epoch_log, post_epoch_log, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;pre_epoch, post_epoch)&quot;</span>
            <span class="p">)</span>

    <span class="n">register_hook</span> <span class="o">=</span> <span class="n">register_op</span>

    <span class="c1"># Process batch</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_process_batch_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_process_ops</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_post_steps_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_ops</span><span class="p">:</span>
            <span class="n">op</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_post_optim_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute logging hooks that run AFTER EACH optimization step.</span>

<span class="sd">        These hooks log metrics that are computed after each individual optimization step,</span>
<span class="sd">        such as gradient norms, individual loss components, or step-specific metrics.</span>
<span class="sd">        Called after each optimization step within the optimization loop.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_log_ops</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">(</span><span class="o">**</span><span class="n">result</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_pre_optim_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_optim_ops</span><span class="p">:</span>
            <span class="n">op</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_process_optim_batch_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_optim_batch_ops</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_post_loss_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_loss_ops</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_process_loss_hook</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">sub_batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">losses_td</span><span class="p">:</span> <span class="n">TensorDictBase</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply registered loss transformation hooks before optimization.</span>

<span class="sd">        Unlike ``post_loss`` hooks which operate on the batch (e.g., for priority updates),</span>
<span class="sd">        ``process_loss`` hooks transform the loss TensorDict itself. These hooks receive</span>
<span class="sd">        both the sub_batch and the losses, and should return the modified losses.</span>

<span class="sd">        Use cases include loss scaling, clipping, or applying importance weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            sub_batch: The batch of data used to compute the losses.</span>
<span class="sd">            losses_td: The TensorDict containing loss components from the loss module.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The (possibly modified) losses TensorDict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_loss_ops</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">,</span> <span class="n">losses_td</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
                <span class="n">losses_td</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">losses_td</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_optimizer_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_ops</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_grad_norm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_norm</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">batch</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_post_optim_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_ops</span><span class="p">:</span>
            <span class="n">op</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_pre_epoch_log_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute logging hooks that run BEFORE each epoch of optimization.</span>

<span class="sd">        These hooks log metrics that should be computed before starting a new epoch</span>
<span class="sd">        of optimization steps. Called once per epoch within the optimization loop.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_epoch_log_ops</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">(</span><span class="o">**</span><span class="n">result</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_pre_epoch_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute regular hooks that run BEFORE each epoch of optimization.</span>

<span class="sd">        These hooks perform non-logging operations before starting a new epoch</span>
<span class="sd">        of optimization steps. Called once per epoch within the optimization loop.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_epoch_ops</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_post_epoch_log_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute logging hooks that run AFTER each epoch of optimization.</span>

<span class="sd">        These hooks log metrics that should be computed after completing an epoch</span>
<span class="sd">        of optimization steps. Called once per epoch within the optimization loop.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_epoch_log_ops</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">(</span><span class="o">**</span><span class="n">result</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_post_epoch_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute regular hooks that run AFTER each epoch of optimization.</span>

<span class="sd">        These hooks perform non-logging operations after completing an epoch</span>
<span class="sd">        of optimization steps. Called once per epoch within the optimization loop.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_epoch_ops</span><span class="p">:</span>
            <span class="n">op</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_pre_steps_log_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute logging hooks that run BEFORE optimization steps.</span>

<span class="sd">        These hooks typically log metrics from the collected batch data,</span>
<span class="sd">        such as rewards, frame counts, or other batch-level statistics.</span>
<span class="sd">        Called once per batch collection, before any optimization occurs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_steps_log_ops</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">(</span><span class="o">**</span><span class="n">result</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_post_steps_log_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Execute logging hooks that run AFTER optimization steps.</span>

<span class="sd">        These hooks typically log metrics that depend on the optimization results,</span>
<span class="sd">        such as validation rewards, evaluation metrics, or post-training statistics.</span>
<span class="sd">        Called once per batch collection, after all optimization steps are complete.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">op</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_log_ops</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">result</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">(</span><span class="o">**</span><span class="n">result</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_frames</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pbar_str</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_collection</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
            <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">getattr_rb</span><span class="p">(</span><span class="s2">&quot;write_count&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

            <span class="c1"># Create async iterator that monitors write_count progress</span>
            <span class="n">iterator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_async_iterator</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">iterator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span>

        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">iterator</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_collection</span><span class="p">:</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_batch_hook</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">current_frames</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">numel</span><span class="p">()))</span>
                    <span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                    <span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">+=</span> <span class="n">current_frames</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># In async mode, batch is None and we track frames via write_count</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">cf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">getattr_rb</span><span class="p">(</span><span class="s2">&quot;write_count&quot;</span><span class="p">)</span>
                <span class="n">current_frames</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">-</span> <span class="n">cf</span>

            <span class="c1"># LOGGING POINT 1: Pre-optimization logging (e.g., rewards, frame counts)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_steps_log_hook</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">init_random_frames</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optim_steps</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_hook</span><span class="p">()</span>

            <span class="c1"># LOGGING POINT 2: Post-optimization logging (e.g., validation rewards, evaluation metrics)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_steps_log_hook</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_pbar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">current_frames</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_pbar_description</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_frames</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer</span><span class="p">(</span><span class="n">force_save</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_trainer</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_async_iterator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create an iterator for async collection that monitors replay buffer write_count.</span>

<span class="sd">        This iterator yields None batches and terminates when total_frames is reached</span>
<span class="sd">        based on the replay buffer&#39;s write_count rather than using a fixed range.</span>
<span class="sd">        This ensures the training loop properly consumes the entire collector output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">current_write_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">getattr_rb</span><span class="p">(</span><span class="s2">&quot;write_count&quot;</span><span class="p">)</span>
            <span class="c1"># Check if we&#39;ve reached the target frames</span>
            <span class="k">if</span> <span class="n">current_write_count</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_frames</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">shutdown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">VERBOSE</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;shutting down collector&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">optim_steps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">average_losses</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pre_optim_hook</span><span class="p">()</span>
        <span class="n">optim_steps_per_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim_steps_per_batch</span>
        <span class="n">j</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_epochs</span><span class="p">):</span>
            <span class="c1"># LOGGING POINT 3: Pre-epoch logging (e.g., epoch-specific metrics)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pre_epoch_log_hook</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="c1"># Regular pre-epoch operations (e.g., epoch setup)</span>
            <span class="n">batch_processed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pre_epoch_hook</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">optim_steps_per_batch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">prog</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">prog</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">optim_steps_per_batch</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">prog</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_optim_count</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">sub_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_optim_batch_hook</span><span class="p">(</span><span class="n">batch_processed</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="k">if</span> <span class="n">sub_batch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="n">losses_td</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_post_loss_hook</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span>

                <span class="n">losses_td</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_loss_hook</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">,</span> <span class="n">losses_td</span><span class="p">)</span>

                <span class="n">losses_detached</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_hook</span><span class="p">(</span><span class="n">losses_td</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_hook</span><span class="p">()</span>

                <span class="c1"># LOGGING POINT 4: Post-optimization step logging (e.g., gradient norms, step-specific metrics)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_post_optim_log</span><span class="p">(</span><span class="n">sub_batch</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">average_losses</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">average_losses</span><span class="p">:</span> <span class="n">TensorDictBase</span> <span class="o">=</span> <span class="n">losses_detached</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">losses_detached</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="n">val</span> <span class="o">=</span> <span class="n">average_losses</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
                        <span class="n">average_losses</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="o">*</span> <span class="n">j</span> <span class="o">/</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">item</span> <span class="o">/</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
                <span class="k">del</span> <span class="n">sub_batch</span><span class="p">,</span> <span class="n">losses_td</span><span class="p">,</span> <span class="n">losses_detached</span>

            <span class="c1"># LOGGING POINT 5: Post-epoch logging (e.g., epoch completion metrics)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_epoch_log_hook</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="c1"># Regular post-epoch operations (e.g., epoch cleanup)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_post_epoch_hook</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">j</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Log optimization statistics and average losses after completing all optimization steps</span>
            <span class="c1"># This is the main logging point for training metrics like loss values and optimization step count</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log</span><span class="p">(</span>
                <span class="n">optim_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_optim_count</span><span class="p">,</span>
                <span class="o">**</span><span class="n">average_losses</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_pbar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Main logging method that handles both logger output and progress bar updates.</span>

<span class="sd">        This method is called from various hooks throughout the training loop to log metrics.</span>
<span class="sd">        It maintains a history of logged values and controls logging frequency based on log_interval.</span>

<span class="sd">        Args:</span>
<span class="sd">            log_pbar: If True, the value will also be displayed in the progress bar</span>
<span class="sd">            **kwargs: Key-value pairs to log, where key is the metric name and value is the metric value</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">collected_frames</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Store all values in history regardless of logging frequency</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

            <span class="c1"># Check if enough frames have passed since last logging for this key</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">collected_frames</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_log</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_interval</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_last_log</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">collected_frames</span>
                <span class="n">_log</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_log</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="c1"># Determine logging method (defaults to &quot;log_scalar&quot;)</span>
            <span class="n">method</span> <span class="o">=</span> <span class="n">LOGGER_METHODS</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="s2">&quot;log_scalar&quot;</span><span class="p">)</span>

            <span class="c1"># Log to external logger (e.g., tensorboard, wandb) if conditions are met</span>
            <span class="k">if</span> <span class="n">_log</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="p">,</span> <span class="n">method</span><span class="p">)(</span><span class="n">key</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">collected_frames</span><span class="p">)</span>

            <span class="c1"># Update progress bar if requested and method is scalar</span>
            <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;log_scalar&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span> <span class="ow">and</span> <span class="n">log_pbar</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                    <span class="n">item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_pbar_str</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_pbar_description</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update the progress bar description with current metric values.</span>

<span class="sd">        This method formats and displays the current values of metrics that have</span>
<span class="sd">        been marked for progress bar display (log_pbar=True) in the logging hooks.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">progress_bar</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_pbar</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span>
                <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_pbar_str</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="w"> </span><span class="si">:{</span><span class="n">TYPE_DESCR</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pbar_str</span><span class="p">[</span><span class="n">key</span><span class="p">]),</span><span class="w"> </span><span class="s1">&#39;4.4f&#39;</span><span class="p">)</span><span class="si">}}</span><span class="s2">&quot;</span>
                        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_pbar_str</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">loss_str</span> <span class="o">=</span> <span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;loss=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_module</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="n">collector_str</span> <span class="o">=</span> <span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;collector=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="n">optimizer_str</span> <span class="o">=</span> <span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;optimizer=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="n">logger</span> <span class="o">=</span> <span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;logger=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>

        <span class="n">string</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">loss_str</span><span class="p">,</span>
                <span class="n">collector_str</span><span class="p">,</span>
                <span class="n">optimizer_str</span><span class="p">,</span>
                <span class="n">logger</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">string</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Trainer(</span><span class="se">\n</span><span class="si">{</span><span class="n">string</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="k">return</span> <span class="n">string</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_list_state_dict</span><span class="p">(</span><span class="n">hook_list</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">item</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="n">hook_list</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s2">&quot;state_dict&quot;</span><span class="p">):</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">item</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">kwargs</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_load_list_state_dict</span><span class="p">(</span><span class="n">list_state_dict</span><span class="p">,</span> <span class="n">hook_list</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">((</span><span class="n">state_dict_item</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">),</span> <span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">_</span><span class="p">))</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span><span class="n">list_state_dict</span><span class="p">,</span> <span class="n">hook_list</span><span class="p">)</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">state_dict_item</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">item</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict_item</span><span class="p">)</span>
            <span class="n">hook_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>


<div class="viewcode-block" id="SelectKeys"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.SelectKeys.html#torchrl.trainers.SelectKeys">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">SelectKeys</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Selects keys in a TensorDict batch.</span>

<span class="sd">    Args:</span>
<span class="sd">        keys (iterable of strings): keys to be selected in the tensordict.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; trainer = make_trainer()</span>
<span class="sd">        &gt;&gt;&gt; key1 = &quot;first key&quot;</span>
<span class="sd">        &gt;&gt;&gt; key2 = &quot;second key&quot;</span>
<span class="sd">        &gt;&gt;&gt; td = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         key1: torch.randn(3),</span>
<span class="sd">        ...         key2: torch.randn(3),</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     [],</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;batch_process&quot;, SelectKeys([key1]))</span>
<span class="sd">        &gt;&gt;&gt; td_out = trainer._process_batch_hook(td)</span>
<span class="sd">        &gt;&gt;&gt; assert key1 in td_out.keys()</span>
<span class="sd">        &gt;&gt;&gt; assert key2 not in td_out.keys()</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keys</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Expected keys to be an iterable of str, got str instead&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">batch</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

<div class="viewcode-block" id="SelectKeys.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.SelectKeys.html#torchrl.trainers.SelectKeys.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;select_keys&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;batch_process&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ReplayBufferTrainer"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.ReplayBufferTrainer.html#torchrl.trainers.ReplayBufferTrainer">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">ReplayBufferTrainer</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Replay buffer hook provider.</span>

<span class="sd">    Args:</span>
<span class="sd">        replay_buffer (TensorDictReplayBuffer): replay buffer to be used.</span>
<span class="sd">        batch_size (int, optional): batch size when sampling data from the</span>
<span class="sd">            latest collection or from the replay buffer. If none is provided,</span>
<span class="sd">            the replay buffer batch-size will be used (preferred option for</span>
<span class="sd">            unchanged batch-sizes).</span>
<span class="sd">        memmap (bool, optional): if ``True``, a memmap tensordict is created.</span>
<span class="sd">            Default is ``False``.</span>
<span class="sd">        device (device, optional): device where the samples must be placed.</span>
<span class="sd">            Default to ``None``.</span>
<span class="sd">        flatten_tensordicts (bool, optional): if ``True``, the tensordicts will be</span>
<span class="sd">            flattened (or equivalently masked with the valid mask obtained from</span>
<span class="sd">            the collector) before being passed to the replay buffer. Otherwise,</span>
<span class="sd">            no transform will be achieved other than padding (see :obj:`max_dims` arg below).</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        max_dims (sequence of int, optional): if :obj:`flatten_tensordicts` is set to False,</span>
<span class="sd">            this will be a list of the length of the batch_size of the provided</span>
<span class="sd">            tensordicts that represent the maximum size of each. If provided,</span>
<span class="sd">            this list of sizes will be used to pad the tensordict and make their shape</span>
<span class="sd">            match before they are passed to the replay buffer. If there is no</span>
<span class="sd">            maximum value, a -1 value should be provided.</span>
<span class="sd">        iterate (bool, optional): if ``True``, the replay buffer will be iterated over</span>
<span class="sd">            in a loop. Defaults to ``False`` (call to :meth:`~torchrl.data.ReplayBuffer.sample` will be used).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; rb_trainer = ReplayBufferTrainer(replay_buffer=replay_buffer, batch_size=N)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;batch_process&quot;, rb_trainer.extend)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;process_optim_batch&quot;, rb_trainer.sample)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;post_loss&quot;, rb_trainer.update_priority)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">replay_buffer</span><span class="p">:</span> <span class="n">TensorDictReplayBuffer</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">memmap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">DEVICE_TYPING</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">flatten_tensordicts</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_dims</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">iterate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">replay_buffer</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">replay_buffer</span><span class="p">,</span> <span class="s2">&quot;update_tensordict_priority&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_priority</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">update_tensordict_priority</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sampler</span><span class="p">,</span> <span class="n">PrioritizedSampler</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Prioritized sampler not supported for replay buffer trainer if not within a TensorDictReplayBuffer&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_priority</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memmap</span> <span class="o">=</span> <span class="n">memmap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten_tensordicts</span> <span class="o">=</span> <span class="n">flatten_tensordicts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_dims</span> <span class="o">=</span> <span class="n">max_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iterate</span> <span class="o">=</span> <span class="n">iterate</span>
        <span class="k">if</span> <span class="n">iterate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten_tensordicts</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">))]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">&quot;truncated&quot;</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;next&quot;</span><span class="p">]:</span>
                    <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;truncated&quot;</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">pads</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">ndimension</span><span class="p">()):</span>
                    <span class="n">pad_value</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="mi">0</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_dims</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span>
                        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_dims</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">batch</span><span class="o">.</span><span class="n">batch_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
                    <span class="p">)</span>
                    <span class="n">pads</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">pad_value</span><span class="p">]</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">pads</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">iterate</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">sample</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer_iter</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
                <span class="c1"># reset the replay buffer</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer_iter</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="p">)</span>
                <span class="k">raise</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sample</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">sample</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_priority</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_update_priority</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;replay_buffer&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;replay_buffer&quot;</span><span class="p">])</span>

<div class="viewcode-block" id="ReplayBufferTrainer.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.ReplayBufferTrainer.html#torchrl.trainers.ReplayBufferTrainer.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;replay_buffer&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;batch_process&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">extend</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;process_optim_batch&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;post_loss&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_priority</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="OptimizerHook"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.OptimizerHook.html#torchrl.trainers.OptimizerHook">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">OptimizerHook</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add an optimizer for one or more loss components.</span>

<span class="sd">    Args:</span>
<span class="sd">        optimizer (optim.Optimizer): An optimizer to apply to the loss_components.</span>
<span class="sd">        loss_components (Sequence[str], optional): The keys in the loss TensorDict</span>
<span class="sd">            for which the optimizer should be appled to the respective values.</span>
<span class="sd">            If omitted, the optimizer is applied to all components with the</span>
<span class="sd">            names starting with `loss_`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; optimizer_hook = OptimizerHook(optimizer, [&quot;loss_actor&quot;])</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;optimizer&quot;, optimizer_hook)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">optim</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">,</span>
        <span class="n">loss_components</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">loss_components</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">loss_components</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;loss_components list cannot be empty. &quot;</span>
                <span class="s2">&quot;Set to None to act on all components of the loss.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span> <span class="o">=</span> <span class="n">loss_components</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_grad_clip</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">clip_grad_norm</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">params</span> <span class="o">+=</span> <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">clip_grad_norm</span> <span class="ow">and</span> <span class="n">clip_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">gn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">gn</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">clip_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_value_</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">gn</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">losses_td</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span>
        <span class="n">clip_grad_norm</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">clip_norm</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="n">loss_components</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">losses_td</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span><span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_components</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">losses_td</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss_components</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="n">grad_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad_clip</span><span class="p">(</span><span class="n">clip_grad_norm</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">)</span>
        <span class="n">losses_td</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;grad_norm_</span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">grad_norm</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">losses_td</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

<div class="viewcode-block" id="OptimizerHook.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.OptimizerHook.html#torchrl.trainers.OptimizerHook.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;optimizer&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;optimizer&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ClearCudaCache"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.ClearCudaCache.html#torchrl.trainers.ClearCudaCache">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">ClearCudaCache</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Clears cuda cache at a given interval.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; clear_cuda = ClearCudaCache(100)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;pre_optim_steps&quot;, clear_cuda)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interval</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">interval</span> <span class="o">=</span> <span class="n">interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">LogTiming</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook to log timing information collected by timeit context managers.</span>

<span class="sd">    This hook extracts timing data from the global timeit registry and logs it</span>
<span class="sd">    to the trainer&#39;s logger (e.g., wandb, tensorboard). It&#39;s useful for profiling</span>
<span class="sd">    different parts of the training loop.</span>

<span class="sd">    Args:</span>
<span class="sd">        prefix (str, optional): Prefix to add to timing metric names.</span>
<span class="sd">            Default is &quot;time&quot;.</span>
<span class="sd">        percall (bool, optional): If True, log average time per call.</span>
<span class="sd">            If False, log total time. Default is True.</span>
<span class="sd">        erase (bool, optional): If True, reset timing data after each log.</span>
<span class="sd">            Default is False.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Log timing data after each optimization step</span>
<span class="sd">        &gt;&gt;&gt; log_timing = LogTiming(prefix=&quot;time&quot;, percall=True)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;post_optim_log&quot;, log_timing)</span>

<span class="sd">        &gt;&gt;&gt; # Log timing data after each batch collection</span>
<span class="sd">        &gt;&gt;&gt; log_timing = LogTiming(prefix=&quot;time&quot;, erase=True)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;post_steps_log&quot;, log_timing)</span>

<span class="sd">    Note:</span>
<span class="sd">        This hook works with timing data collected using the `timeit` context manager.</span>
<span class="sd">        For example, hooks registered with `register_op` are automatically wrapped</span>
<span class="sd">        with timing measurement.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;time&quot;</span><span class="p">,</span>
        <span class="n">percall</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">erase</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">=</span> <span class="n">prefix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">percall</span> <span class="o">=</span> <span class="n">percall</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">erase</span> <span class="o">=</span> <span class="n">erase</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract timing data and return as a dict for logging.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch: The batch (unused, but required by hook signature)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dictionary of timing metrics with the format {metric_name: value}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">timing_dict</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">todict</span><span class="p">(</span><span class="n">percall</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">percall</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prefix</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">erase</span><span class="p">:</span>
            <span class="n">timeit</span><span class="o">.</span><span class="n">erase</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">timing_dict</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return state dict for checkpointing.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;prefix&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span><span class="p">,</span>
            <span class="s2">&quot;percall&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">percall</span><span class="p">,</span>
            <span class="s2">&quot;erase&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">erase</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load state dict from checkpoint.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;prefix&quot;</span><span class="p">,</span> <span class="s2">&quot;time&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">percall</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;percall&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">erase</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;erase&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;log_timing&quot;</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;post_steps_log&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>


<div class="viewcode-block" id="LogScalar"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.LogScalar.html#torchrl.trainers.LogScalar">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">LogScalar</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generic scalar logger hook for any tensor values in the batch.</span>

<span class="sd">    This hook can log any scalar values from the collected batch data, including</span>
<span class="sd">    rewards, action norms, done states, and any other metrics. It automatically</span>
<span class="sd">    handles masking and computes both mean and standard deviation.</span>

<span class="sd">    Args:</span>
<span class="sd">        key (NestedKey): the key where to find the value in the input batch.</span>
<span class="sd">            Can be a string for simple keys or a tuple for nested keys.</span>
<span class="sd">            Default is `torchrl.trainers.trainers.REWARD_KEY` (= `(&quot;next&quot;, &quot;reward&quot;)`).</span>
<span class="sd">        logname (str, optional): name of the metric to be logged. If None, will use</span>
<span class="sd">            the key as the log name. Default is None.</span>
<span class="sd">        log_pbar (bool, optional): if ``True``, the value will be logged on</span>
<span class="sd">            the progression bar. Default is ``False``.</span>
<span class="sd">        include_std (bool, optional): if ``True``, also log the standard deviation</span>
<span class="sd">            of the values. Default is ``True``.</span>
<span class="sd">        reduction (str, optional): reduction method to apply. Can be &quot;mean&quot;, &quot;sum&quot;,</span>
<span class="sd">            &quot;min&quot;, &quot;max&quot;. Default is &quot;mean&quot;.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Log training rewards</span>
<span class="sd">        &gt;&gt;&gt; log_reward = LogScalar((&quot;next&quot;, &quot;reward&quot;), &quot;r_training&quot;, log_pbar=True)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;pre_steps_log&quot;, log_reward)</span>

<span class="sd">        &gt;&gt;&gt; # Log action norms</span>
<span class="sd">        &gt;&gt;&gt; log_action_norm = LogScalar(&quot;action&quot;, &quot;action_norm&quot;, include_std=True)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;pre_steps_log&quot;, log_action_norm)</span>

<span class="sd">        &gt;&gt;&gt; # Log done states (as percentage)</span>
<span class="sd">        &gt;&gt;&gt; log_done = LogScalar((&quot;next&quot;, &quot;done&quot;), &quot;done_percentage&quot;, reduction=&quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;pre_steps_log&quot;, log_done)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">=</span> <span class="n">REWARD_KEY</span><span class="p">,</span>
        <span class="n">logname</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">log_pbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">include_std</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logname</span> <span class="o">=</span> <span class="n">logname</span> <span class="k">if</span> <span class="n">logname</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span> <span class="o">=</span> <span class="n">log_pbar</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">include_std</span> <span class="o">=</span> <span class="n">include_std</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

        <span class="c1"># Validate reduction method</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;reduction must be one of [&#39;mean&#39;, &#39;sum&#39;, &#39;min&#39;, &#39;max&#39;], got </span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_reduction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply the specified reduction to the tensor.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;min&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown reduction: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># Get the tensor from the batch</span>
        <span class="n">tensor</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key</span><span class="p">)</span>

        <span class="c1"># Apply mask if available</span>
        <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">))</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

        <span class="c1"># Compute the main statistic</span>
        <span class="n">main_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_reduction</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Prepare the result dictionary</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">{</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logname</span><span class="p">:</span> <span class="n">main_value</span><span class="p">,</span>
            <span class="s2">&quot;log_pbar&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="c1"># Add standard deviation if requested</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">include_std</span> <span class="ow">and</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">std_value</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">result</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">logname</span><span class="si">}</span><span class="s2">_std&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">std_value</span>

        <span class="k">return</span> <span class="n">result</span>

<div class="viewcode-block" id="LogScalar.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.LogScalar.html#torchrl.trainers.LogScalar.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;log_</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">logname</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;pre_steps_log&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div></div>


<span class="k">class</span><span class="w"> </span><span class="nc">LogReward</span><span class="p">(</span><span class="n">LogScalar</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Deprecated class. Use LogScalar instead.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">logname</span><span class="o">=</span><span class="s2">&quot;r_training&quot;</span><span class="p">,</span>
        <span class="n">log_pbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">reward_key</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;The &#39;LogReward&#39; class is deprecated and will be removed in v0.9. Please use &#39;LogScalar&#39; instead.&quot;</span><span class="p">,</span>
            <span class="ne">DeprecationWarning</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Convert old API to new API</span>
        <span class="k">if</span> <span class="n">reward_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">reward_key</span> <span class="o">=</span> <span class="n">REWARD_KEY</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">reward_key</span><span class="p">,</span> <span class="n">logname</span><span class="o">=</span><span class="n">logname</span><span class="p">,</span> <span class="n">log_pbar</span><span class="o">=</span><span class="n">log_pbar</span><span class="p">)</span>


<div class="viewcode-block" id="RewardNormalizer"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.RewardNormalizer.html#torchrl.trainers.RewardNormalizer">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">RewardNormalizer</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reward normalizer hook.</span>

<span class="sd">    Args:</span>
<span class="sd">        decay (:obj:`float`, optional): exponential moving average decay parameter.</span>
<span class="sd">            Default is 0.999</span>
<span class="sd">        scale (:obj:`float`, optional): the scale used to multiply the reward once</span>
<span class="sd">            normalized. Defaults to 1.0.</span>
<span class="sd">        eps (:obj:`float`, optional): the epsilon jitter used to prevent numerical</span>
<span class="sd">            underflow. Defaults to ``torch.finfo(DEFAULT_DTYPE).eps``</span>
<span class="sd">            where ``DEFAULT_DTYPE=torch.get_default_dtype()``.</span>
<span class="sd">        reward_key (str or tuple, optional): the key where to find the reward</span>
<span class="sd">            in the input batch. Defaults to ``(&quot;next&quot;, &quot;reward&quot;)``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; reward_normalizer = RewardNormalizer()</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;batch_process&quot;, reward_normalizer.update_reward_stats)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;process_optim_batch&quot;, reward_normalizer.normalize_reward)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
        <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">log_pbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">reward_key</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_has_been_called</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_has_been_called</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;decay&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">())</span><span class="o">.</span><span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="k">if</span> <span class="n">reward_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">reward_key</span> <span class="o">=</span> <span class="n">REWARD_KEY</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_key</span> <span class="o">=</span> <span class="n">reward_key</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">update_reward_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_key</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span><span class="p">[</span><span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">))]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_has_been_called</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_has_been_called</span><span class="p">:</span>
            <span class="c1"># We&#39;d like to check that rewards are normalized. Problem is that the trainer can collect data without calling steps...</span>
            <span class="c1"># raise RuntimeError(</span>
            <span class="c1">#     &quot;There have been two consecutive calls to update_reward_stats without a call to normalize_reward. &quot;</span>
            <span class="c1">#     &quot;Check that normalize_reward has been registered in the trainer.&quot;</span>
            <span class="c1"># )</span>
            <span class="k">pass</span>
        <span class="n">decay</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;decay&quot;</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)</span>
        <span class="nb">sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">reward</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">ssq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;ssq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ssq&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">reward</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;count&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">+</span> <span class="n">reward</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span> <span class="o">/</span> <span class="n">count</span>
        <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;var&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">ssq</span> <span class="o">-</span> <span class="nb">sum</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">count</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;var&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">sum</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;std&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_has_been_called</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">normalize_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="n">tensordict</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">to_tensordict</span><span class="p">()</span>  <span class="c1"># make sure it is not a SubTensorDict</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_key</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">reward</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">reward</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;std&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">reward</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">]</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">[</span><span class="s2">&quot;std&quot;</span><span class="p">]</span>

        <span class="n">tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_key</span><span class="p">,</span> <span class="n">reward</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_has_been_called</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="n">tensordict</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;_reward_stats&quot;</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reward_stats</span><span class="p">),</span>
            <span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
            <span class="s2">&quot;_normalize_has_been_called&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_has_been_called</span><span class="p">,</span>
            <span class="s2">&quot;_update_has_been_called&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_has_been_called</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

<div class="viewcode-block" id="RewardNormalizer.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.RewardNormalizer.html#torchrl.trainers.RewardNormalizer.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;reward_normalizer&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;batch_process&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_reward_stats</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;process_optim_batch&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize_reward</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div></div>


<span class="k">def</span><span class="w"> </span><span class="nf">mask_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Batch masking hook.</span>

<span class="sd">    If a tensordict contained padded trajectories but only single events are</span>
<span class="sd">    needed, this hook can be used to select the valid events from the original</span>
<span class="sd">    tensordict.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch:</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; trainer = mocking_trainer()</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;batch_process&quot;, mask_batch)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">batch</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">batch</span>


<div class="viewcode-block" id="BatchSubSampler"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.BatchSubSampler.html#torchrl.trainers.BatchSubSampler">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">BatchSubSampler</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Data subsampler for online RL sota-implementations.</span>

<span class="sd">    This class subsamples a part of a whole batch of data just collected from the</span>
<span class="sd">    environment.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size (int): sub-batch size to collect. The provided batch size</span>
<span class="sd">            must be equal to the total number of items in the output tensordict,</span>
<span class="sd">            which will have size [batch_size // sub_traj_len, sub_traj_len].</span>
<span class="sd">        sub_traj_len (int, optional): length of the trajectories that</span>
<span class="sd">            sub-samples must have in online settings. Default is -1 (i.e.</span>
<span class="sd">            takes the full length of the trajectory)</span>
<span class="sd">        min_sub_traj_len (int, optional): minimum value of :obj:`sub_traj_len`, in</span>
<span class="sd">            case some elements of the batch contain few steps.</span>
<span class="sd">            Default is -1 (i.e. no minimum value)</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; td = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         key1: torch.stack([torch.arange(0, 10), torch.arange(10, 20)], 0),</span>
<span class="sd">        ...         key2: torch.stack([torch.arange(0, 10), torch.arange(10, 20)], 0),</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     [2, 10],</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(</span>
<span class="sd">        ...     &quot;process_optim_batch&quot;,</span>
<span class="sd">        ...     BatchSubSampler(batch_size=batch_size, sub_traj_len=sub_traj_len),</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; td_out = trainer._process_optim_batch_hook(td)</span>
<span class="sd">        &gt;&gt;&gt; assert td_out.shape == torch.Size([batch_size // sub_traj_len, sub_traj_len])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">sub_traj_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">min_sub_traj_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub_traj_len</span> <span class="o">=</span> <span class="n">sub_traj_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_sub_traj_len</span> <span class="o">=</span> <span class="n">min_sub_traj_len</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sub-sampled part of a batch randomly.</span>

<span class="sd">        If the batch has one dimension, a random subsample of length</span>
<span class="sd">        self.bach_size will be returned. If the batch has two or more</span>
<span class="sd">        dimensions, it is assumed that the first dimension represents the</span>
<span class="sd">        batch, and the second the time. If so, the resulting subsample will</span>
<span class="sd">        contain consecutive samples across time.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">batch</span><span class="o">.</span><span class="n">ndimension</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">batch</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">]]</span>

        <span class="n">sub_traj_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_traj_len</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub_traj_len</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
            <span class="c1"># if a valid mask is present, it&#39;s important to sample only</span>
            <span class="c1"># valid steps</span>
            <span class="n">traj_len</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">sub_traj_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">min_sub_traj_len</span><span class="p">,</span>
                <span class="nb">min</span><span class="p">(</span><span class="n">sub_traj_len</span><span class="p">,</span> <span class="n">traj_len</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">traj_len</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
                <span class="o">*</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="n">len_mask</span> <span class="o">=</span> <span class="n">traj_len</span> <span class="o">&gt;=</span> <span class="n">sub_traj_len</span>
        <span class="n">valid_trajectories</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">device</span><span class="p">)[</span><span class="n">len_mask</span><span class="p">]</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">sub_traj_len</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Resulting batch size is zero. The batch size given to &quot;</span>
                <span class="s2">&quot;BatchSubSampler must be equal to the total number of elements &quot;</span>
                <span class="s2">&quot;that will result in a batch provided to the loss function.&quot;</span>
            <span class="p">)</span>
        <span class="n">traj_idx</span> <span class="o">=</span> <span class="n">valid_trajectories</span><span class="p">[</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span>
                <span class="n">valid_trajectories</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="n">sub_traj_len</span> <span class="o">&lt;</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">_traj_len</span> <span class="o">=</span> <span class="n">traj_len</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span>
            <span class="n">seq_idx</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">_traj_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
                <span class="o">*</span> <span class="p">(</span><span class="n">_traj_len</span> <span class="o">-</span> <span class="n">sub_traj_len</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
            <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">seq_idx</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sub_traj_len</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">sub_traj_len</span> <span class="o">==</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">sub_traj_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;sub_traj_len=</span><span class="si">{</span><span class="n">sub_traj_len</span><span class="si">}</span><span class="s2"> is not allowed. Accepted values &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;are in the range [1, </span><span class="si">{</span><span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">].&quot;</span>
            <span class="p">)</span>

        <span class="n">seq_idx</span> <span class="o">=</span> <span class="n">seq_idx</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sub_traj_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">seq_idx</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">td</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">traj_idx</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">td</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
                <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">index</span><span class="o">=</span><span class="n">expand_right</span><span class="p">(</span><span class="n">seq_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sub_traj_len</span><span class="p">,</span> <span class="o">*</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])),</span>
            <span class="p">),</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sub_traj_len</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">td</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="p">(</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Sampled invalid steps&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">td</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">pass</span>

<div class="viewcode-block" id="BatchSubSampler.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.BatchSubSampler.html#torchrl.trainers.BatchSubSampler.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;batch_subsampler&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span>
            <span class="s2">&quot;process_optim_batch&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LogValidationReward"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.LogValidationReward.html#torchrl.trainers.LogValidationReward">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">LogValidationReward</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Recorder hook for :class:`~torchrl.trainers.Trainer`.</span>

<span class="sd">    Args:</span>
<span class="sd">        record_interval (int): total number of optimization steps</span>
<span class="sd">            between two calls to the recorder for testing.</span>
<span class="sd">        record_frames (int): number of frames to be recorded during</span>
<span class="sd">            testing.</span>
<span class="sd">        frame_skip (int): frame_skip used in the environment. It is</span>
<span class="sd">            important to let the trainer know the number of frames skipped at</span>
<span class="sd">            each iteration, otherwise the frame count can be underestimated.</span>
<span class="sd">            For logging, this parameter is important to normalize the reward.</span>
<span class="sd">            Finally, to compare different runs with different frame_skip,</span>
<span class="sd">            one must normalize the frame count and rewards. Defaults to ``1``.</span>
<span class="sd">        policy_exploration (ProbabilisticTDModule): a policy</span>
<span class="sd">            instance used for</span>

<span class="sd">            (1) updating the exploration noise schedule;</span>

<span class="sd">            (2) testing the policy on the recorder.</span>

<span class="sd">            Given that this instance is supposed to both explore and render</span>
<span class="sd">            the performance of the policy, it should be possible to turn off</span>
<span class="sd">            the explorative behavior by calling the</span>
<span class="sd">            `set_exploration_type(ExplorationType.DETERMINISTIC)` context manager.</span>
<span class="sd">        environment (EnvBase): An environment instance to be used</span>
<span class="sd">            for testing.</span>
<span class="sd">        exploration_type (ExplorationType, optional): exploration mode to use for the</span>
<span class="sd">            policy. By default, no exploration is used and the value used is</span>
<span class="sd">            ``ExplorationType.DETERMINISTIC``. Set to ``ExplorationType.RANDOM`` to enable exploration</span>
<span class="sd">        log_keys (sequence of str or tuples or str, optional): keys to read in the tensordict</span>
<span class="sd">            for logging. Defaults to ``[(&quot;next&quot;, &quot;reward&quot;)]``.</span>
<span class="sd">        out_keys (Dict[str, str], optional): a dictionary mapping the ``log_keys``</span>
<span class="sd">            to their name in the logs. Defaults to ``{(&quot;next&quot;, &quot;reward&quot;): &quot;r_evaluation&quot;}``.</span>
<span class="sd">        suffix (str, optional): suffix of the video to be recorded.</span>
<span class="sd">        log_pbar (bool, optional): if ``True``, the reward value will be logged on</span>
<span class="sd">            the progression bar. Default is `False`.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">ENV_DEPREC</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s2">&quot;the environment should be passed under the &#39;environment&#39; key&quot;</span>
        <span class="s2">&quot; and not the &#39;recorder&#39; key.&quot;</span>
    <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">record_interval</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">record_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">frame_skip</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">policy_exploration</span><span class="p">:</span> <span class="n">TensorDictModule</span><span class="p">,</span>
        <span class="n">environment</span><span class="p">:</span> <span class="n">EnvBase</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">exploration_type</span><span class="p">:</span> <span class="n">ExplorationType</span> <span class="o">=</span> <span class="n">ExplorationType</span><span class="o">.</span><span class="n">RANDOM</span><span class="p">,</span>
        <span class="n">log_keys</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">out_keys</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">suffix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">log_pbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">recorder</span><span class="p">:</span> <span class="n">EnvBase</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">environment</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">recorder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ENV_DEPREC</span><span class="p">)</span>
            <span class="n">environment</span> <span class="o">=</span> <span class="n">recorder</span>
        <span class="k">elif</span> <span class="n">environment</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">recorder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;environment and recorder conflict.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span> <span class="o">=</span> <span class="n">policy_exploration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">environment</span> <span class="o">=</span> <span class="n">environment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_frames</span> <span class="o">=</span> <span class="n">record_frames</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span> <span class="o">=</span> <span class="n">frame_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">record_interval</span> <span class="o">=</span> <span class="n">record_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exploration_type</span> <span class="o">=</span> <span class="n">exploration_type</span>
        <span class="k">if</span> <span class="n">log_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">log_keys</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">)]</span>
        <span class="k">if</span> <span class="n">out_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">out_keys</span> <span class="o">=</span> <span class="n">KeyDependentDefaultDict</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
            <span class="n">out_keys</span><span class="p">[(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="s2">&quot;r_evaluation&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_keys</span> <span class="o">=</span> <span class="n">log_keys</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span> <span class="o">=</span> <span class="n">out_keys</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">suffix</span> <span class="o">=</span> <span class="n">suffix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span> <span class="o">=</span> <span class="n">log_pbar</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">record_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">set_exploration_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">exploration_type</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
                <span class="n">td_record</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">rollout</span><span class="p">(</span>
                    <span class="n">policy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span><span class="p">,</span>
                    <span class="n">max_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">record_frames</span><span class="p">,</span>
                    <span class="n">auto_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">auto_cast_to_device</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">break_when_any_done</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="n">td_record</span> <span class="o">=</span> <span class="n">split_trajectories</span><span class="p">(</span><span class="n">td_record</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">policy_exploration</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">suffix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">suffix</span><span class="p">)</span>

                <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_keys</span><span class="p">:</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">td_record</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">):</span>
                        <span class="n">mask</span> <span class="o">=</span> <span class="n">td_record</span><span class="p">[</span><span class="s2">&quot;mask&quot;</span><span class="p">]</span>
                        <span class="n">mean_value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span>
                        <span class="n">total_value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">td_record</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                        <span class="n">out</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span> <span class="o">=</span> <span class="n">mean_value</span>
                        <span class="n">out</span><span class="p">[</span><span class="s2">&quot;total_&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span> <span class="o">=</span> <span class="n">total_value</span>
                        <span class="k">continue</span>
                    <span class="n">out</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="n">key</span><span class="p">]]</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">out</span><span class="p">[</span><span class="s2">&quot;log_pbar&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;_count&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count</span><span class="p">,</span>
            <span class="s2">&quot;recorder_state_dict&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_count</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_count&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">environment</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;recorder_state_dict&quot;</span><span class="p">])</span>

<div class="viewcode-block" id="LogValidationReward.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.LogValidationReward.html#torchrl.trainers.LogValidationReward.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;recorder&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span>
            <span class="s2">&quot;post_steps_log&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span></div></div>


<span class="k">class</span><span class="w"> </span><span class="nc">Recorder</span><span class="p">(</span><span class="n">LogValidationReward</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Deprecated class. Use LogValidationReward instead.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">record_interval</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">record_frames</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">frame_skip</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">policy_exploration</span><span class="p">:</span> <span class="n">TensorDictModule</span><span class="p">,</span>
        <span class="n">environment</span><span class="p">:</span> <span class="n">EnvBase</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">exploration_type</span><span class="p">:</span> <span class="n">ExplorationType</span> <span class="o">=</span> <span class="n">ExplorationType</span><span class="o">.</span><span class="n">RANDOM</span><span class="p">,</span>
        <span class="n">log_keys</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">out_keys</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">suffix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">log_pbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">recorder</span><span class="p">:</span> <span class="n">EnvBase</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
            <span class="s2">&quot;The &#39;Recorder&#39; class is deprecated and will be removed in v0.9. Please use &#39;LogValidationReward&#39; instead.&quot;</span><span class="p">,</span>
            <span class="ne">DeprecationWarning</span><span class="p">,</span>
            <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">record_interval</span><span class="o">=</span><span class="n">record_interval</span><span class="p">,</span>
            <span class="n">record_frames</span><span class="o">=</span><span class="n">record_frames</span><span class="p">,</span>
            <span class="n">frame_skip</span><span class="o">=</span><span class="n">frame_skip</span><span class="p">,</span>
            <span class="n">policy_exploration</span><span class="o">=</span><span class="n">policy_exploration</span><span class="p">,</span>
            <span class="n">environment</span><span class="o">=</span><span class="n">environment</span><span class="p">,</span>
            <span class="n">exploration_type</span><span class="o">=</span><span class="n">exploration_type</span><span class="p">,</span>
            <span class="n">log_keys</span><span class="o">=</span><span class="n">log_keys</span><span class="p">,</span>
            <span class="n">out_keys</span><span class="o">=</span><span class="n">out_keys</span><span class="p">,</span>
            <span class="n">suffix</span><span class="o">=</span><span class="n">suffix</span><span class="p">,</span>
            <span class="n">log_pbar</span><span class="o">=</span><span class="n">log_pbar</span><span class="p">,</span>
            <span class="n">recorder</span><span class="o">=</span><span class="n">recorder</span><span class="p">,</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_resolve_module</span><span class="p">(</span><span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Resolve a module from a trainer using a string path.</span>

<span class="sd">    Args:</span>
<span class="sd">        trainer (Trainer): The trainer instance to resolve from.</span>
<span class="sd">        path (str): A dot-separated path to the module (e.g., &quot;loss_module.actor_network&quot;).</span>

<span class="sd">    Returns:</span>
<span class="sd">        The resolved module.</span>

<span class="sd">    Raises:</span>
<span class="sd">        AttributeError: If the path cannot be resolved.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; module = _resolve_module(trainer, &quot;loss_module.actor_network&quot;)</span>
<span class="sd">        &gt;&gt;&gt; module = _resolve_module(trainer, &quot;collector.policy&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">obj</span> <span class="o">=</span> <span class="n">trainer</span>
    <span class="k">for</span> <span class="n">attr</span> <span class="ow">in</span> <span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">):</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">obj</span>


<div class="viewcode-block" id="UpdateWeights"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.UpdateWeights.html#torchrl.trainers.UpdateWeights">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">UpdateWeights</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A collector weights update hook class.</span>

<span class="sd">    This hook must be used whenever the collector policy weights sit on a</span>
<span class="sd">    different device than the policy weights being trained by the Trainer.</span>
<span class="sd">    In that case, those weights must be synced across devices at regular</span>
<span class="sd">    intervals. If the devices match, this will result in a no-op.</span>

<span class="sd">    Args:</span>
<span class="sd">        collector (BaseCollector): A data collector where the policy weights</span>
<span class="sd">            must be synced.</span>
<span class="sd">        update_weights_interval (int): Interval (in terms of number of batches</span>
<span class="sd">            collected) where the sync must take place.</span>
<span class="sd">        policy_weights_getter (Callable, optional): A callable that returns the policy</span>
<span class="sd">            weights to sync. Used for backward compatibility. If both this and</span>
<span class="sd">            weight_update_map are provided, weight_update_map takes precedence.</span>
<span class="sd">        weight_update_map (dict[str, str], optional): A mapping from destination paths</span>
<span class="sd">            (keys in collector&#39;s weight_sync_schemes) to source paths on the trainer.</span>
<span class="sd">            Example: {&quot;policy&quot;: &quot;loss_module.actor_network&quot;,</span>
<span class="sd">                     &quot;replay_buffer.transforms[0]&quot;: &quot;loss_module.critic_network&quot;}</span>
<span class="sd">        trainer (Trainer, optional): The trainer instance, required when using</span>
<span class="sd">            weight_update_map to resolve source paths.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Legacy usage with policy_weights_getter</span>
<span class="sd">        &gt;&gt;&gt; update_weights = UpdateWeights(</span>
<span class="sd">        ...     trainer.collector, T,</span>
<span class="sd">        ...     policy_weights_getter=lambda: TensorDict.from_module(policy)</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;post_steps&quot;, update_weights)</span>

<span class="sd">        &gt;&gt;&gt; # New usage with weight_update_map</span>
<span class="sd">        &gt;&gt;&gt; update_weights = UpdateWeights(</span>
<span class="sd">        ...     trainer.collector, T,</span>
<span class="sd">        ...     weight_update_map={</span>
<span class="sd">        ...         &quot;policy&quot;: &quot;loss_module.actor_network&quot;,</span>
<span class="sd">        ...         &quot;replay_buffer.transforms[0]&quot;: &quot;loss_module.critic_network&quot;</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     trainer=trainer</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;post_steps&quot;, update_weights)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">collector</span><span class="p">:</span> <span class="n">BaseCollector</span><span class="p">,</span>
        <span class="n">update_weights_interval</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">policy_weights_getter</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">weight_update_map</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span> <span class="o">=</span> <span class="n">collector</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_weights_interval</span> <span class="o">=</span> <span class="n">update_weights_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_weights_getter</span> <span class="o">=</span> <span class="n">policy_weights_getter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_update_map</span> <span class="o">=</span> <span class="n">weight_update_map</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">trainer</span>

        <span class="c1"># Validate inputs</span>
        <span class="k">if</span> <span class="n">weight_update_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">trainer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;trainer must be provided when using weight_update_map&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_weights_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># New approach: use weight_update_map if provided</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_update_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_update_with_map</span><span class="p">()</span>
            <span class="c1"># Legacy approach: use policy_weights_getter</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">policy_weights_getter</span><span class="p">()</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_weights_getter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="kc">None</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">update_policy_weights_</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">update_policy_weights_</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update_with_map</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update weights using the weight_update_map.&quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.weight_update.weight_sync_schemes</span><span class="w"> </span><span class="kn">import</span> <span class="n">WeightStrategy</span>

        <span class="n">weights_dict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">destination</span><span class="p">,</span> <span class="n">source_path</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_update_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Resolve the source module from the trainer</span>
            <span class="n">source_module</span> <span class="o">=</span> <span class="n">_resolve_module</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="p">,</span> <span class="n">source_path</span><span class="p">)</span>

            <span class="c1"># Get the scheme for this destination to know the extraction strategy</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="p">,</span> <span class="s2">&quot;_weight_sync_schemes&quot;</span><span class="p">)</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">_weight_sync_schemes</span>
                <span class="ow">and</span> <span class="n">destination</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">_weight_sync_schemes</span>
            <span class="p">):</span>
                <span class="n">scheme</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">_weight_sync_schemes</span><span class="p">[</span><span class="n">destination</span><span class="p">]</span>
                <span class="n">strategy</span> <span class="o">=</span> <span class="n">WeightStrategy</span><span class="p">(</span><span class="n">extract_as</span><span class="o">=</span><span class="n">scheme</span><span class="o">.</span><span class="n">strategy_str</span><span class="p">)</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="n">strategy</span><span class="o">.</span><span class="n">extract_weights</span><span class="p">(</span><span class="n">source_module</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Fallback: use TensorDict extraction if no scheme found</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="o">.</span><span class="n">from_module</span><span class="p">(</span><span class="n">source_module</span><span class="p">)</span>

            <span class="n">weights_dict</span><span class="p">[</span><span class="n">destination</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>

        <span class="c1"># Send all weights atomically</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">update_policy_weights_</span><span class="p">(</span><span class="n">weights_dict</span><span class="o">=</span><span class="n">weights_dict</span><span class="p">)</span>

<div class="viewcode-block" id="UpdateWeights.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.UpdateWeights.html#torchrl.trainers.UpdateWeights.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;update_weights&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span>
            <span class="s2">&quot;post_steps&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span></div>


<div class="viewcode-block" id="CountFramesLog"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.CountFramesLog.html#torchrl.trainers.CountFramesLog">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">CountFramesLog</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A frame counter hook.</span>

<span class="sd">    Args:</span>
<span class="sd">        frame_skip (int): frame skip of the environment. This argument is</span>
<span class="sd">            important to keep track of the total number of frames, not the</span>
<span class="sd">            apparent one.</span>
<span class="sd">        log_pbar (bool, optional): if ``True``, the reward value will be logged on</span>
<span class="sd">            the progression bar. Default is `False`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; count_frames = CountFramesLog(frame_skip=frame_skip)</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;pre_steps_log&quot;, count_frames)</span>


<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">frame_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">frame_skip</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">log_pbar</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span> <span class="o">=</span> <span class="n">frame_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span> <span class="o">=</span> <span class="n">log_pbar</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">)</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">current_frames</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;collector&quot;</span><span class="p">,</span> <span class="s2">&quot;mask&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">current_frames</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frame_count</span> <span class="o">+=</span> <span class="n">current_frames</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;n_frames&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_count</span><span class="p">,</span> <span class="s2">&quot;log_pbar&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_pbar</span><span class="p">}</span>

<div class="viewcode-block" id="CountFramesLog.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.CountFramesLog.html#torchrl.trainers.CountFramesLog.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;count_frames_log&quot;</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span>
            <span class="s2">&quot;pre_steps_log&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;frame_count&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">frame_count</span><span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frame_count</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;frame_count&quot;</span><span class="p">]</span></div>


<span class="k">def</span><span class="w"> </span><span class="nf">_check_input_output_typehint</span><span class="p">(</span>
    <span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="nb">type</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">type</span><span class="p">],</span> <span class="n">output</span><span class="p">:</span> <span class="nb">type</span>
<span class="p">):</span>
    <span class="c1"># Placeholder for a function that checks the types input / output against expectations</span>
    <span class="k">return</span>


<span class="k">def</span><span class="w"> </span><span class="nf">flatten_dict</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Flattens a dictionary with sub-dictionaries accessed through point-separated (:obj:`&quot;var1.var2&quot;`) fields.&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">flatten_dict</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_key</span><span class="p">,</span> <span class="n">_item</span> <span class="ow">in</span> <span class="n">item</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">out</span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">key</span><span class="p">,</span> <span class="n">_key</span><span class="p">])]</span> <span class="o">=</span> <span class="n">_item</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span>
    <span class="k">return</span> <span class="n">out</span>


<div class="viewcode-block" id="TargetNetUpdaterHook"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.TargetNetUpdaterHook.html#torchrl.trainers.TargetNetUpdaterHook">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">TargetNetUpdaterHook</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A hook for target parameters update.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # define a loss module</span>
<span class="sd">        &gt;&gt;&gt; loss_module = SACLoss(actor_network, qvalue_network)</span>
<span class="sd">        &gt;&gt;&gt; # define a target network updater</span>
<span class="sd">        &gt;&gt;&gt; target_net_updater = SoftUpdate(loss_module)</span>
<span class="sd">        &gt;&gt;&gt; # define a target network updater hook</span>
<span class="sd">        &gt;&gt;&gt; target_net_updater_hook = TargetNetUpdaterHook(target_net_updater)</span>
<span class="sd">        &gt;&gt;&gt; # register the target network updater hook</span>
<span class="sd">        &gt;&gt;&gt; trainer.register_op(&quot;post_optim&quot;, target_net_updater_hook)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_params_updater</span><span class="p">:</span> <span class="n">TargetNetUpdater</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target_params_updater</span><span class="p">,</span> <span class="n">TargetNetUpdater</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected a target network updater, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">target_params_updater</span><span class="p">)</span><span class="si">=}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_params_updater</span> <span class="o">=</span> <span class="n">target_params_updater</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorCollection</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_params_updater</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">tensordict</span>

<div class="viewcode-block" id="TargetNetUpdaterHook.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.TargetNetUpdaterHook.html#torchrl.trainers.TargetNetUpdaterHook.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;post_steps&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="UTDRHook"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.UTDRHook.html#torchrl.trainers.UTDRHook">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">UTDRHook</span><span class="p">(</span><span class="n">TrainerHookBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook for logging Update-to-Data (UTD) ratio during async collection.</span>

<span class="sd">    The UTD ratio measures how many optimization steps are performed per</span>
<span class="sd">    collected data sample, providing insight into training efficiency during</span>
<span class="sd">    asynchronous data collection. This metric is particularly useful for</span>
<span class="sd">    off-policy algorithms where data collection and training happen concurrently.</span>

<span class="sd">    The UTD ratio is calculated as: (batch_size * update_count) / write_count</span>
<span class="sd">    where:</span>
<span class="sd">    - batch_size: Size of batches sampled from replay buffer</span>
<span class="sd">    - update_count: Total number of optimization steps performed</span>
<span class="sd">    - write_count: Total number of samples written to replay buffer</span>

<span class="sd">    Args:</span>
<span class="sd">        trainer (Trainer): The trainer instance to monitor for UTD calculation.</span>
<span class="sd">                          Must have async_collection=True for meaningful results.</span>

<span class="sd">    Note:</span>
<span class="sd">        This hook is only meaningful when async_collection is enabled, as it</span>
<span class="sd">        relies on the replay buffer&#39;s write_count to track data collection progress.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">trainer</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDictBase</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="p">,</span> <span class="s2">&quot;replay_buffer&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">replay_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">write_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">write_count</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">write_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">getattr_rb</span><span class="p">(</span><span class="s2">&quot;write_count&quot;</span><span class="p">)</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">collector</span><span class="o">.</span><span class="n">getattr_rb</span><span class="p">(</span><span class="s2">&quot;batch_size&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">write_count</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">rl_warnings</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Batch size is not set. Using 1.&quot;</span><span class="p">)</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">update_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">_optim_count</span>
        <span class="n">utd_ratio</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">update_count</span> <span class="o">/</span> <span class="n">write_count</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;utd_ratio&quot;</span><span class="p">:</span> <span class="n">utd_ratio</span><span class="p">,</span>
            <span class="s2">&quot;write_count&quot;</span><span class="p">:</span> <span class="n">write_count</span><span class="p">,</span>
            <span class="s2">&quot;update_count&quot;</span><span class="p">:</span> <span class="n">update_count</span><span class="p">,</span>
            <span class="s2">&quot;log_pbar&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="p">}</span>

<div class="viewcode-block" id="UTDRHook.register"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.UTDRHook.html#torchrl.trainers.UTDRHook.register">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">register</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;utdr_hook&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Register the UTD ratio hook with the trainer.</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer (Trainer): The trainer to register with.</span>
<span class="sd">            name (str): Name to use when registering the hook module.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_op</span><span class="p">(</span><span class="s2">&quot;pre_steps_log&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">register_module</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span></div>

<div class="viewcode-block" id="UTDRHook.state_dict"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.UTDRHook.html#torchrl.trainers.UTDRHook.state_dict">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return state dictionary for checkpointing.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{}</span></div>

<div class="viewcode-block" id="UTDRHook.load_state_dict"><a class="viewcode-back" href="../../../reference/generated/torchrl.trainers.UTDRHook.html#torchrl.trainers.UTDRHook.load_state_dict">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load state from dictionary.&quot;&quot;&quot;</span></div></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>