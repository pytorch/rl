


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchrl.collectors.distributed.ray &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../../../../versions.html"><span style="font-size:110%">main (0.0.0+unknown) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/export.html">Exporting TorchRL modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/llm_browser.html">TorchRL LLM: Building Tool-Enabled Environments</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
      <li>torchrl.collectors.distributed.ray</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
    
    
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchrl.collectors.distributed.ray</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the MIT license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">threading</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections</span><span class="w"> </span><span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDict</span><span class="p">,</span> <span class="n">TensorDictBase</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl._utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">as_remote</span><span class="p">,</span> <span class="n">logger</span> <span class="k">as</span> <span class="n">torchrl_logger</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors.collectors</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">DataCollectorBase</span><span class="p">,</span>
    <span class="n">DEFAULT_EXPLORATION_TYPE</span><span class="p">,</span>
    <span class="n">MultiaSyncDataCollector</span><span class="p">,</span>
    <span class="n">MultiSyncDataCollector</span><span class="p">,</span>
    <span class="n">SyncDataCollector</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_NON_NN_POLICY_WEIGHTS</span><span class="p">,</span> <span class="n">split_trajectories</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors.weight_update</span><span class="w"> </span><span class="kn">import</span> <span class="n">RayWeightUpdater</span><span class="p">,</span> <span class="n">WeightUpdaterBase</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">ReplayBuffer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.common</span><span class="w"> </span><span class="kn">import</span> <span class="n">EnvBase</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.env_creator</span><span class="w"> </span><span class="kn">import</span> <span class="n">EnvCreator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.weight_update.weight_sync_schemes</span><span class="w"> </span><span class="kn">import</span> <span class="n">WeightSyncScheme</span>

<span class="n">RAY_ERR</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">ray</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">ray._private.services</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_node_ip_address</span>

    <span class="n">_has_ray</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
    <span class="n">_has_ray</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">RAY_ERR</span> <span class="o">=</span> <span class="n">err</span>

<span class="n">DEFAULT_RAY_INIT_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;address&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;num_cpus&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;num_gpus&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;resources&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;object_store_memory&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;local_mode&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;ignore_reinit_error&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="s2">&quot;include_dashboard&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;dashboard_host&quot;</span><span class="p">:</span> <span class="s2">&quot;127.0.0.1&quot;</span><span class="p">,</span>
    <span class="s2">&quot;dashboard_port&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;job_config&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;configure_logging&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;logging_level&quot;</span><span class="p">:</span> <span class="s2">&quot;info&quot;</span><span class="p">,</span>
    <span class="s2">&quot;logging_format&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;log_to_driver&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s2">&quot;namespace&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;runtime_env&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">DEFAULT_REMOTE_CLASS_CONFIG</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;num_cpus&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;num_gpus&quot;</span><span class="p">:</span> <span class="mf">0.2</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="s2">&quot;memory&quot;</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span>
<span class="p">}</span>


<span class="k">def</span><span class="w"> </span><span class="nf">print_remote_collector_info</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Prints some information about the remote collector.&quot;&quot;&quot;</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Created remote collector with in machine &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">get_node_ip_address</span><span class="p">()</span><span class="si">}</span><span class="s2"> using gpus </span><span class="si">{</span><span class="n">ray</span><span class="o">.</span><span class="n">get_gpu_ids</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="c1"># torchrl_logger.warning(s)</span>
    <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>


<div class="viewcode-block" id="RayCollector"><a class="viewcode-back" href="../../../../reference/generated/torchrl.collectors.distributed.RayCollector.html#torchrl.collectors.distributed.RayCollector">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">RayCollector</span><span class="p">(</span><span class="n">DataCollectorBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Distributed data collector with `Ray &lt;https://docs.ray.io/&gt;`_ backend.</span>

<span class="sd">    This Python class serves as a ray-based solution to instantiate and coordinate multiple</span>
<span class="sd">    data collectors in a distributed cluster. Like TorchRL non-distributed collectors, this</span>
<span class="sd">    collector is an iterable that yields TensorDicts until a target number of collected</span>
<span class="sd">    frames is reached, but handles distributed data collection under the hood.</span>

<span class="sd">    The class dictionary input parameter &quot;ray_init_config&quot; can be used to provide the kwargs to</span>
<span class="sd">    call Ray initialization method ray.init(). If &quot;ray_init_config&quot; is not provided, the default</span>
<span class="sd">    behavior is to autodetect an existing Ray cluster or start a new Ray instance locally if no</span>
<span class="sd">    existing cluster is found. Refer to Ray documentation for advanced initialization kwargs.</span>

<span class="sd">    Similarly, dictionary input parameter &quot;remote_configs&quot; can be used to specify the kwargs for</span>
<span class="sd">    ray.remote() when called to create each remote collector actor, including collector compute</span>
<span class="sd">    resources.The sum of all collector resources should be available in the cluster. Refer to Ray</span>
<span class="sd">    documentation for advanced configuration of the ray.remote() method. Default kwargs are:</span>

<span class="sd">    &gt;&gt;&gt; kwargs = {</span>
<span class="sd">    ...     &quot;num_cpus&quot;: 1,</span>
<span class="sd">    ...     &quot;num_gpus&quot;: 0.2,</span>
<span class="sd">    ...     &quot;memory&quot;: 2 * 1024 ** 3,</span>
<span class="sd">    ... }</span>


<span class="sd">    The coordination between collector instances can be specified as &quot;synchronous&quot; or &quot;asynchronous&quot;.</span>
<span class="sd">    In synchronous coordination, this class waits for all remote collectors to collect a rollout,</span>
<span class="sd">    concatenates all rollouts into a single TensorDict instance and finally yields the concatenated</span>
<span class="sd">    data. On the other hand, if the coordination is to be carried out asynchronously, this class</span>
<span class="sd">    provides the rollouts as they become available from individual remote collectors.</span>

<span class="sd">    Args:</span>
<span class="sd">        create_env_fn (Callable or List[Callabled]): list of Callables, each returning an</span>
<span class="sd">            instance of :class:`~torchrl.envs.EnvBase`.</span>
<span class="sd">        policy (Callable, optional): Policy to be executed in the environment.</span>
<span class="sd">            Must accept :class:`tensordict.tensordict.TensorDictBase` object as input.</span>
<span class="sd">            If ``None`` is provided, the policy used will be a</span>
<span class="sd">            :class:`~torchrl.collectors.RandomPolicy` instance with the environment</span>
<span class="sd">            ``action_spec``.</span>
<span class="sd">            Accepted policies are usually subclasses of :class:`~tensordict.nn.TensorDictModuleBase`.</span>
<span class="sd">            This is the recommended usage of the collector.</span>
<span class="sd">            Other callables are accepted too:</span>
<span class="sd">            If the policy is not a ``TensorDictModuleBase`` (e.g., a regular :class:`~torch.nn.Module`</span>
<span class="sd">            instances) it will be wrapped in a `nn.Module` first.</span>
<span class="sd">            Then, the collector will try to assess if these</span>
<span class="sd">            modules require wrapping in a :class:`~tensordict.nn.TensorDictModule` or not.</span>

<span class="sd">            - If the policy forward signature matches any of ``forward(self, tensordict)``,</span>
<span class="sd">              ``forward(self, td)`` or ``forward(self, &lt;anything&gt;: TensorDictBase)`` (or</span>
<span class="sd">              any typing with a single argument typed as a subclass of ``TensorDictBase``)</span>
<span class="sd">              then the policy won&#39;t be wrapped in a :class:`~tensordict.nn.TensorDictModule`.</span>

<span class="sd">            - In all other cases an attempt to wrap it will be undergone as such: ``TensorDictModule(policy, in_keys=env_obs_key, out_keys=env.action_keys)``.</span>

<span class="sd">            .. note:: If the policy needs to be passed as a policy factory (e.g., in case it mustn&#39;t be serialized /</span>
<span class="sd">                pickled directly), the ``policy_factory`` should be used instead.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        policy_factory (Callable[[], Callable], list of Callable[[], Callable], optional): a callable</span>
<span class="sd">            (or list of callables) that returns a policy instance. This is exclusive with the `policy` argument.</span>

<span class="sd">            .. note:: `policy_factory` comes in handy whenever the policy cannot be serialized.</span>

<span class="sd">        trust_policy (bool, optional): if ``True``, a non-TensorDictModule policy will be trusted to be</span>
<span class="sd">            assumed to be compatible with the collector. This defaults to ``True`` for CudaGraphModules</span>
<span class="sd">            and ``False`` otherwise.</span>
<span class="sd">        frames_per_batch (int): A keyword-only argument representing the</span>
<span class="sd">            total number of elements in a batch.</span>
<span class="sd">        total_frames (int, Optional): lower bound of the total number of frames returned by the collector.</span>
<span class="sd">            The iterator will stop once the total number of frames equates or exceeds the total number of</span>
<span class="sd">            frames passed to the collector. Default value is -1, which mean no target total number of frames</span>
<span class="sd">            (i.e. the collector will run indefinitely).</span>
<span class="sd">        device (int, str or torch.device, optional): The generic device of the</span>
<span class="sd">            collector. The ``device`` args fills any non-specified device: if</span>
<span class="sd">            ``device`` is not ``None`` and any of ``storing_device``, ``policy_device`` or</span>
<span class="sd">            ``env_device`` is not specified, its value will be set to ``device``.</span>
<span class="sd">            Defaults to ``None`` (No default device).</span>
<span class="sd">            Lists of devices are supported.</span>
<span class="sd">        storing_device (int, str or torch.device, optional): The *remote* device on which</span>
<span class="sd">            the output :class:`~tensordict.TensorDict` will be stored.</span>
<span class="sd">            If ``device`` is passed and ``storing_device`` is ``None``, it will</span>
<span class="sd">            default to the value indicated by ``device``.</span>
<span class="sd">            For long trajectories, it may be necessary to store the data on a different</span>
<span class="sd">            device than the one where the policy and env are executed.</span>
<span class="sd">            Defaults to ``None`` (the output tensordict isn&#39;t on a specific device,</span>
<span class="sd">            leaf tensors sit on the device where they were created).</span>
<span class="sd">            Lists of devices are supported.</span>
<span class="sd">        env_device (int, str or torch.device, optional): The *remote* device on which</span>
<span class="sd">            the environment should be cast (or executed if that functionality is</span>
<span class="sd">            supported). If not specified and the env has a non-``None`` device,</span>
<span class="sd">            ``env_device`` will default to that value. If ``device`` is passed</span>
<span class="sd">            and ``env_device=None``, it will default to ``device``. If the value</span>
<span class="sd">            as such specified of ``env_device`` differs from ``policy_device``</span>
<span class="sd">            and one of them is not ``None``, the data will be cast to ``env_device``</span>
<span class="sd">            before being passed to the env (i.e., passing different devices to</span>
<span class="sd">            policy and env is supported). Defaults to ``None``.</span>
<span class="sd">            Lists of devices are supported.</span>
<span class="sd">        policy_device (int, str or torch.device, optional): The *remote* device on which</span>
<span class="sd">            the policy should be cast.</span>
<span class="sd">            If ``device`` is passed and ``policy_device=None``, it will default</span>
<span class="sd">            to ``device``. If the value as such specified of ``policy_device``</span>
<span class="sd">            differs from ``env_device`` and one of them is not ``None``,</span>
<span class="sd">            the data will be cast to ``policy_device`` before being passed to</span>
<span class="sd">            the policy (i.e., passing different devices to policy and env is</span>
<span class="sd">            supported). Defaults to ``None``.</span>
<span class="sd">            Lists of devices are supported.</span>
<span class="sd">        create_env_kwargs (dict, optional): Dictionary of kwargs for</span>
<span class="sd">            ``create_env_fn``.</span>
<span class="sd">        max_frames_per_traj (int, optional): Maximum steps per trajectory.</span>
<span class="sd">            Note that a trajectory can span across multiple batches (unless</span>
<span class="sd">            ``reset_at_each_iter`` is set to ``True``, see below).</span>
<span class="sd">            Once a trajectory reaches ``n_steps``, the environment is reset.</span>
<span class="sd">            If the environment wraps multiple environments together, the number</span>
<span class="sd">            of steps is tracked for each environment independently. Negative</span>
<span class="sd">            values are allowed, in which case this argument is ignored.</span>
<span class="sd">            Defaults to ``None`` (i.e., no maximum number of steps).</span>
<span class="sd">        init_random_frames (int, optional): Number of frames for which the</span>
<span class="sd">            policy is ignored before it is called. This feature is mainly</span>
<span class="sd">            intended to be used in offline/model-based settings, where a</span>
<span class="sd">            batch of random trajectories can be used to initialize training.</span>
<span class="sd">            If provided, it will be rounded up to the closest multiple of frames_per_batch.</span>
<span class="sd">            Defaults to ``None`` (i.e. no random frames).</span>
<span class="sd">        reset_at_each_iter (bool, optional): Whether environments should be reset</span>
<span class="sd">            at the beginning of a batch collection.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        postproc (Callable, optional): A post-processing transform, such as</span>
<span class="sd">            a :class:`~torchrl.envs.Transform` or a :class:`~torchrl.data.postprocs.MultiStep`</span>
<span class="sd">            instance.</span>
<span class="sd">            Defaults to ``None``.</span>
<span class="sd">        split_trajs (bool, optional): Boolean indicating whether the resulting</span>
<span class="sd">            TensorDict should be split according to the trajectories.</span>
<span class="sd">            See :func:`~torchrl.collectors.utils.split_trajectories` for more</span>
<span class="sd">            information.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        exploration_type (ExplorationType, optional): interaction mode to be used when</span>
<span class="sd">            collecting data. Must be one of ``torchrl.envs.utils.ExplorationType.DETERMINISTIC``,</span>
<span class="sd">            ``torchrl.envs.utils.ExplorationType.RANDOM``, ``torchrl.envs.utils.ExplorationType.MODE``</span>
<span class="sd">            or ``torchrl.envs.utils.ExplorationType.MEAN``.</span>
<span class="sd">        collector_class (Python class or constructor): a collector class to be remotely instantiated. Can be</span>
<span class="sd">            :class:`~torchrl.collectors.SyncDataCollector`,</span>
<span class="sd">            :class:`~torchrl.collectors.MultiSyncDataCollector`,</span>
<span class="sd">            :class:`~torchrl.collectors.MultiaSyncDataCollector`</span>
<span class="sd">            or a derived class of these.</span>
<span class="sd">            Defaults to :class:`~torchrl.collectors.SyncDataCollector`.</span>
<span class="sd">        collector_kwargs (dict or list, optional): a dictionary of parameters to be passed to the</span>
<span class="sd">            remote data-collector. If a list is provided, each element will</span>
<span class="sd">            correspond to an individual set of keyword arguments for the</span>
<span class="sd">            dedicated collector.</span>
<span class="sd">        num_workers_per_collector (int): the number of copies of the</span>
<span class="sd">            env constructor that is to be used on the remote nodes.</span>
<span class="sd">            Defaults to 1 (a single env per collector).</span>
<span class="sd">            On a single worker node all the sub-workers will be</span>
<span class="sd">            executing the same environment. If different environments need to</span>
<span class="sd">            be executed, they should be dispatched across worker nodes, not</span>
<span class="sd">            subnodes.</span>
<span class="sd">        ray_init_config (dict, Optional): kwargs used to call ray.init().</span>
<span class="sd">        remote_configs (list of dicts, Optional): ray resource specs for each remote collector.</span>
<span class="sd">            A single dict can be provided as well, and will be used in all collectors.</span>
<span class="sd">        num_collectors (int, Optional): total number of collectors to be instantiated.</span>
<span class="sd">        sync (bool): if ``True``, the resulting tensordict is a stack of all the</span>
<span class="sd">            tensordicts collected on each node. If ``False`` (default), each</span>
<span class="sd">            tensordict results from a separate node in a &quot;first-ready,</span>
<span class="sd">            first-served&quot; fashion.</span>
<span class="sd">        update_after_each_batch (bool, optional): if ``True``, the weights will</span>
<span class="sd">            be updated after each collection. For ``sync=True``, this means that</span>
<span class="sd">            all workers will see their weights updated. For ``sync=False``,</span>
<span class="sd">            only the worker from which the data has been gathered will be</span>
<span class="sd">            updated.</span>
<span class="sd">            This is equivalent to `max_weight_update_interval=0`.</span>
<span class="sd">            Defaults to ``False``, i.e. updates have to be executed manually</span>
<span class="sd">            through</span>
<span class="sd">            :meth:`torchrl.collectors.DataCollector.update_policy_weights_`</span>
<span class="sd">        max_weight_update_interval (int, optional): the maximum number of</span>
<span class="sd">            batches that can be collected before the policy weights of a worker</span>
<span class="sd">            is updated.</span>
<span class="sd">            For sync collections, this parameter is overwritten by ``update_after_each_batch``.</span>
<span class="sd">            For async collections, it may be that one worker has not seen its</span>
<span class="sd">            parameters being updated for a certain time even if ``update_after_each_batch``</span>
<span class="sd">            is turned on.</span>
<span class="sd">            Defaults to -1 (no forced update).</span>
<span class="sd">        replay_buffer (RayReplayBuffer, optional): if provided, the collector will not yield tensordicts</span>
<span class="sd">            but populate the buffer instead. Defaults to ``None``.</span>

<span class="sd">            .. note:: although it is not enfoced (to allow users to implement their own replay buffer class), a</span>
<span class="sd">                :class:`~torchrl.data.RayReplayBuffer` instance should be used here.</span>
<span class="sd">        weight_updater (WeightUpdaterBase or constructor, optional): (Deprecated) An instance of :class:`~torchrl.collectors.WeightUpdaterBase`</span>
<span class="sd">            or its subclass, responsible for updating the policy weights on remote inference workers managed by Ray.</span>
<span class="sd">            If not provided, a :class:`~torchrl.collectors.RayWeightUpdater` will be used by default, leveraging</span>
<span class="sd">            Ray&#39;s distributed capabilities.</span>
<span class="sd">            Consider using a constructor if the updater needs to be serialized.</span>
<span class="sd">        weight_sync_schemes (dict[str, WeightSyncScheme], optional): Dictionary mapping model identifiers to</span>
<span class="sd">            :class:`~torchrl.weight_update.weight_sync_schemes.WeightSyncScheme` instances.</span>
<span class="sd">            This is the recommended way to configure weight synchronization. If not provided,</span>
<span class="sd">            defaults to ``{&quot;policy&quot;: RayWeightSyncScheme()}``.</span>
<span class="sd">        use_env_creator (bool, optional): if ``True``, the environment constructor functions will be wrapped</span>
<span class="sd">            in :class:`~torchrl.envs.EnvCreator`. This is useful for multiprocessed settings where shared memory</span>
<span class="sd">            needs to be managed, but Ray has its own object storage mechanism, so this is typically not needed.</span>
<span class="sd">            Defaults to ``False``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from torch import nn</span>
<span class="sd">        &gt;&gt;&gt; from tensordict.nn import TensorDictModule</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.envs.libs.gym import GymEnv</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.collectors import SyncDataCollector</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.collectors.distributed import RayCollector</span>
<span class="sd">        &gt;&gt;&gt; env_maker = lambda: GymEnv(&quot;Pendulum-v1&quot;, device=&quot;cpu&quot;)</span>
<span class="sd">        &gt;&gt;&gt; policy = TensorDictModule(nn.Linear(3, 1), in_keys=[&quot;observation&quot;], out_keys=[&quot;action&quot;])</span>
<span class="sd">        &gt;&gt;&gt; distributed_collector = RayCollector(</span>
<span class="sd">        ...     create_env_fn=[env_maker],</span>
<span class="sd">        ...     policy=policy,</span>
<span class="sd">        ...     collector_class=SyncDataCollector,</span>
<span class="sd">        ...     max_frames_per_traj=50,</span>
<span class="sd">        ...     init_random_frames=-1,</span>
<span class="sd">        ...     reset_at_each_iter=-False,</span>
<span class="sd">        ...     collector_kwargs={</span>
<span class="sd">        ...         &quot;device&quot;: &quot;cpu&quot;,</span>
<span class="sd">        ...         &quot;storing_device&quot;: &quot;cpu&quot;,</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     num_collectors=1,</span>
<span class="sd">        ...     total_frames=10000,</span>
<span class="sd">        ...     frames_per_batch=200,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; for i, data in enumerate(collector):</span>
<span class="sd">        ...     if i == 2:</span>
<span class="sd">        ...         print(data)</span>
<span class="sd">        ...         break</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">create_env_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="n">EnvBase</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">EnvBase</span><span class="p">],</span>
        <span class="n">policy</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">TensorDictBase</span><span class="p">],</span> <span class="n">TensorDictBase</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">policy_factory</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Callable</span><span class="p">]</span>
        <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">Callable</span><span class="p">]]</span>
        <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trust_policy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">frames_per_batch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">total_frames</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">storing_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">env_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_frames_per_traj</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">init_random_frames</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">reset_at_each_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">postproc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">split_trajs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">exploration_type</span><span class="o">=</span><span class="n">DEFAULT_EXPLORATION_TYPE</span><span class="p">,</span>
        <span class="n">collector_class</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">TensorDict</span><span class="p">],</span> <span class="n">TensorDict</span><span class="p">]</span> <span class="o">=</span> <span class="n">SyncDataCollector</span><span class="p">,</span>
        <span class="n">collector_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_workers_per_collector</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">sync</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">ray_init_config</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">remote_configs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_collectors</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">update_after_each_batch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_weight_update_interval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">replay_buffer</span><span class="p">:</span> <span class="n">ReplayBuffer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">weight_updater</span><span class="p">:</span> <span class="n">WeightUpdaterBase</span>
        <span class="o">|</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">WeightUpdaterBase</span><span class="p">]</span>
        <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">weight_sync_schemes</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">WeightSyncScheme</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_env_creator</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">no_cuda_sync</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frames_per_batch</span> <span class="o">=</span> <span class="n">frames_per_batch</span>
        <span class="k">if</span> <span class="n">remote_configs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">remote_configs</span> <span class="o">=</span> <span class="n">DEFAULT_REMOTE_CLASS_CONFIG</span>

        <span class="k">if</span> <span class="n">ray_init_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ray_init_config</span> <span class="o">=</span> <span class="n">DEFAULT_RAY_INIT_CONFIG</span>

        <span class="k">if</span> <span class="n">collector_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">collector_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">replay_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">collector_kwargs</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">collector_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;replay_buffer&quot;</span><span class="p">,</span> <span class="n">replay_buffer</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">collector_kwargs</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">ck</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;replay_buffer&quot;</span><span class="p">,</span> <span class="n">replay_buffer</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">ck</span> <span class="ow">in</span> <span class="n">collector_kwargs</span>
                <span class="p">]</span>

        <span class="c1"># Make sure input parameters are consistent</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">check_consistency_with_num_collectors</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">num_collectors</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Checks that if param is a list, it has length num_collectors.&quot;&quot;&quot;</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_collectors</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Inconsistent RayDistributedCollector parameters, </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> is a list of length &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="p">)</span><span class="si">}</span><span class="s2"> but the specified number of collectors is </span><span class="si">{</span><span class="n">num_collectors</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">param</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_collectors</span>
            <span class="k">return</span> <span class="n">param</span>

        <span class="k">if</span> <span class="n">num_collectors</span><span class="p">:</span>
            <span class="n">create_env_fn</span> <span class="o">=</span> <span class="n">check_consistency_with_num_collectors</span><span class="p">(</span>
                <span class="n">create_env_fn</span><span class="p">,</span> <span class="s2">&quot;create_env_fn&quot;</span><span class="p">,</span> <span class="n">num_collectors</span>
            <span class="p">)</span>
            <span class="n">collector_kwargs</span> <span class="o">=</span> <span class="n">check_consistency_with_num_collectors</span><span class="p">(</span>
                <span class="n">collector_kwargs</span><span class="p">,</span> <span class="s2">&quot;collector_kwargs&quot;</span><span class="p">,</span> <span class="n">num_collectors</span>
            <span class="p">)</span>
            <span class="n">remote_configs</span> <span class="o">=</span> <span class="n">check_consistency_with_num_collectors</span><span class="p">(</span>
                <span class="n">remote_configs</span><span class="p">,</span> <span class="s2">&quot;remote_config&quot;</span><span class="p">,</span> <span class="n">num_collectors</span>
            <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">check_list_length_consistency</span><span class="p">(</span><span class="o">*</span><span class="n">lists</span><span class="p">):</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Checks that all input lists have the same length.</span>

<span class="sd">            If any non-list input is given, it is converted to a list</span>
<span class="sd">            of the same length as the others by repeating the same</span>
<span class="sd">            element multiple times.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">lengths</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
            <span class="n">new_lists</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">lst</span> <span class="ow">in</span> <span class="n">lists</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lst</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="n">lengths</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lst</span><span class="p">))</span>
                    <span class="n">new_lists</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">lst</span><span class="p">]</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
                    <span class="n">new_lists</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_lst</span><span class="p">)</span>
                    <span class="n">lengths</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_lst</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Inconsistent RayDistributedCollector parameters. create_env_fn, &quot;</span>
                    <span class="s2">&quot;collector_kwargs and remote_configs are lists of different length.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">new_lists</span>

        <span class="n">out_lists</span> <span class="o">=</span> <span class="n">check_list_length_consistency</span><span class="p">(</span>
            <span class="n">create_env_fn</span><span class="p">,</span> <span class="n">collector_kwargs</span><span class="p">,</span> <span class="n">remote_configs</span>
        <span class="p">)</span>
        <span class="n">create_env_fn</span><span class="p">,</span> <span class="n">collector_kwargs</span><span class="p">,</span> <span class="n">remote_configs</span> <span class="o">=</span> <span class="n">out_lists</span>
        <span class="n">num_collectors</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">create_env_fn</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_env_creator</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">create_env_fn</span><span class="p">)):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">create_env_fn</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="n">EnvBase</span><span class="p">,</span> <span class="n">EnvCreator</span><span class="p">)):</span>
                    <span class="n">create_env_fn</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">EnvCreator</span><span class="p">(</span><span class="n">create_env_fn</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="c1"># If ray available, try to connect to an existing Ray cluster or start one and connect to it.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_ray</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;ray library not found, unable to create a DistributedCollector. &quot;</span>
            <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">RAY_ERR</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ray</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="o">**</span><span class="n">ray_init_config</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ray</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Ray could not be initialized.&quot;</span><span class="p">)</span>

        <span class="c1"># Define collector_class, monkey patch it with as_remote and print_remote_collector_info methods</span>
        <span class="k">if</span> <span class="n">collector_class</span> <span class="o">==</span> <span class="s2">&quot;async&quot;</span><span class="p">:</span>
            <span class="n">collector_class</span> <span class="o">=</span> <span class="n">MultiaSyncDataCollector</span>
        <span class="k">elif</span> <span class="n">collector_class</span> <span class="o">==</span> <span class="s2">&quot;sync&quot;</span><span class="p">:</span>
            <span class="n">collector_class</span> <span class="o">=</span> <span class="n">MultiSyncDataCollector</span>
        <span class="k">elif</span> <span class="n">collector_class</span> <span class="o">==</span> <span class="s2">&quot;single&quot;</span><span class="p">:</span>
            <span class="n">collector_class</span> <span class="o">=</span> <span class="n">SyncDataCollector</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">collector_class</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span>
            <span class="n">collector_class</span><span class="p">,</span> <span class="n">DataCollectorBase</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;The collector_class must be an instance of DataCollectorBase.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">collector_class</span><span class="p">,</span> <span class="s2">&quot;as_remote&quot;</span><span class="p">):</span>
            <span class="n">collector_class</span><span class="o">.</span><span class="n">as_remote</span> <span class="o">=</span> <span class="n">as_remote</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">collector_class</span><span class="p">,</span> <span class="s2">&quot;print_remote_collector_info&quot;</span><span class="p">):</span>
            <span class="n">collector_class</span><span class="o">.</span><span class="n">print_remote_collector_info</span> <span class="o">=</span> <span class="n">print_remote_collector_info</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">no_cuda_sync</span> <span class="o">=</span> <span class="n">no_cuda_sync</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">replay_buffer</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policy_factory</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">):</span>
            <span class="n">policy_factory</span> <span class="o">=</span> <span class="p">[</span><span class="n">policy_factory</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">create_env_fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_factory</span> <span class="o">=</span> <span class="n">policy_factory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="n">policy</span>  <span class="c1"># Store policy for weight extraction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trust_policy</span> <span class="o">=</span> <span class="n">trust_policy</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="n">policy_weights</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="o">.</span><span class="n">from_module</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
            <span class="n">policy_weights</span> <span class="o">=</span> <span class="n">policy_weights</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">lock_</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">policy_weights</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">(</span><span class="n">lock</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">weight_updater</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">_NON_NN_POLICY_WEIGHTS</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_weights</span> <span class="o">=</span> <span class="n">policy_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collector_class</span> <span class="o">=</span> <span class="n">collector_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">split_trajs</span> <span class="o">=</span> <span class="n">split_trajs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_frames</span> <span class="o">=</span> <span class="n">total_frames</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span> <span class="o">=</span> <span class="n">num_collectors</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">update_after_each_batch</span> <span class="o">=</span> <span class="n">update_after_each_batch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_weight_update_interval</span> <span class="o">=</span> <span class="n">max_weight_update_interval</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">collector_kwargs</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">collector_kwargs</span> <span class="k">if</span> <span class="n">collector_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">[{}]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">storing_device</span> <span class="o">=</span> <span class="n">storing_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_device</span> <span class="o">=</span> <span class="n">env_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy_device</span> <span class="o">=</span> <span class="n">policy_device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_batches_since_weight_update</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync</span> <span class="o">=</span> <span class="n">sync</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stop_event</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Event</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">frames_per_batch</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot dispatch </span><span class="si">{</span><span class="n">frames_per_batch</span><span class="si">}</span><span class="s2"> frames across </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Consider using a number of frames per batch that is divisible by the number of workers.&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_frames_per_batch_corrected</span> <span class="o">=</span> <span class="n">frames_per_batch</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_frames_per_batch_corrected</span> <span class="o">=</span> <span class="n">frames_per_batch</span>

        <span class="c1"># update collector kwargs</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">collector_kwarg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">collector_kwargs</span><span class="p">):</span>
            <span class="c1"># Don&#39;t pass policy_factory if we have a policy - remote collectors need the policy object</span>
            <span class="c1"># to be able to apply weight updates</span>
            <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;policy_factory&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy_factory</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;max_frames_per_traj&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">max_frames_per_traj</span>
            <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;init_random_frames&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">init_random_frames</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync</span> <span class="ow">and</span> <span class="n">init_random_frames</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                    <span class="s2">&quot;async distributed data collection with init_random_frames &gt; 0 &quot;</span>
                    <span class="s2">&quot;may have unforeseen consequences as we do not control that once &quot;</span>
                    <span class="s2">&quot;non-random data is being collected all nodes are returning non-random data. &quot;</span>
                    <span class="s2">&quot;If this is a feature that you feel should be fixed, please raise an issue on &quot;</span>
                    <span class="s2">&quot;torchrl&#39;s repo.&quot;</span>
                <span class="p">)</span>
            <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;reset_at_each_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">reset_at_each_iter</span>
            <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;exploration_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">exploration_type</span>
            <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;split_trajs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;frames_per_batch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frames_per_batch_corrected</span>
            <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;storing_device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">storing_device</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;env_device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_device</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;policy_device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy_device</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="s2">&quot;trust_policy&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">collector_kwarg</span><span class="p">:</span>
                <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;trust_policy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trust_policy</span>
            <span class="k">if</span> <span class="s2">&quot;no_cuda_sync&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">collector_kwarg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_cuda_sync</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">collector_kwarg</span><span class="p">[</span><span class="s2">&quot;no_cuda_sync&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">no_cuda_sync</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">postproc</span> <span class="o">=</span> <span class="n">postproc</span>

        <span class="c1"># Create remote instances of the collector class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_remote_collectors</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_collectors</span><span class="p">(</span>
                <span class="n">create_env_fn</span><span class="p">,</span>
                <span class="n">num_workers_per_collector</span><span class="p">,</span>
                <span class="n">policy</span><span class="p">,</span>
                <span class="n">collector_kwargs</span><span class="p">,</span>
                <span class="n">remote_configs</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># Set up weight synchronization - prefer new schemes over legacy updater</span>
        <span class="k">if</span> <span class="n">weight_updater</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">weight_sync_schemes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Default to Ray weight sync scheme for Ray collectors</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.weight_update.weight_sync_schemes</span><span class="w"> </span><span class="kn">import</span> <span class="n">RayWeightSyncScheme</span>

            <span class="n">weight_sync_schemes</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;policy&quot;</span><span class="p">:</span> <span class="n">RayWeightSyncScheme</span><span class="p">()}</span>

        <span class="k">if</span> <span class="n">weight_sync_schemes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use new weight synchronization system</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight_sync_schemes</span> <span class="o">=</span> <span class="n">weight_sync_schemes</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight_senders</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="c1"># Set up weight senders now that remote collectors exist</span>
            <span class="k">for</span> <span class="n">model_id</span><span class="p">,</span> <span class="n">scheme</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_sync_schemes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">sender</span> <span class="o">=</span> <span class="n">scheme</span><span class="o">.</span><span class="n">create_sender</span><span class="p">()</span>
                <span class="n">sender</span><span class="o">.</span><span class="n">_model_id</span> <span class="o">=</span> <span class="n">model_id</span>

                <span class="c1"># Register each remote collector as a separate worker</span>
                <span class="c1"># This follows the same pattern as multiprocess collectors</span>
                <span class="k">for</span> <span class="n">worker_idx</span><span class="p">,</span> <span class="n">remote_collector</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="p">):</span>
                    <span class="c1"># Create a transport for this specific collector</span>
                    <span class="c1"># Pass the collector as context so the transport knows which one to talk to</span>
                    <span class="n">sender</span><span class="o">.</span><span class="n">register_worker</span><span class="p">(</span><span class="n">worker_idx</span><span class="p">,</span> <span class="n">remote_collector</span><span class="p">)</span>

                <span class="c1"># Set context and register model</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">sender</span><span class="p">,</span> <span class="s2">&quot;set_context&quot;</span><span class="p">):</span>
                    <span class="n">sender</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_id</span><span class="p">)</span>

                <span class="c1"># Store reference to source model for automatic extraction</span>
                <span class="k">if</span> <span class="n">model_id</span> <span class="o">==</span> <span class="s2">&quot;policy&quot;</span><span class="p">:</span>
                    <span class="n">sender</span><span class="o">.</span><span class="n">_source_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_weight_senders</span><span class="p">[</span><span class="n">model_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">sender</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">weight_updater</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Don&#39;t use legacy system</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Fall back to legacy weight updater system</span>
            <span class="k">if</span> <span class="n">weight_updater</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">weight_updater</span> <span class="o">=</span> <span class="n">RayWeightUpdater</span><span class="p">(</span>
                    <span class="n">policy_weights</span><span class="o">=</span><span class="n">policy_weights</span><span class="p">,</span>
                    <span class="n">remote_collectors</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="p">,</span>
                    <span class="n">max_interval</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_weight_update_interval</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight_updater</span> <span class="o">=</span> <span class="n">weight_updater</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight_sync_schemes</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight_senders</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Print info of all remote workers (fire and forget - no need to wait)</span>
        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="p">:</span>
            <span class="n">e</span><span class="o">.</span><span class="n">print_remote_collector_info</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_extract_weights_if_needed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">model_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract weights from a model if needed.</span>

<span class="sd">        For Ray collectors, when weights is None and we have a weight sync scheme,</span>
<span class="sd">        extract fresh weights from the tracked policy model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">scheme</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weight_sync_schemes</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_sync_schemes</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scheme</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Extract fresh weights from the source model</span>
            <span class="n">sender</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_senders</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">sender</span>
                <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">sender</span><span class="p">,</span> <span class="s2">&quot;_source_model&quot;</span><span class="p">)</span>
                <span class="ow">and</span> <span class="n">sender</span><span class="o">.</span><span class="n">_source_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">):</span>
                <span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.weight_update.weight_sync_schemes</span><span class="w"> </span><span class="kn">import</span> <span class="n">WeightStrategy</span>

                <span class="n">strategy</span> <span class="o">=</span> <span class="n">WeightStrategy</span><span class="p">(</span><span class="n">extract_as</span><span class="o">=</span><span class="n">scheme</span><span class="o">.</span><span class="n">strategy</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">strategy</span><span class="o">.</span><span class="n">extract_weights</span><span class="p">(</span><span class="n">sender</span><span class="o">.</span><span class="n">_source_model</span><span class="p">)</span>

        <span class="c1"># Fall back to base class behavior</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_extract_weights_if_needed</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">model_id</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">num_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_device</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">storing_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storing_device</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">env_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env_device</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">policy_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_policy_device</span>

    <span class="nd">@device</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_device</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span>

    <span class="nd">@storing_device</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">storing_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_storing_device</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_storing_device</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span>

    <span class="nd">@env_device</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">env_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_env_device</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_env_device</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span>

    <span class="nd">@policy_device</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">policy_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_policy_device</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_policy_device</span> <span class="o">=</span> <span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_make_collector</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">env_maker</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">other_params</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a single collector instance.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">other_params</span><span class="p">[</span><span class="s2">&quot;policy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy</span>
        <span class="n">collector</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">env_maker</span><span class="p">,</span>
            <span class="n">total_frames</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="o">**</span><span class="n">other_params</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">collector</span>

<div class="viewcode-block" id="RayCollector.add_collectors"><a class="viewcode-back" href="../../../../reference/generated/torchrl.collectors.distributed.RayCollector.html#torchrl.collectors.distributed.RayCollector.add_collectors">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">add_collectors</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">create_env_fn</span><span class="p">,</span>
        <span class="n">num_envs</span><span class="p">,</span>
        <span class="n">policy</span><span class="p">,</span>
        <span class="n">collector_kwargs</span><span class="p">,</span>
        <span class="n">remote_configs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates and adds a number of remote collectors to the set.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">env_maker</span><span class="p">,</span> <span class="n">other_params</span><span class="p">,</span> <span class="n">remote_config</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">create_env_fn</span><span class="p">,</span> <span class="n">collector_kwargs</span><span class="p">,</span> <span class="n">remote_configs</span>
        <span class="p">):</span>
            <span class="bp">cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector_class</span><span class="o">.</span><span class="n">as_remote</span><span class="p">(</span><span class="n">remote_config</span><span class="p">)</span><span class="o">.</span><span class="n">remote</span>
            <span class="n">collector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_collector</span><span class="p">(</span>
                <span class="bp">cls</span><span class="p">,</span>
                <span class="n">env_maker</span><span class="o">=</span><span class="p">[</span><span class="n">env_maker</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_envs</span>
                <span class="k">if</span> <span class="n">num_envs</span> <span class="o">&gt;</span> <span class="mi">1</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">collector_class</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">collector_class</span><span class="p">,</span> <span class="n">SyncDataCollector</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="k">else</span> <span class="n">env_maker</span><span class="p">,</span>
                <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
                <span class="n">other_params</span><span class="o">=</span><span class="n">other_params</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_remote_collectors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">collector</span><span class="p">)</span></div>

<div class="viewcode-block" id="RayCollector.local_policy"><a class="viewcode-back" href="../../../../reference/generated/torchrl.collectors.distributed.RayCollector.html#torchrl.collectors.distributed.RayCollector.local_policy">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">local_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns local collector.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_policy</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">remote_collectors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns list of remote collectors.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remote_collectors</span>

<div class="viewcode-block" id="RayCollector.stop_remote_collectors"><a class="viewcode-back" href="../../../../reference/generated/torchrl.collectors.distributed.RayCollector.html#torchrl.collectors.distributed.RayCollector.stop_remote_collectors">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">stop_remote_collectors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Stops all remote collectors.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_remote_collectors</span><span class="p">)):</span>
            <span class="n">collector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
            <span class="c1"># collector.__ray_terminate__.remote()  # This will kill the actor but let pending tasks finish</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">kill</span><span class="p">(</span>
                <span class="n">collector</span>
            <span class="p">)</span>  <span class="c1"># This will interrupt any running tasks on the actor, causing them to fail immediately</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">iterator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">proc</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="c1"># When using RayReplayBuffer, sub-collectors write directly to buffer</span>
            <span class="c1"># and return None, so skip processing</span>
            <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_trajs</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">split_trajectories</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">postproc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">postproc</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">data</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync</span><span class="p">:</span>
            <span class="n">meth</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_iterator</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">meth</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_async_iterator</span>
        <span class="k">yield from</span> <span class="p">(</span><span class="n">proc</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">meth</span><span class="p">())</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_asyncio_iterator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">proc</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="c1"># When using RayReplayBuffer, sub-collectors write directly to buffer</span>
            <span class="c1"># and return None, so skip processing</span>
            <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_trajs</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">split_trajectories</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">postproc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">postproc</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">data</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_iterator</span><span class="p">():</span>
                <span class="k">yield</span> <span class="n">proc</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_async_iterator</span><span class="p">():</span>
                <span class="k">yield</span> <span class="n">proc</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_sync_iterator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TensorDictBase</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Collects one data batch per remote collector in each iteration.&quot;&quot;&quot;</span>
        <span class="k">while</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_frames</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stop_event</span><span class="o">.</span><span class="n">is_set</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_after_each_batch</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_weight_update_interval</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Updating weights on all workers&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_policy_weights_</span><span class="p">()</span>

            <span class="c1"># Ask for batches to all remote workers.</span>
            <span class="n">pending_tasks</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span><span class="o">.</span><span class="n">next</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="p">]</span>

            <span class="c1"># Wait for all rollouts</span>
            <span class="n">samples_ready</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">samples_ready</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_collectors</span><span class="p">:</span>
                <span class="n">samples_ready</span><span class="p">,</span> <span class="n">samples_not_ready</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span>
                    <span class="n">pending_tasks</span><span class="p">,</span> <span class="n">num_returns</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">pending_tasks</span><span class="p">)</span>
                <span class="p">)</span>

            <span class="c1"># Retrieve and concatenate Tensordicts</span>
            <span class="n">out_td</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">pending_tasks</span><span class="p">:</span>
                <span class="n">rollouts</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
                <span class="n">ray</span><span class="o">.</span><span class="n">internal</span><span class="o">.</span><span class="n">free</span><span class="p">(</span>
                    <span class="n">r</span>
                <span class="p">)</span>  <span class="c1"># should not be necessary, deleted automatically when ref count is down to 0</span>
                <span class="n">out_td</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rollouts</span><span class="p">)</span>

            <span class="c1"># Handle case where replay_buffer is used and rollouts are None</span>
            <span class="k">if</span> <span class="n">out_td</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Sub-collectors are writing directly to RayReplayBuffer</span>
                <span class="c1"># Track frames and yield None to signal completion</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">frames_per_batch</span>
                <span class="k">yield</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Normal case: concatenate and yield rollouts</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rollouts</span><span class="o">.</span><span class="n">batch_size</span><span class="p">):</span>
                    <span class="n">out_td</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">out_td</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">out_td</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">out_td</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">+=</span> <span class="n">out_td</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="k">yield</span> <span class="n">out_td</span>

        <span class="c1"># Only auto-shutdown if not running in a background thread.</span>
        <span class="c1"># When using replay buffer, users should explicitly manage shutdown order.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span><span class="p">(</span><span class="n">shutdown_ray</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_run_collection_loop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs the collection loop in a background thread.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">iterator</span><span class="p">():</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stop_event</span><span class="o">.</span><span class="n">is_set</span><span class="p">():</span>
                    <span class="k">break</span>
                <span class="c1"># When RayReplayBuffer is configured, sub-collectors write directly</span>
                <span class="c1"># to the buffer and data will be None. Otherwise, data contains rollouts.</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error in collection thread: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span>

<div class="viewcode-block" id="RayCollector.start"><a class="viewcode-back" href="../../../../reference/generated/torchrl.collectors.distributed.RayCollector.html#torchrl.collectors.distributed.RayCollector.start">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Starts the RayCollector in a background thread.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">replay_buffer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Replay buffer must be defined for background execution.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_stop_event</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span>
                <span class="n">target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_run_collection_loop</span><span class="p">,</span> <span class="n">daemon</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span></div>

<div class="viewcode-block" id="RayCollector.async_shutdown"><a class="viewcode-back" href="../../../../reference/generated/torchrl.collectors.distributed.RayCollector.html#torchrl.collectors.distributed.RayCollector.async_shutdown">[docs]</a>    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">async_shutdown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shutdown_ray</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Finishes processes started by the collector during async execution.</span>

<span class="sd">        Args:</span>
<span class="sd">            shutdown_ray (bool): If True, also shutdown the Ray cluster. Defaults to False.</span>
<span class="sd">                Note: Setting this to True will kill all Ray actors in the cluster, including</span>
<span class="sd">                any replay buffers or other services. Only set to True if you&#39;re sure you want</span>
<span class="sd">                to shut down the entire Ray cluster.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stop_event</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop_remote_collectors</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">shutdown_ray</span><span class="p">:</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_async_iterator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="n">TensorDictBase</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Collects a data batch from a single remote collector in each iteration.&quot;&quot;&quot;</span>
        <span class="n">pending_tasks</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">collector</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="p">):</span>
            <span class="n">future</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">next</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span>
            <span class="n">pending_tasks</span><span class="p">[</span><span class="n">future</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>

        <span class="k">while</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_frames</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stop_event</span><span class="o">.</span><span class="n">is_set</span><span class="p">()</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">pending_tasks</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Missing pending tasks, something went wrong&quot;</span><span class="p">)</span>

            <span class="c1"># Wait for first worker to finish</span>
            <span class="n">wait_results</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">pending_tasks</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
            <span class="n">future</span> <span class="o">=</span> <span class="n">wait_results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">collector_index</span> <span class="o">=</span> <span class="n">pending_tasks</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>
            <span class="n">collector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="p">[</span><span class="n">collector_index</span><span class="p">]</span>

            <span class="c1"># Retrieve single rollouts</span>
            <span class="n">out_td</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">internal</span><span class="o">.</span><span class="n">free</span><span class="p">(</span>
                <span class="p">[</span><span class="n">future</span><span class="p">]</span>
            <span class="p">)</span>  <span class="c1"># should not be necessary, deleted automatically when ref count is down to 0</span>

            <span class="c1"># Track collected frames - use frames_per_batch since out_td might be None</span>
            <span class="c1"># when using RayReplayBuffer (sub-collectors write directly to buffer)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">collected_frames</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">frames_per_batch</span>

            <span class="k">yield</span> <span class="n">out_td</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_after_each_batch</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_weight_update_interval</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Updating weights on worker </span><span class="si">{</span><span class="n">collector_index</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update_policy_weights_</span><span class="p">(</span><span class="n">worker_ids</span><span class="o">=</span><span class="n">collector_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Schedule a new collection task</span>
            <span class="n">future</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">next</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span>
            <span class="n">pending_tasks</span><span class="p">[</span><span class="n">future</span><span class="p">]</span> <span class="o">=</span> <span class="n">collector_index</span>

        <span class="c1"># Wait for the in-process collections tasks to finish.</span>
        <span class="n">refs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pending_tasks</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">ray</span><span class="o">.</span><span class="n">wait</span><span class="p">(</span><span class="n">refs</span><span class="p">,</span> <span class="n">num_returns</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">refs</span><span class="p">))</span>

        <span class="c1"># Cancel the in-process collections tasks</span>
        <span class="c1"># for ref in refs:</span>
        <span class="c1">#     ray.cancel(</span>
        <span class="c1">#         object_ref=ref,</span>
        <span class="c1">#         force=False,</span>
        <span class="c1">#     )</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>

<div class="viewcode-block" id="RayCollector.set_seed"><a class="viewcode-back" href="../../../../reference/generated/torchrl.collectors.distributed.RayCollector.html#torchrl.collectors.distributed.RayCollector.set_seed">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">static_seed</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calls parent method for each remote collector iteratively and returns final seed.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">collector</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="p">:</span>
            <span class="n">seed</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">object_refs</span><span class="o">=</span><span class="n">collector</span><span class="o">.</span><span class="n">set_seed</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">static_seed</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">seed</span></div>

<div class="viewcode-block" id="RayCollector.state_dict"><a class="viewcode-back" href="../../../../reference/generated/torchrl.collectors.distributed.RayCollector.html#torchrl.collectors.distributed.RayCollector.state_dict">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">OrderedDict</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calls parent method for each remote collector and returns a list of results.&quot;&quot;&quot;</span>
        <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">collector</span><span class="o">.</span><span class="n">state_dict</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">collector</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span>
        <span class="p">]</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">object_refs</span><span class="o">=</span><span class="n">futures</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">results</span></div>

<div class="viewcode-block" id="RayCollector.load_state_dict"><a class="viewcode-back" href="../../../../reference/generated/torchrl.collectors.distributed.RayCollector.html#torchrl.collectors.distributed.RayCollector.load_state_dict">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="n">OrderedDict</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">OrderedDict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calls parent method for each remote collector.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="n">OrderedDict</span><span class="p">):</span>
            <span class="n">state_dicts</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_dict</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">state_dicts</span> <span class="o">=</span> <span class="n">state_dict</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">collector</span><span class="p">,</span> <span class="n">state_dict</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">remote_collectors</span><span class="p">,</span> <span class="n">state_dicts</span><span class="p">):</span>
            <span class="n">collector</span><span class="o">.</span><span class="n">load_state_dict</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span></div>

<div class="viewcode-block" id="RayCollector.shutdown"><a class="viewcode-back" href="../../../../reference/generated/torchrl.collectors.distributed.RayCollector.html#torchrl.collectors.distributed.RayCollector.shutdown">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">shutdown</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">shutdown_ray</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Finishes processes started by the collector.</span>

<span class="sd">        Args:</span>
<span class="sd">            timeout (float, optional): Timeout for stopping the collection thread.</span>
<span class="sd">            shutdown_ray (bool): If True, also shutdown the Ray cluster. Defaults to False.</span>
<span class="sd">                Note: Setting this to True will kill all Ray actors in the cluster, including</span>
<span class="sd">                any replay buffers or other services. Only set to True if you&#39;re sure you want</span>
<span class="sd">                to shut down the entire Ray cluster.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_stop_event</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span><span class="o">.</span><span class="n">is_alive</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_collection_thread</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span> <span class="k">if</span> <span class="n">timeout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mf">5.0</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop_remote_collectors</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">shutdown_ray</span><span class="p">:</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span></div>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="n">string</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">()&quot;</span>
        <span class="k">return</span> <span class="n">string</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>