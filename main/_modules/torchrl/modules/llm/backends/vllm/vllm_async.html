


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchrl.modules.llm.backends.vllm.vllm_async &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../../../../../../versions.html"><span style="font-size:110%">main (0.0.0+unknown) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/getting-started-1.html">Get started with TorchRLâ€™s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/export.html">Exporting TorchRL modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/llm_browser.html">TorchRL LLM: Building Tool-Enabled Environments</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../../../index.html">Module code</a> &gt;</li>
        
      <li>torchrl.modules.llm.backends.vllm.vllm_async</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
    
    
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchrl.modules.llm.backends.vllm.vllm_async</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the MIT license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="sd">&quot;&quot;&quot;Async vLLM engine implementation for efficient batching and inference.</span>

<span class="sd">This module provides an async vLLM engine that leverages native vLLM batching</span>
<span class="sd">for better performance and memory efficiency compared to the explicit batching</span>
<span class="sd">approach used in the legacy vLLM backend.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">uuid</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Iterator</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">concurrent.futures</span><span class="w"> </span><span class="kn">import</span> <span class="n">ThreadPoolExecutor</span><span class="p">,</span> <span class="n">wait</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl._utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span> <span class="k">as</span> <span class="n">torchrl_logger</span>

<span class="c1"># Import RLvLLMEngine and shared utilities</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">RLvLLMEngine</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.vllm_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">stateless_init_process_group</span>


<span class="n">_has_vllm</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">vllm.engine.async_llm_engine</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncEngineArgs</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">vllm.engine.request</span><span class="w"> </span><span class="kn">import</span> <span class="n">RequestOutput</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">vllm.engine.sampling_params</span><span class="w"> </span><span class="kn">import</span> <span class="n">SamplingParams</span>

<span class="n">TIMEOUT_SECONDS</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TORCHRL_VLLM_TIMEOUT_SECONDS&quot;</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">vllm</span>

    <span class="n">_has_vllm</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">vllm</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_has_vllm</span> <span class="o">=</span> <span class="kc">False</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_get_ray</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Import Ray on demand to avoid global import side-effects.</span>

<span class="sd">    Returns:</span>
<span class="sd">        ModuleType: The imported Ray module.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ImportError: If Ray is not installed.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">ray</span>  <span class="c1"># type: ignore</span>

        <span class="k">return</span> <span class="n">ray</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>  <span class="c1"># pragma: no cover - surfaced to callers</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
            <span class="s2">&quot;ray is not installed. Please install it with `pip install ray`.&quot;</span>
        <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_AsyncvLLMWorker</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Async vLLM worker extension for Ray with weight update capabilities.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init_weight_update_group</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">master_address</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">master_port</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">rank_offset</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize weight update group for this worker (non-blocking).</span>

<span class="sd">        This method starts NCCL initialization in a background thread and returns immediately,</span>
<span class="sd">        allowing the RPC to complete. The NCCL collective will complete when the trainer joins.</span>

<span class="sd">        Args:</span>
<span class="sd">            master_address (str): The master address for distributed training.</span>
<span class="sd">            master_port (str): The master port for distributed training.</span>
<span class="sd">            rank_offset (int): Rank offset for this worker in the global weight update group.</span>
<span class="sd">            world_size (int): Total number of processes in the weight update group.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">threading</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">vllm.distributed.parallel_state</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_world_group</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;=&gt; in </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.init_weight_update_group&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;model_update_group&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Model update group already initialized&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># Get the local rank within the tensor parallel group</span>
        <span class="n">tp_group</span> <span class="o">=</span> <span class="n">get_world_group</span><span class="p">()</span>
        <span class="n">local_rank</span> <span class="o">=</span> <span class="n">tp_group</span><span class="o">.</span><span class="n">rank</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Local rank in tensor parallel group: </span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Calculate the global rank for weight update group</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">local_rank</span> <span class="o">+</span> <span class="n">rank_offset</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Starting </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> weight update group init (non-blocking) with &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">master_address</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">master_port</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">rank</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">world_size</span><span class="si">=}</span><span class="s2">, device=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Start NCCL init in a background thread so this RPC can return immediately</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">_init_nccl_background</span><span class="p">():</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="kn">from</span><span class="w"> </span><span class="nn">.vllm_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">stateless_init_process_group</span>

                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Worker rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: Starting NCCL init (will block until collective completes)...&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_update_group</span> <span class="o">=</span> <span class="n">stateless_init_process_group</span><span class="p">(</span>
                    <span class="n">master_address</span><span class="p">,</span> <span class="n">master_port</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
                <span class="p">)</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Worker rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: NCCL init complete!&quot;</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Worker rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">: NCCL init failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">raise</span>

        <span class="n">thread</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">_init_nccl_background</span><span class="p">,</span> <span class="n">daemon</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

        <span class="c1"># Store thread reference for potential cleanup</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_nccl_init_thread</span> <span class="o">=</span> <span class="n">thread</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.init_weight_update_group dispatched (non-blocking)&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dtype_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update weight via broadcast from master (rank 0) - periodic-mono pattern.</span>

<span class="sd">        Args:</span>
<span class="sd">            name (str): Parameter name.</span>
<span class="sd">            dtype_name (str): Parameter dtype name (e.g., &#39;bfloat16&#39;).</span>
<span class="sd">            shape (tuple[int, ...]): Parameter shape.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_update_group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Weight update group not initialized&quot;</span><span class="p">)</span>

        <span class="c1"># Convert dtype name to dtype (like periodic-mono)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">dtype_name</span><span class="p">)</span>

        <span class="c1"># Workers receive broadcast from master (rank 0)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_update_group</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span>
            <span class="n">weight</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_runner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="p">[(</span><span class="n">name</span><span class="p">,</span> <span class="n">weight</span><span class="p">)])</span>
        <span class="k">del</span> <span class="n">weight</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_nccl_group_ready</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if NCCL group is ready for communication.&quot;&quot;&quot;</span>
        <span class="n">ready</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_update_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Worker NCCL group ready: </span><span class="si">{</span><span class="n">ready</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ready</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">load_weights_from_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">num_threads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load weights from shared storage (double-buffer approach).</span>

<span class="sd">        This method reads weights from a memory-mapped TensorDict directory</span>
<span class="sd">        and loads them into the model. Used for file-based weight synchronization</span>
<span class="sd">        as an alternative to NCCL collectives.</span>

<span class="sd">        Args:</span>
<span class="sd">            storage_path: Path to the directory containing memory-mapped weights</span>
<span class="sd">            num_threads: Number of threads for reading (default: 1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">tensordict</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDict</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Worker loading weights from </span><span class="si">{</span><span class="n">storage_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Read weights from shared storage</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="o">.</span><span class="n">load_memmap</span><span class="p">(</span><span class="n">storage_path</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">flatten_keys</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>

        <span class="c1"># Convert to list of (name, tensor) tuples</span>
        <span class="n">weights_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Worker loading </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">weights_list</span><span class="p">)</span><span class="si">}</span><span class="s2"> weights into model&quot;</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="n">num_threads</span><span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
            <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_runner</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">weights</span> <span class="ow">in</span> <span class="n">weights_list</span>
            <span class="p">]</span>
            <span class="n">wait</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Worker successfully loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">weights_list</span><span class="p">)</span><span class="si">}</span><span class="s2"> weights from storage&quot;</span>
        <span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_AsyncLLMEngine</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Extended AsyncLLMEngine with TorchRL-specific features.</span>

<span class="sd">    This class wraps vLLM&#39;s AsyncLLMEngine and adds functionality needed</span>
<span class="sd">    for TorchRL integration, including weight updates and batch management.</span>

<span class="sd">    This is a private class and should not be used directly. Use the ray remote actor class :class:`AsyncLLMEngineActor` instead.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        engine_args (AsyncEngineArgs): Arguments for creating the AsyncLLMEngine instances.</span>
<span class="sd">        bundle_indices (list[int], optional): Bundle indices for the engine.</span>
<span class="sd">        enable_prefix_caching (bool, optional): Whether to enable prefix caching.</span>

<span class="sd">            .. warning::</span>
<span class="sd">                enable_prefix_caching is set to False by default, which is recommended if prompt log probs are needed.</span>
<span class="sd">                Set it to True if prompt log probs are not needed.</span>
<span class="sd">                See `this issue &lt;https://github.com/vllm-project/vllm/issues/8268&gt;`_ for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">engine_args</span><span class="p">:</span> <span class="n">AsyncEngineArgs</span><span class="p">,</span>
        <span class="n">bundle_indices</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_prefix_caching</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_vllm</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                <span class="s2">&quot;vllm is not installed. Please install it with `pip install vllm`.&quot;</span>
            <span class="p">)</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncLLMEngine</span>

        <span class="k">if</span> <span class="n">bundle_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;VLLM_RAY_BUNDLE_INDICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">bundle_indices</span><span class="p">))</span>

        <span class="n">engine_args</span><span class="o">.</span><span class="n">enable_prefix_caching</span> <span class="o">=</span> <span class="n">enable_prefix_caching</span>

        <span class="c1"># Create the engine directly - this is the source of the blocking ray.get issue</span>
        <span class="c1"># but we need to handle it differently for multiple replicas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">engine</span> <span class="o">=</span> <span class="n">AsyncLLMEngine</span><span class="o">.</span><span class="n">from_engine_args</span><span class="p">(</span><span class="n">engine_args</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bundle_indices</span> <span class="o">=</span> <span class="n">bundle_indices</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">ready</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if engine is ready for inference.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_tqdm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_adapter_request</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">guided_options_request</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timeout_seconds</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RequestOutput</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text with the same interface as vLLM.LLM.generate.</span>

<span class="sd">        This method mirrors the interface of vLLM.LLM.generate to provide seamless</span>
<span class="sd">        compatibility between sync and async engines.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompts: String, TokensPrompt, or list of these. Input prompts for generation.</span>
<span class="sd">            sampling_params: SamplingParams object for controlling generation behavior.</span>
<span class="sd">            prompt_token_ids: Alternative to prompts - token IDs for generation.</span>
<span class="sd">            use_tqdm: Whether to show progress bar (not used in async engine).</span>
<span class="sd">            lora_request: LoRA request for adapter-based generation.</span>
<span class="sd">            guided_options_request: Guided decoding options.</span>
<span class="sd">            timeout_seconds: Timeout for generation in seconds.</span>

<span class="sd">        Returns:</span>
<span class="sd">            RequestOutput or list of RequestOutput: Generated outputs from vLLM.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_vllm</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                <span class="s2">&quot;vllm is not installed. Please install it with `pip install vllm`.&quot;</span>
            <span class="p">)</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SamplingParams</span><span class="p">,</span> <span class="n">TokensPrompt</span>

        <span class="c1"># Track whether input was originally a single prompt</span>
        <span class="n">single_prompt_input</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Handle prompt_token_ids if provided</span>
        <span class="k">if</span> <span class="n">prompt_token_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">prompts</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot specify both prompts and prompt_token_ids&quot;</span><span class="p">)</span>

            <span class="c1"># Convert token IDs to TokensPrompt objects</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">prompt_token_ids</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;prompt_token_ids cannot be empty&quot;</span><span class="p">)</span>

            <span class="c1"># Check if it&#39;s a list of lists or a single list</span>
            <span class="k">if</span> <span class="n">prompt_token_ids</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
                <span class="c1"># List of token ID lists</span>
                <span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">TokensPrompt</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="o">=</span><span class="n">tokens</span><span class="p">)</span> <span class="k">for</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="n">prompt_token_ids</span>
                <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Single token ID list - cast to ensure type compatibility</span>
                <span class="n">token_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span> <span class="k">if</span> <span class="n">prompt_token_ids</span> <span class="k">else</span> <span class="p">[]</span>
                <span class="n">prompts</span> <span class="o">=</span> <span class="n">TokensPrompt</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="o">=</span><span class="n">token_list</span><span class="p">)</span>
                <span class="n">single_prompt_input</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">elif</span> <span class="n">prompts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must specify either prompts or prompt_token_ids&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># prompts was provided directly</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="n">single_prompt_input</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Default sampling params if not provided</span>
        <span class="k">if</span> <span class="n">sampling_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">()</span>

        <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_gen_one</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RequestOutput</span><span class="p">:</span>
            <span class="n">request_id</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())</span>
            <span class="n">final</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Build kwargs for engine.generate</span>
            <span class="n">gen_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
                <span class="s2">&quot;sampling_params&quot;</span><span class="p">:</span> <span class="n">sampling_params</span><span class="p">,</span>
                <span class="s2">&quot;request_id&quot;</span><span class="p">:</span> <span class="n">request_id</span><span class="p">,</span>
            <span class="p">}</span>

            <span class="c1"># Add optional parameters if provided</span>
            <span class="k">if</span> <span class="n">lora_request</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">gen_kwargs</span><span class="p">[</span><span class="s2">&quot;lora_request&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lora_request</span>
            <span class="k">if</span> <span class="n">prompt_adapter_request</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">gen_kwargs</span><span class="p">[</span><span class="s2">&quot;prompt_adapter_request&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">prompt_adapter_request</span>
            <span class="k">if</span> <span class="n">guided_options_request</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">gen_kwargs</span><span class="p">[</span><span class="s2">&quot;guided_options_request&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">guided_options_request</span>

            <span class="k">async</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">gen_kwargs</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
                    <span class="n">final</span> <span class="o">=</span> <span class="n">output</span>
            <span class="k">assert</span> <span class="n">final</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">return</span> <span class="n">final</span>

        <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">_run_generation</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">single_prompt_input</span><span class="p">:</span>
                <span class="k">return</span> <span class="k">await</span> <span class="n">_gen_one</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>

            <span class="c1"># List of prompts: run concurrently</span>
            <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="n">asyncio</span><span class="o">.</span><span class="n">create_task</span><span class="p">(</span><span class="n">_gen_one</span><span class="p">(</span><span class="n">p</span><span class="p">))</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">]</span>
            <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">results</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">timeout_seconds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">timeout_seconds</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">wait_for</span><span class="p">(</span>
                    <span class="n">_run_generation</span><span class="p">(),</span> <span class="n">timeout</span><span class="o">=</span><span class="n">timeout_seconds</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="k">await</span> <span class="n">_run_generation</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">TimeoutError</span><span class="p">:</span>
            <span class="c1"># Best-effort cleanup</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">abort_fn</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="p">,</span> <span class="s2">&quot;abort&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">callable</span><span class="p">(</span><span class="n">abort_fn</span><span class="p">):</span>
                    <span class="c1"># We can&#39;t easily track all request IDs, so this is best-effort</span>
                    <span class="k">pass</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="k">raise</span> <span class="ne">TimeoutError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;vLLM generation timed out after </span><span class="si">{</span><span class="n">timeout_seconds</span><span class="si">}</span><span class="s2"> seconds&quot;</span>
            <span class="p">)</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">get_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the tokenizer from the engine.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">get_tokenizer</span><span class="p">()</span>

    <span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">collective_rpc_v1</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">timeout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(),</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform a collective RPC call to the given method (vLLM V1).</span>

<span class="sd">        Args:</span>
<span class="sd">            method (str): Method name to call.</span>
<span class="sd">            timeout (float | None): Timeout for the RPC call.</span>
<span class="sd">            args (tuple): Arguments to pass to the method.</span>
<span class="sd">            kwargs (dict | None): Keyword arguments to pass to the method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">envs</span>

        <span class="k">if</span> <span class="n">envs</span> <span class="ow">and</span> <span class="n">envs</span><span class="o">.</span><span class="n">VLLM_USE_V1</span><span class="p">:</span>
            <span class="k">return</span> <span class="k">await</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">timeout</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">timeout</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">collective_rpc_v0</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">timeout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(),</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform a collective RPC call to the given method (vLLM V0).</span>

<span class="sd">        Args:</span>
<span class="sd">            method (str): Method name to call.</span>
<span class="sd">            timeout (float | None): Timeout for the RPC call.</span>
<span class="sd">            args (tuple): Arguments to pass to the method.</span>
<span class="sd">            kwargs (dict | None): Keyword arguments to pass to the method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">timeout</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_num_unfinished_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the number of unfinished requests in the engine.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: Number of unfinished requests.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Try to access the method directly if available</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="p">,</span> <span class="s2">&quot;get_num_unfinished_requests&quot;</span><span class="p">):</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">get_num_unfinished_requests</span><span class="p">()</span>
            <span class="c1"># Fallback to accessing through engine.engine for v0</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="p">,</span> <span class="s2">&quot;engine&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">engine</span><span class="p">,</span> <span class="s2">&quot;get_num_unfinished_requests&quot;</span>
            <span class="p">):</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">get_num_unfinished_requests</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># If method not available, return 0 as fallback</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;get_num_unfinished_requests not available, returning 0&quot;</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="mi">0</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error getting unfinished requests count: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_cache_usage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the KV cache usage as a fraction between 0 and 1.</span>

<span class="sd">        Returns:</span>
<span class="sd">            float: Cache usage fraction (0.0 = empty, 1.0 = full).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Try to get cache usage from the engine</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="p">,</span> <span class="s2">&quot;engine&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">engine</span><span class="p">,</span> <span class="s2">&quot;cache_config&quot;</span>
            <span class="p">):</span>
                <span class="c1"># Access the LLM engine&#39;s cache information</span>
                <span class="n">cache_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">cache_config</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">cache_config</span><span class="p">,</span> <span class="s2">&quot;cache_usage&quot;</span><span class="p">):</span>
                    <span class="k">return</span> <span class="n">cache_config</span><span class="o">.</span><span class="n">cache_usage</span>
                <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">engine</span><span class="p">,</span> <span class="s2">&quot;scheduler&quot;</span><span class="p">):</span>
                    <span class="c1"># Try to get usage from the scheduler</span>
                    <span class="n">scheduler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">scheduler</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">scheduler</span><span class="p">,</span> <span class="s2">&quot;get_num_free_gpu_blocks&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span>
                        <span class="n">scheduler</span><span class="p">,</span> <span class="s2">&quot;get_num_total_gpu_blocks&quot;</span>
                    <span class="p">):</span>
                        <span class="n">free_blocks</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_num_free_gpu_blocks</span><span class="p">()</span>
                        <span class="n">total_blocks</span> <span class="o">=</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_num_total_gpu_blocks</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">total_blocks</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="n">free_blocks</span> <span class="o">/</span> <span class="n">total_blocks</span><span class="p">)</span>
            <span class="c1"># Fallback: return a random value for now (this should be replaced with actual metrics)</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Cache usage metrics not available, returning random value&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.5</span>
            <span class="p">)</span>  <span class="c1"># Return a value between 0 and 0.5 to simulate partial usage</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error getting cache usage: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="mf">0.0</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_gpus_per_replica</span><span class="p">(</span><span class="n">engine_args</span><span class="p">:</span> <span class="n">AsyncEngineArgs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the number of GPUs per replica for the given engine args.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">engine_args</span><span class="o">.</span><span class="n">tensor_parallel_size</span>
        <span class="o">*</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">engine_args</span><span class="p">,</span> <span class="s2">&quot;data_parallel_size&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Default to 1 if not present</span>
        <span class="o">*</span> <span class="nb">getattr</span><span class="p">(</span>
            <span class="n">engine_args</span><span class="p">,</span> <span class="s2">&quot;pipeline_parallel_size&quot;</span><span class="p">,</span> <span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># Default to 1 if not present</span>
    <span class="p">)</span>


<span class="c1"># Ray actor wrapper is created lazily in __init__ to avoid global Ray import.</span>


<div class="viewcode-block" id="AsyncVLLM"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">AsyncVLLM</span><span class="p">(</span><span class="n">RLvLLMEngine</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A service that manages multiple async vLLM engine actors for distributed inference.</span>

<span class="sd">    This is the main entry point for async vLLM inference in TorchRL. It manages multiple</span>
<span class="sd">    vLLM engine replicas running as Ray actors, providing load balancing, weight updates,</span>
<span class="sd">    and a unified interface for text generation.</span>

<span class="sd">    The service automatically handles Ray actor lifecycle management, GPU allocation through</span>
<span class="sd">    placement groups, and provides both synchronous and asynchronous generation interfaces</span>
<span class="sd">    that are compatible with the standard vLLM API.</span>

<span class="sd">    Args:</span>
<span class="sd">        engine_args (AsyncEngineArgs): Configuration for the vLLM engines.</span>
<span class="sd">        num_replicas (int, optional): Number of engine replicas to create. Defaults to 1.</span>
<span class="sd">        actor_class (optional): Custom Ray actor class. Defaults to the internal actor implementation.</span>
<span class="sd">        enable_prefix_caching (bool, optional): Whether to enable prefix caching. Defaults to False.</span>

<span class="sd">            .. warning::</span>
<span class="sd">                enable_prefix_caching is set to False by default, which is recommended if prompt log probs are needed.</span>
<span class="sd">                Set it to True if prompt log probs are not needed.</span>
<span class="sd">                See `this issue &lt;https://github.com/vllm-project/vllm/issues/8268&gt;`_ for more details.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.modules.llm import AsyncVLLM</span>
<span class="sd">        &gt;&gt;&gt; from vllm import SamplingParams</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Simple usage - single GPU, single replica</span>
<span class="sd">        &gt;&gt;&gt; service = AsyncVLLM.from_pretrained(&quot;Qwen/Qwen2.5-3B&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Advanced usage - multi-GPU tensor parallel with multiple replicas</span>
<span class="sd">        &gt;&gt;&gt; service = AsyncVLLM.from_pretrained(</span>
<span class="sd">        ...     &quot;Qwen/Qwen2.5-7B&quot;,</span>
<span class="sd">        ...     num_devices=2,  # Use 2 GPUs for tensor parallelism</span>
<span class="sd">        ...     num_replicas=2,  # Create 2 replicas for higher throughput</span>
<span class="sd">        ...     max_model_len=4096</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Generate text</span>
<span class="sd">        &gt;&gt;&gt; sampling_params = SamplingParams(temperature=0.7, max_tokens=100)</span>
<span class="sd">        &gt;&gt;&gt; result = service.generate(&quot;Hello, world!&quot;, sampling_params)</span>
<span class="sd">        &gt;&gt;&gt; print(result.outputs[0].text)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Alternative: using AsyncEngineArgs directly for advanced configuration</span>
<span class="sd">        &gt;&gt;&gt; from vllm import AsyncEngineArgs</span>
<span class="sd">        &gt;&gt;&gt; engine_args = AsyncEngineArgs(</span>
<span class="sd">        ...     model=&quot;Qwen/Qwen2.5-3B&quot;,</span>
<span class="sd">        ...     tensor_parallel_size=2</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; service = AsyncVLLM.launch(engine_args, num_replicas=2)</span>

<span class="sd">    .. note::</span>
<span class="sd">        **Architecture and Design**</span>

<span class="sd">        The AsyncVLLM service implements a distributed inference architecture with the following key components:</span>

<span class="sd">        1. **Ray Actor Management**: Each replica runs as a separate Ray actor with dedicated GPU resources.</span>
<span class="sd">           The service creates a placement group to ensure optimal GPU allocation and co-location of</span>
<span class="sd">           tensor-parallel workers on the same node when possible.</span>

<span class="sd">        2. **Load Balancing**: Generation requests are distributed across replicas using random selection</span>
<span class="sd">           by default, or can target specific replicas using the `actor_index` parameter.</span>

<span class="sd">        3. **Weight Synchronization**: The service supports weight updates across all replicas through</span>
<span class="sd">           NCCL communication groups, enabling integration with distributed training workflows.</span>

<span class="sd">        4. **Resource Management**: Automatic GPU allocation and cleanup through Ray placement groups,</span>
<span class="sd">           with proper shutdown procedures to prevent resource leaks.</span>

<span class="sd">        5. **API Compatibility**: Provides the same interface as vLLM&#39;s synchronous `LLM.generate()`</span>
<span class="sd">           method, making it a drop-in replacement for async workloads.</span>

<span class="sd">        **Ray Integration**</span>

<span class="sd">        The service leverages Ray&#39;s actor model for distributed execution. Each replica is an independent</span>
<span class="sd">        Ray actor that can be scheduled on different nodes. The service handles actor lifecycle,</span>
<span class="sd">        monitors readiness, and provides centralized access to all replicas.</span>

<span class="sd">        **Performance Considerations**</span>

<span class="sd">        - Prefix caching is enabled by default for better performance with repeated prompts</span>
<span class="sd">        - Tensor parallelism is supported for large models that don&#39;t fit on single GPUs</span>
<span class="sd">        - Multiple replicas allow concurrent processing of different requests</span>
<span class="sd">        - Native vLLM batching is used within each replica for optimal throughput</span>

<span class="sd">        **Error Handling**</span>

<span class="sd">        The service includes timeout support, graceful shutdown procedures, and best-effort</span>
<span class="sd">        request cleanup on failures. Ray&#39;s fault tolerance mechanisms provide additional</span>
<span class="sd">        resilience for long-running inference workloads.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">engine_args</span><span class="p">:</span> <span class="n">AsyncEngineArgs</span><span class="p">,</span>
        <span class="n">num_replicas</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">actor_class</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">enable_prefix_caching</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_vllm</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
                <span class="s2">&quot;vllm is not installed. Please install it with `pip install vllm`.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Lazily import ray only when constructing the actor class to avoid global import</span>

        <span class="c1"># Enable prefix caching by default for better performance</span>
        <span class="n">engine_args</span><span class="o">.</span><span class="n">enable_prefix_caching</span> <span class="o">=</span> <span class="n">enable_prefix_caching</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span> <span class="o">=</span> <span class="n">engine_args</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_replicas</span> <span class="o">=</span> <span class="n">num_replicas</span>
        <span class="k">if</span> <span class="n">actor_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">actor_class</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">num_cpus</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">_AsyncLLMEngine</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">actor_class</span> <span class="o">=</span> <span class="n">actor_class</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_launched</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_service_id</span> <span class="o">=</span> <span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span><span class="p">[</span>
            <span class="p">:</span><span class="mi">8</span>
        <span class="p">]</span>  <span class="c1"># Unique suffix to avoid name collisions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_placement_group</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_load_balancer</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_launch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Launch all actor replicas.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_launched</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;AsyncVLLMEngineService already launched&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># Local imports to avoid global Ray dependency</span>
        <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">ray.util.placement_group</span><span class="w"> </span><span class="kn">import</span> <span class="n">placement_group</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">ray.util.scheduling_strategies</span><span class="w"> </span><span class="kn">import</span> <span class="n">PlacementGroupSchedulingStrategy</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Launching </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_replicas</span><span class="si">}</span><span class="s2"> async vLLM engine actors...&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Create placement groups - one per replica to avoid conflicts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_placement_groups</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Create actor replicas sequentially to avoid race conditions</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_replicas</span><span class="p">):</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Creating async actor replica </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_replicas</span><span class="si">}</span><span class="s2"> ...&quot;</span>
            <span class="p">)</span>

            <span class="c1"># Create individual placement group for this replica</span>
            <span class="n">num_gpus</span> <span class="o">=</span> <span class="n">_gpus_per_replica</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="p">)</span>
            <span class="n">bundles</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_gpus</span><span class="p">)]</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Creating placement group for replica </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">bundles</span><span class="p">)</span><span class="si">}</span><span class="s2"> bundles&quot;</span>
            <span class="p">)</span>

            <span class="n">placement_group_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;vllm-replica-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_service_id</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">pg</span> <span class="o">=</span> <span class="n">placement_group</span><span class="p">(</span><span class="n">bundles</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;PACK&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">placement_group_name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_placement_groups</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Placement group </span><span class="si">{</span><span class="n">placement_group_name</span><span class="si">}</span><span class="s2"> created: </span><span class="si">{</span><span class="n">pg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="c1"># Wait for placement group to be ready</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">pg</span><span class="o">.</span><span class="n">ready</span><span class="p">(),</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">180</span><span class="p">)</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Placement group </span><span class="si">{</span><span class="n">placement_group_name</span><span class="si">}</span><span class="s2"> ready&quot;</span><span class="p">)</span>

            <span class="c1"># Calculate bundle indices for tensor parallelism</span>
            <span class="n">bundle_indices</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">num_gpus</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">bundle_indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_gpus</span><span class="p">))</span>
            <span class="n">bundle_index</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Always use first bundle since each replica has its own placement group</span>

            <span class="n">scheduling_strategy</span> <span class="o">=</span> <span class="n">PlacementGroupSchedulingStrategy</span><span class="p">(</span>
                <span class="n">placement_group</span><span class="o">=</span><span class="n">pg</span><span class="p">,</span>
                <span class="n">placement_group_capture_child_tasks</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">placement_group_bundle_index</span><span class="o">=</span><span class="n">bundle_index</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">actor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actor_class</span><span class="o">.</span><span class="n">options</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;async-vllm-replica-</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_service_id</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">namespace</span><span class="o">=</span><span class="s2">&quot;torchrl_vllm&quot;</span><span class="p">,</span>
                <span class="n">scheduling_strategy</span><span class="o">=</span><span class="n">scheduling_strategy</span><span class="p">,</span>
                <span class="n">num_gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                <span class="n">num_cpus</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
                <span class="n">engine_args</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="p">,</span>
                <span class="n">bundle_indices</span><span class="o">=</span><span class="n">bundle_indices</span><span class="p">,</span>
                <span class="n">enable_prefix_caching</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="o">.</span><span class="n">enable_prefix_caching</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">actor</span><span class="p">)</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Waiting for actors to be ready&quot;</span><span class="p">)</span>
        <span class="c1"># Wait for this actor to be ready before creating the next one</span>
        <span class="n">ready_futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">actor</span><span class="o">.</span><span class="n">ready</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">actor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="n">ready_futures</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="n">TIMEOUT_SECONDS</span>
            <span class="p">)</span>  <span class="c1"># 5 minute timeout for engine initialization</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;âœ… Actors are ready&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;âŒ Failed to initialize actors within </span><span class="si">{</span><span class="n">TIMEOUT_SECONDS</span><span class="si">}</span><span class="s2"> seconds: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. You can increase the timeout by setting the TORCHRL_VLLM_TIMEOUT_SECONDS environment variable.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span>

        <span class="c1"># Store the first placement group for backward compatibility</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_placement_group</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_placement_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_placement_groups</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_launched</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;âœ… Successfully launched </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span><span class="si">}</span><span class="s2"> async vLLM engine actors&quot;</span>
        <span class="p">)</span>

<div class="viewcode-block" id="AsyncVLLM.launch"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.launch">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">launch</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">engine_args</span><span class="p">:</span> <span class="n">AsyncEngineArgs</span><span class="p">,</span>
        <span class="n">num_replicas</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncVLLM</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Launch a new AsyncVLLMEngineService.</span>

<span class="sd">        Args:</span>
<span class="sd">            engine_args (AsyncEngineArgs): Arguments for creating the AsyncLLMEngine instances.</span>
<span class="sd">            num_replicas (int): Number of actor replicas to create.</span>

<span class="sd">        Returns:</span>
<span class="sd">            AsyncVLLMEngineService: The launched service.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">service</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">engine_args</span><span class="p">,</span> <span class="n">num_replicas</span><span class="p">)</span>
        <span class="n">service</span><span class="o">.</span><span class="n">_launch</span><span class="p">()</span>
        <span class="c1"># create a default load balancer with smart routing</span>
        <span class="n">service</span><span class="o">.</span><span class="n">create_load_balancer</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">service</span></div>

<div class="viewcode-block" id="AsyncVLLM.from_pretrained"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.from_pretrained">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_pretrained</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_replicas</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="nb">compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">enable_fp32_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncVLLM</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create an AsyncVLLM instance from a pretrained model.</span>

<span class="sd">        This is a convenience method that combines model loading and service launching</span>
<span class="sd">        in a single call, similar to how other ML libraries work.</span>

<span class="sd">        Args:</span>
<span class="sd">            model_name (str): The model name to pass to vLLM.</span>
<span class="sd">            num_devices (int, optional): Number of devices to use, per replica.</span>
<span class="sd">            num_replicas (int): Number of engine replicas to create.</span>
<span class="sd">            verbose (bool, optional): Whether to enable verbose logging with throughput statistics. Defaults to True.</span>
<span class="sd">            compile (bool, optional): Whether to enable model compilation for better performance. Defaults to True.</span>
<span class="sd">            enable_fp32_output (bool, optional): Whether to enable FP32 output for the final layer. Defaults to False.</span>
<span class="sd">            **kwargs: Additional arguments passed to AsyncEngineArgs.</span>

<span class="sd">        Returns:</span>
<span class="sd">            AsyncVLLM: The launched async vLLM service.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; # Simple usage with defaults</span>
<span class="sd">            &gt;&gt;&gt; service = AsyncVLLM.from_pretrained(&quot;Qwen/Qwen2.5-3B&quot;)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Multi-GPU tensor parallel with multiple replicas</span>
<span class="sd">            &gt;&gt;&gt; service = AsyncVLLM.from_pretrained(</span>
<span class="sd">            ...     &quot;Qwen/Qwen2.5-7B&quot;,</span>
<span class="sd">            ...     num_devices=2,</span>
<span class="sd">            ...     num_replicas=2,</span>
<span class="sd">            ...     max_model_len=4096</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Generate text</span>
<span class="sd">            &gt;&gt;&gt; from vllm import SamplingParams</span>
<span class="sd">            &gt;&gt;&gt; result = service.generate(&quot;Hello, world!&quot;, SamplingParams(max_tokens=50))</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Enable FP32 output for better numerical stability</span>
<span class="sd">            &gt;&gt;&gt; service = AsyncVLLM.from_pretrained(</span>
<span class="sd">            ...     &quot;Qwen/Qwen2.5-3B&quot;,</span>
<span class="sd">            ...     enable_fp32_output=True</span>
<span class="sd">            ... )</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">make_async_vllm_engine</span><span class="p">(</span>
            <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">num_devices</span><span class="o">=</span><span class="n">num_devices</span><span class="p">,</span>
            <span class="n">num_replicas</span><span class="o">=</span><span class="n">num_replicas</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
            <span class="nb">compile</span><span class="o">=</span><span class="nb">compile</span><span class="p">,</span>
            <span class="n">enable_fp32_output</span><span class="o">=</span><span class="n">enable_fp32_output</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_is_batch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if the input represents a batch of prompts.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompts: Input prompts that can be string, TokensPrompt, or list of these</span>
<span class="sd">            prompt_token_ids: Alternative token IDs input</span>

<span class="sd">        Returns:</span>
<span class="sd">            bool: True if this represents multiple prompts, False for single prompt</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If prompts is a list, we need to determine if it&#39;s a batch or a single prompt</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="c1"># Empty list is not a batch</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>

            <span class="c1"># If all elements are integers, it&#39;s a single prompt represented as token IDs</span>
            <span class="c1"># We trust that if one is an int, then all are ints.</span>
            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">):</span>
                <span class="k">return</span> <span class="kc">False</span>

            <span class="c1"># If it contains strings, TokensPrompt objects, or other non-integer types,</span>
            <span class="c1"># it&#39;s a batch of prompts</span>
            <span class="k">return</span> <span class="kc">True</span>

        <span class="c1"># If prompt_token_ids is provided and is a list of lists, it&#39;s a batch</span>
        <span class="k">if</span> <span class="n">prompt_token_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
                <span class="k">return</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_iterate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">prompts</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Iterate over individual prompts in a batch.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompts: Input prompts that can be string, TokensPrompt, or list of these</span>
<span class="sd">            prompt_token_ids: Alternative token IDs input</span>

<span class="sd">        Yields:</span>
<span class="sd">            tuple: (individual_prompt, individual_prompt_token_ids) for each item</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="c1"># Check if this is actually a single prompt represented as token IDs</span>
            <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">):</span>
                <span class="c1"># This is a single prompt as token IDs, not a batch</span>
                <span class="k">yield</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">prompt_token_ids</span>
                <span class="k">return</span>

            <span class="c1"># Handle list of prompts (actual batch)</span>
            <span class="k">if</span> <span class="n">prompt_token_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">prompt</span><span class="p">,</span> <span class="kc">None</span>
            <span class="k">elif</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="c1"># Both prompts and prompt_token_ids are lists</span>
                <span class="k">for</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">token_ids</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">):</span>
                    <span class="k">yield</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">token_ids</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># prompts is list, but prompt_token_ids is single list - replicate it</span>
                <span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">prompt_token_ids</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Single prompt case</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">prompt_token_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="c1"># Single prompt but multiple token_ids - replicate prompt</span>
                <span class="k">for</span> <span class="n">token_ids</span> <span class="ow">in</span> <span class="n">prompt_token_ids</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">token_ids</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Single prompt, single (or no) token_ids</span>
                <span class="k">yield</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">prompt_token_ids</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_impl</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_tqdm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_adapter_request</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">guided_options_request</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timeout_seconds</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">actor_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text for a single prompt and return a Ray future.</span>

<span class="sd">        This is the internal implementation that returns a future instead of the result.</span>
<span class="sd">        Used for batched generation to enable parallel execution.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt: Single prompt (string, TokensPrompt, etc.)</span>
<span class="sd">            sampling_params: SamplingParams object for controlling generation behavior</span>
<span class="sd">            prompt_token_ids: Token IDs for a single prompt</span>
<span class="sd">            use_tqdm: Whether to show progress bar (not used in async engine)</span>
<span class="sd">            lora_request: LoRA request for adapter-based generation</span>
<span class="sd">            prompt_adapter_request: Prompt adapter request</span>
<span class="sd">            guided_options_request: Guided decoding options</span>
<span class="sd">            timeout_seconds: Timeout for generation in seconds</span>
<span class="sd">            actor_index: Specific actor to use (random if None)</span>

<span class="sd">        Returns:</span>
<span class="sd">            Ray ObjectRef: Future that will resolve to RequestOutput</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">actor_index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">actor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_balancer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;LoadBalancer is not created. Create a LoadBalancer using AsyncVLLM.create_load_balancer before calling generate.&quot;</span>
                    <span class="p">)</span>
                <span class="c1"># Extract single prompt for prefix-aware routing</span>
                <span class="n">single_prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_single_prompt_for_routing</span><span class="p">(</span>
                    <span class="n">prompt</span><span class="p">,</span> <span class="n">prompt_token_ids</span>
                <span class="p">)</span>
                <span class="n">actor_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_balancer</span><span class="o">.</span><span class="n">select_actor</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">single_prompt</span><span class="p">)</span>
                <span class="n">actor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">[</span><span class="n">actor_index</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">actor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">[</span><span class="n">actor_index</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">actor</span><span class="o">.</span><span class="n">generate</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
            <span class="n">prompt</span><span class="p">,</span>
            <span class="n">sampling_params</span><span class="p">,</span>
            <span class="n">prompt_token_ids</span><span class="o">=</span><span class="n">prompt_token_ids</span><span class="p">,</span>
            <span class="n">use_tqdm</span><span class="o">=</span><span class="n">use_tqdm</span><span class="p">,</span>
            <span class="n">lora_request</span><span class="o">=</span><span class="n">lora_request</span><span class="p">,</span>
            <span class="n">prompt_adapter_request</span><span class="o">=</span><span class="n">prompt_adapter_request</span><span class="p">,</span>
            <span class="n">guided_options_request</span><span class="o">=</span><span class="n">guided_options_request</span><span class="p">,</span>
            <span class="n">timeout_seconds</span><span class="o">=</span><span class="n">timeout_seconds</span><span class="p">,</span>
        <span class="p">)</span>

<div class="viewcode-block" id="AsyncVLLM.generate"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.generate">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_tqdm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">lora_request</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_adapter_request</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">guided_options_request</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timeout_seconds</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">actor_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RequestOutput</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate text using one of the actors with vLLM.LLM.generate interface.</span>

<span class="sd">        This method provides the same interface as vLLM.LLM.generate for seamless</span>
<span class="sd">        compatibility between sync and async engines. It can be used to generate text</span>
<span class="sd">        within multiple threads / actors. If `actor_index` is not provided, the load balancer</span>
<span class="sd">        will be used to select the actor.</span>

<span class="sd">        `generate` is a blocking method, so it will wait for the generation to complete.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompts (String, TokensPrompt, or list of these): Input prompts for generation.</span>
<span class="sd">            sampling_params (SamplingParams): SamplingParams object for controlling generation behavior.</span>
<span class="sd">            prompt_token_ids (list[int] | list[list[int]]): Alternative to prompts - token IDs for generation.</span>
<span class="sd">            use_tqdm (bool): Whether to show progress bar (not used in async engine).</span>
<span class="sd">            lora_request (Any): LoRA request for adapter-based generation.</span>
<span class="sd">            prompt_adapter_request (Any): Prompt adapter request.</span>
<span class="sd">            guided_options_request (Any): Guided decoding options.</span>
<span class="sd">            timeout_seconds (float | None): Timeout for generation in seconds.</span>
<span class="sd">            actor_index (int | None): Specific actor to use (random if None).</span>

<span class="sd">        Returns:</span>
<span class="sd">            RequestOutput | list[RequestOutput]: Generated outputs from vLLM.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
        <span class="c1"># Check if this is a batch request</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_batch</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">):</span>
            <span class="c1"># Handle batched input by unbinding and sending individual requests</span>
            <span class="n">futures</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">prompt_token_ids_i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_iterate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">):</span>
                <span class="n">future</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_impl</span><span class="p">(</span>
                    <span class="n">prompt</span><span class="p">,</span>
                    <span class="n">sampling_params</span><span class="p">,</span>
                    <span class="n">prompt_token_ids</span><span class="o">=</span><span class="n">prompt_token_ids_i</span><span class="p">,</span>
                    <span class="n">use_tqdm</span><span class="o">=</span><span class="n">use_tqdm</span><span class="p">,</span>
                    <span class="n">lora_request</span><span class="o">=</span><span class="n">lora_request</span><span class="p">,</span>
                    <span class="n">prompt_adapter_request</span><span class="o">=</span><span class="n">prompt_adapter_request</span><span class="p">,</span>
                    <span class="n">guided_options_request</span><span class="o">=</span><span class="n">guided_options_request</span><span class="p">,</span>
                    <span class="n">timeout_seconds</span><span class="o">=</span><span class="n">timeout_seconds</span><span class="p">,</span>
                    <span class="n">actor_index</span><span class="o">=</span><span class="n">actor_index</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>

            <span class="c1"># Collect all results</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">results</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Single prompt case - call _generate_impt and get result directly</span>
            <span class="n">future</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_impl</span><span class="p">(</span>
                <span class="n">prompts</span><span class="p">,</span>
                <span class="n">sampling_params</span><span class="p">,</span>
                <span class="n">prompt_token_ids</span><span class="o">=</span><span class="n">prompt_token_ids</span><span class="p">,</span>
                <span class="n">use_tqdm</span><span class="o">=</span><span class="n">use_tqdm</span><span class="p">,</span>
                <span class="n">lora_request</span><span class="o">=</span><span class="n">lora_request</span><span class="p">,</span>
                <span class="n">prompt_adapter_request</span><span class="o">=</span><span class="n">prompt_adapter_request</span><span class="p">,</span>
                <span class="n">guided_options_request</span><span class="o">=</span><span class="n">guided_options_request</span><span class="p">,</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="n">timeout_seconds</span><span class="p">,</span>
                <span class="n">actor_index</span><span class="o">=</span><span class="n">actor_index</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">result</span></div>

<div class="viewcode-block" id="AsyncVLLM.get_random_actor_index"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.get_random_actor_index">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_random_actor_index</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get a random actor index.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_weight_update_group_internal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">master_address</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">master_port</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NCCL weight update group across all actors.</span>

<span class="sd">        Args:</span>
<span class="sd">            master_address (str): Master address for distributed training.</span>
<span class="sd">            master_port (str): Master port for distributed training.</span>

<span class="sd">        Returns:</span>
<span class="sd">            list: Ray futures for initialization calls.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">gpus_per_replica</span> <span class="o">=</span> <span class="n">_gpus_per_replica</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="p">)</span>
        <span class="n">weight_sync_world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_replicas</span> <span class="o">*</span> <span class="n">gpus_per_replica</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;AsyncVLLMEngineService requests weight update group for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_replicas</span><span class="si">}</span><span class="s2"> actors &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;with </span><span class="si">{</span><span class="n">gpus_per_replica</span><span class="si">}</span><span class="s2"> GPUs per replica and </span><span class="si">{</span><span class="n">weight_sync_world_size</span><span class="si">}</span><span class="s2"> world size&quot;</span>
        <span class="p">)</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">envs</span>

        <span class="n">refs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">actor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">):</span>
            <span class="n">rank_offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">gpus_per_replica</span>
            <span class="k">if</span> <span class="n">envs</span> <span class="ow">and</span> <span class="n">envs</span><span class="o">.</span><span class="n">VLLM_USE_V1</span><span class="p">:</span>
                <span class="n">actor_collective_rpc</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">collective_rpc_v1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">actor_collective_rpc</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">collective_rpc_v0</span>

            <span class="n">refs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">actor_collective_rpc</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
                    <span class="s2">&quot;init_weight_update_group&quot;</span><span class="p">,</span>
                    <span class="n">args</span><span class="o">=</span><span class="p">(</span>
                        <span class="n">master_address</span><span class="p">,</span>
                        <span class="n">master_port</span><span class="p">,</span>
                        <span class="n">rank_offset</span><span class="p">,</span>
                        <span class="n">weight_sync_world_size</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;AsyncVLLMEngineService args: </span><span class="si">{</span><span class="n">master_address</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">master_port</span><span class="si">=}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">rank_offset</span><span class="si">=}</span><span class="s2">, </span><span class="si">{</span><span class="n">weight_sync_world_size</span><span class="si">=}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;AsyncVLLMEngineService requests weight update group for actor </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;with rank_offset </span><span class="si">{</span><span class="n">rank_offset</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">refs</span>

<div class="viewcode-block" id="AsyncVLLM.collective_rpc"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.collective_rpc">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">collective_rpc</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">timeout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">(),</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward an RPC to all actors.</span>

<span class="sd">        Args:</span>
<span class="sd">            method (str): Method name to call.</span>
<span class="sd">            timeout (float | None): Timeout for the RPC call.</span>
<span class="sd">            args (tuple): Arguments to pass to the method.</span>
<span class="sd">            kwargs (dict | None): Keyword arguments to pass to the method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            list[Any]: Ray futures for all RPC calls.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">envs</span>

        <span class="n">futures</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">actor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">envs</span> <span class="ow">and</span> <span class="n">envs</span><span class="o">.</span><span class="n">VLLM_USE_V1</span><span class="p">:</span>
                <span class="n">actor_collective_rpc</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">collective_rpc_v1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">actor_collective_rpc</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">collective_rpc_v0</span>
            <span class="n">futures</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">actor_collective_rpc</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">timeout</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">futures</span></div>

<div class="viewcode-block" id="AsyncVLLM.shutdown"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.shutdown">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">shutdown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Shutdown all actors and clean up resources.&quot;&quot;&quot;</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Shutting down </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span><span class="si">}</span><span class="s2"> async vLLM engine actors...&quot;</span>
        <span class="p">)</span>

        <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">ray.util.placement_group</span><span class="w"> </span><span class="kn">import</span> <span class="n">remove_placement_group</span>

        <span class="c1"># Kill all actors</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">actor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">ray</span><span class="o">.</span><span class="n">kill</span><span class="p">(</span><span class="n">actor</span><span class="p">)</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shutdown async actor </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error shutting down async actor </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Clear the actors list</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="c1"># Remove placement groups if any</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_placement_groups&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_placement_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_placement_groups</span><span class="p">):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">remove_placement_group</span><span class="p">(</span><span class="n">pg</span><span class="p">)</span>
                    <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Removed placement group </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_placement_groups</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Error removing placement group </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_placement_groups</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Remove legacy single placement group if any</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_placement_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">remove_placement_group</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_placement_group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_placement_group</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_launched</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;AsyncVLLMEngineService shutdown complete&quot;</span><span class="p">)</span></div>

    <span class="c1"># RLvLLMEngine interface implementation</span>
<div class="viewcode-block" id="AsyncVLLM.get_tp_size"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.get_tp_size">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_tp_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the tensor parallel size.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="o">.</span><span class="n">tensor_parallel_size</span></div>

<div class="viewcode-block" id="AsyncVLLM.get_model_metadata"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.get_model_metadata">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_model_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get model parameter metadata.</span>

<span class="sd">        Note: This requires the model to be loaded. For now, we return an empty dict</span>
<span class="sd">        and expect the metadata to be provided externally during weight updates.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO: Implement metadata extraction from loaded model</span>
        <span class="c1"># This would require accessing the model from one of the actors</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;AsyncVLLM.get_model_metadata() not yet implemented - returning empty dict&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="p">{}</span></div>

<div class="viewcode-block" id="AsyncVLLM.get_master_address"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.get_master_address">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_master_address</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the master address for weight synchronization.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="s2">&quot;localhost&quot;</span>  <span class="c1"># Default for now</span></div>

<div class="viewcode-block" id="AsyncVLLM.get_master_port"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.get_master_port">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_master_port</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the master port for weight synchronization.&quot;&quot;&quot;</span>
        <span class="c1"># Cache the port like V1 does to ensure consistency</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_cached_master_port&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">_has_vllm</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="kn">from</span><span class="w"> </span><span class="nn">vllm.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_open_port</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">_cached_master_port</span> <span class="o">=</span> <span class="n">get_open_port</span><span class="p">()</span>
                <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cached_master_port</span> <span class="o">=</span> <span class="mi">29500</span>  <span class="c1"># Default port if import fails</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cached_master_port</span> <span class="o">=</span> <span class="mi">29500</span>  <span class="c1"># Default port</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cached_master_port</span></div>

<div class="viewcode-block" id="AsyncVLLM.init_weight_update_group"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.init_weight_update_group">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">init_weight_update_group</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">master_address</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">master_port</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward the request to init NCCL weight update group to all actors.</span>

<span class="sd">        This method initializes the weight update group for all vLLM workers.</span>
<span class="sd">        The external trainer should be rank 0, and vLLM workers will be ranks 1+.</span>

<span class="sd">        Args:</span>
<span class="sd">            master_address: Master address for NCCL communication.</span>
<span class="sd">            master_port: Master port for NCCL communication.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List of Ray futures for the initialization calls.</span>

<span class="sd">        Note:</span>
<span class="sd">            The caller must wait on the returned futures (ray.get(refs)) to ensure</span>
<span class="sd">            all workers have completed initialization before sending weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_launched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;AsyncVLLM service must be launched before initializing weight update group&quot;</span>
            <span class="p">)</span>

        <span class="n">gpus_per_replica</span> <span class="o">=</span> <span class="n">_gpus_per_replica</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="p">)</span>
        <span class="n">weight_sync_world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_replicas</span> <span class="o">*</span> <span class="n">gpus_per_replica</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Initializing weight update group for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">num_replicas</span><span class="si">}</span><span class="s2"> replicas &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;with </span><span class="si">{</span><span class="n">gpus_per_replica</span><span class="si">}</span><span class="s2"> GPUs each (world_size=</span><span class="si">{</span><span class="n">weight_sync_world_size</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="p">)</span>

        <span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">envs</span>

        <span class="n">refs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">actor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">):</span>
            <span class="n">rank_offset</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">gpus_per_replica</span>
            <span class="k">if</span> <span class="n">envs</span> <span class="ow">and</span> <span class="n">envs</span><span class="o">.</span><span class="n">VLLM_USE_V1</span><span class="p">:</span>
                <span class="n">actor_collective_rpc</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">collective_rpc_v1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">actor_collective_rpc</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">collective_rpc_v0</span>
            <span class="n">refs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">actor_collective_rpc</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
                    <span class="s2">&quot;init_weight_update_group&quot;</span><span class="p">,</span>
                    <span class="n">args</span><span class="o">=</span><span class="p">(</span>
                        <span class="n">master_address</span><span class="p">,</span>
                        <span class="nb">str</span><span class="p">(</span><span class="n">master_port</span><span class="p">),</span>
                        <span class="n">rank_offset</span><span class="p">,</span>
                        <span class="n">weight_sync_world_size</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Requested init for actor </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> with rank_offset </span><span class="si">{</span><span class="n">rank_offset</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">refs</span></div>

<div class="viewcode-block" id="AsyncVLLM.update_weights"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.update_weights">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update model weights across all replicas using NCCL broadcast.</span>

<span class="sd">        Args:</span>
<span class="sd">            weights: Iterator yielding (parameter_name, tensor) tuples</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_launched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;AsyncVLLM service must be launched before updating weights&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Convert iterator to dict for easier handling</span>
        <span class="n">weights_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">weights_dict</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;No weights provided for update&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Updating </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">weights_dict</span><span class="p">)</span><span class="si">}</span><span class="s2"> parameters across </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span><span class="si">}</span><span class="s2"> replicas using NCCL broadcast&quot;</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_update_weights_with_nccl_broadcast_simple</span><span class="p">(</span><span class="n">weights_dict</span><span class="p">)</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;AsyncVLLM NCCL weight update completed&quot;</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update_weights_with_nccl_broadcast_simple</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">weights_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update weights using simple NCCL broadcast like V1.</span>

<span class="sd">        This approach follows the V1 pattern:</span>
<span class="sd">        1. Training process (master) broadcasts as rank 0</span>
<span class="sd">        2. All vLLM workers receive as ranks 1, 2, 3...</span>
<span class="sd">        3. Simple and reliable like the working V1 implementation</span>

<span class="sd">        Args:</span>
<span class="sd">            weights_dict: Dictionary of parameter names to weight tensors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_nccl_master_group&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nccl_master_group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;NCCL master group not initialized. This is a bug in the setup process.&quot;</span>
            <span class="p">)</span>

        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

        <span class="c1"># Move all weights to cuda:0 (matching NCCL communicator device)</span>
        <span class="n">gpu_weights</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">weights_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Ensure weight is on cuda:0 (matching NCCL communicator)</span>
            <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">):</span>
                <span class="n">gpu_weights</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">gpu_weights</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span>

        <span class="c1"># Use periodic-mono pattern: individual weight updates with immediate RPC-&gt;NCCL</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Updating </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">gpu_weights</span><span class="p">)</span><span class="si">}</span><span class="s2"> weights using periodic-mono pattern...&quot;</span>
        <span class="p">)</span>

        <span class="n">updated_weights</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>  <span class="c1"># Ensure we&#39;re on the correct CUDA device</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">gpu_weights</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># Convert dtype to string name (like periodic-mono)</span>
                <span class="n">dtype_name</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span>
                    <span class="o">-</span><span class="mi">1</span>
                <span class="p">]</span>  <span class="c1"># &quot;torch.bfloat16&quot; -&gt; &quot;bfloat16&quot;</span>

                <span class="c1"># Step 1: Send RPC to workers for this weight</span>
                <span class="n">futures</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collective_rpc</span><span class="p">(</span>
                    <span class="s2">&quot;update_weight&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype_name</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
                <span class="p">)</span>

                <span class="c1"># Step 2: Immediately broadcast this weight (like periodic-mono)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_nccl_master_group</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span>
                    <span class="n">weight</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
                <span class="p">)</span>

                <span class="c1"># Step 3: Wait for workers to complete this weight</span>
                <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span>
                <span class="n">updated_weights</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">t2</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Successfully updated </span><span class="si">{</span><span class="n">updated_weights</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">gpu_weights</span><span class="p">)</span><span class="si">}</span><span class="s2"> weights in </span><span class="si">{</span><span class="n">t2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">t0</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">s&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_setup_nccl_master_group</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set up NCCL communication group for the master node (rank 0).&quot;&quot;&quot;</span>
        <span class="c1"># Calculate world size (should match what workers use)</span>
        <span class="n">gpus_per_replica</span> <span class="o">=</span> <span class="n">_gpus_per_replica</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">engine_args</span><span class="p">)</span>
        <span class="n">weight_sync_world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_replicas</span> <span class="o">*</span> <span class="n">gpus_per_replica</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="n">master_address</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_master_address</span><span class="p">()</span>
        <span class="n">master_port</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_master_port</span><span class="p">()</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Setting up NCCL master group: rank=0, world_size=</span><span class="si">{</span><span class="n">weight_sync_world_size</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;address=</span><span class="si">{</span><span class="n">master_address</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">master_port</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Ensure CUDA is available and initialized</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;CUDA not available for NCCL communication&quot;</span><span class="p">)</span>

        <span class="c1"># Set CUDA device before initializing NCCL</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Initialize master as rank 0 in the NCCL group (use synchronous version)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_nccl_master_group</span> <span class="o">=</span> <span class="n">stateless_init_process_group</span><span class="p">(</span>
            <span class="n">master_address</span><span class="o">=</span><span class="n">master_address</span><span class="p">,</span>
            <span class="n">master_port</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">master_port</span><span class="p">),</span>
            <span class="n">rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Master is always rank 0</span>
            <span class="n">world_size</span><span class="o">=</span><span class="n">weight_sync_world_size</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;NCCL master group initialized successfully&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="AsyncVLLM.get_num_unfinished_requests"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.get_num_unfinished_requests">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_num_unfinished_requests</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">actor_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the number of unfinished requests for one or all actors.</span>

<span class="sd">        Args:</span>
<span class="sd">            actor_index (int | None): Index of specific actor, or None for all actors.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int | list[int]: Number of unfinished requests for the specified actor,</span>
<span class="sd">                           or list of counts for all actors if actor_index is None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_launched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;AsyncVLLM service must be launched before getting request counts&quot;</span>
            <span class="p">)</span>

        <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">actor_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">actor_index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Actor index </span><span class="si">{</span><span class="n">actor_index</span><span class="si">}</span><span class="s2"> out of range [0, </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>

            <span class="n">actor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">[</span><span class="n">actor_index</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">actor</span><span class="o">.</span><span class="n">get_num_unfinished_requests</span><span class="o">.</span><span class="n">remote</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Get counts from all actors</span>
            <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">actor</span><span class="o">.</span><span class="n">get_num_unfinished_requests</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">actor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span>
            <span class="p">]</span>
            <span class="k">return</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span></div>

<div class="viewcode-block" id="AsyncVLLM.get_cache_usage"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.get_cache_usage">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_cache_usage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actor_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the KV cache usage for one or all actors.</span>

<span class="sd">        Args:</span>
<span class="sd">            actor_index (int | None): Index of specific actor, or None for all actors.</span>

<span class="sd">        Returns:</span>
<span class="sd">            float | list[float]: Cache usage fraction for the specified actor,</span>
<span class="sd">                               or list of usage fractions for all actors if actor_index is None.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_launched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;AsyncVLLM service must be launched before getting cache usage&quot;</span>
            <span class="p">)</span>

        <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">actor_index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">actor_index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Actor index </span><span class="si">{</span><span class="n">actor_index</span><span class="si">}</span><span class="s2"> out of range [0, </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>
                <span class="p">)</span>

            <span class="n">actor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">[</span><span class="n">actor_index</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">actor</span><span class="o">.</span><span class="n">get_cache_usage</span><span class="o">.</span><span class="n">remote</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Get usage from all actors</span>
            <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">actor</span><span class="o">.</span><span class="n">get_cache_usage</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">actor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span></div>

<div class="viewcode-block" id="AsyncVLLM.create_load_balancer"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.create_load_balancer">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">create_load_balancer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">strategy</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;requests&quot;</span><span class="p">,</span> <span class="s2">&quot;kv-cache&quot;</span><span class="p">]</span>
        <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;prefix-aware&quot;</span><span class="p">,</span> <span class="s2">&quot;requests&quot;</span><span class="p">,</span> <span class="s2">&quot;kv-cache&quot;</span><span class="p">,</span> <span class="s2">&quot;round-robin&quot;</span><span class="p">]]</span>
        <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LoadBalancer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a load balancer for this AsyncVLLM service.</span>

<span class="sd">        Args:</span>
<span class="sd">            strategy: Load balancing strategy or sequence of strategies in fallback order.</span>
<span class="sd">                Default: [&quot;prefix-aware&quot;, &quot;requests&quot;] - tries cache-aware routing first,</span>
<span class="sd">                then load balancing. Single strategies: &quot;requests&quot;, &quot;kv-cache&quot;</span>
<span class="sd">                Strategy sequences: [&quot;prefix-aware&quot;, &quot;requests&quot;, &quot;round-robin&quot;]</span>
<span class="sd">            **kwargs: Additional arguments passed to LoadBalancer constructor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            LoadBalancer: Configured load balancer instance. This is stored in the AsyncVLLM instance.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; service = AsyncVLLM.from_pretrained(&quot;Qwen/Qwen2.5-3B&quot;, num_replicas=3)</span>

<span class="sd">            &gt;&gt;&gt; # Use smart defaults (prefix-aware -&gt; requests)</span>
<span class="sd">            &gt;&gt;&gt; lb = service.create_load_balancer()</span>
<span class="sd">            &gt;&gt;&gt; selected_actor_index = lb.select_actor(prompt=&quot;Hello world&quot;)</span>

<span class="sd">            &gt;&gt;&gt; # Simple single strategy</span>
<span class="sd">            &gt;&gt;&gt; lb = service.create_load_balancer(&quot;requests&quot;)</span>
<span class="sd">            &gt;&gt;&gt; selected_actor_index = lb.select_actor()</span>

<span class="sd">            &gt;&gt;&gt; # Custom strategy hierarchy</span>
<span class="sd">            &gt;&gt;&gt; lb = service.create_load_balancer(</span>
<span class="sd">            ...     [&quot;prefix-aware&quot;, &quot;kv-cache&quot;, &quot;round-robin&quot;],</span>
<span class="sd">            ...     prefix_length=16,</span>
<span class="sd">            ...     overload_threshold=2.0</span>
<span class="sd">            ... )</span>
<span class="sd">            &gt;&gt;&gt; selected_actor_index = lb.select_actor(prompt=&quot;Hello world&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_launched</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;AsyncVLLM service must be launched before creating load balancer&quot;</span>
            <span class="p">)</span>

        <span class="n">load_balancer</span> <span class="o">=</span> <span class="n">LoadBalancer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_load_balancer</span> <span class="o">=</span> <span class="n">load_balancer</span>
        <span class="k">return</span> <span class="n">load_balancer</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_extract_single_prompt_for_routing</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract a single prompt for load balancer routing, if possible.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompts: The prompts argument passed to generate().</span>
<span class="sd">            prompt_token_ids: The prompt_token_ids argument passed to generate().</span>

<span class="sd">        Returns:</span>
<span class="sd">            str | list[int] | None: Single prompt for routing, or None if multiple prompts.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Handle prompt_token_ids first (takes precedence over prompts)</span>
            <span class="k">if</span> <span class="n">prompt_token_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Empty list</span>
                    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="c1"># Single prompt case - could be tokens directly or nested list</span>
                        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">):</span>
                            <span class="c1"># Single token sequence: [token1, token2, ...]</span>
                            <span class="k">return</span> <span class="n">prompt_token_ids</span>
                        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
                            <span class="c1"># Nested list with single prompt: [[token1, token2, ...]]</span>
                            <span class="k">return</span> <span class="n">prompt_token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">return</span> <span class="kc">None</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Multiple prompts: [[tokens1...], [tokens2...], ...]</span>
                        <span class="k">return</span> <span class="kc">None</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Not a list, invalid format</span>
                    <span class="k">return</span> <span class="kc">None</span>

            <span class="c1"># Handle prompts argument</span>
            <span class="k">if</span> <span class="n">prompts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">None</span>

            <span class="c1"># Import vLLM types for proper checking</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
                <span class="c1"># Fallback if imports fail</span>
                <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

            <span class="c1"># Single string prompt</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">prompts</span>

            <span class="c1"># TokensPrompt object</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="s2">&quot;prompt_token_ids&quot;</span><span class="p">):</span>  <span class="c1"># TokensPrompt-like object</span>
                <span class="k">return</span> <span class="n">prompts</span><span class="o">.</span><span class="n">prompt_token_ids</span>

            <span class="c1"># TextPrompt object</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="s2">&quot;prompt&quot;</span><span class="p">):</span>  <span class="c1"># TextPrompt-like object</span>
                <span class="k">return</span> <span class="n">prompts</span><span class="o">.</span><span class="n">prompt</span>

            <span class="c1"># List of prompts</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># Empty list</span>
                <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="c1"># Single prompt in list - recursively extract</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_single_prompt_for_routing</span><span class="p">(</span><span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="kc">None</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Multiple prompts - cannot do prefix routing</span>
                    <span class="k">return</span> <span class="kc">None</span>

            <span class="c1"># Other types (shouldn&#39;t happen in normal usage)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unknown prompt type for routing: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="kc">None</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error extracting single prompt for routing: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">LoadBalancer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load balancer for distributing requests across AsyncVLLM actors with strategy hierarchy.</span>

<span class="sd">    This class implements sophisticated load balancing with multiple strategies and intelligent</span>
<span class="sd">    fallback mechanisms. Strategies are tried in order until one succeeds, providing robust</span>
<span class="sd">    request routing even when some strategies fail.</span>

<span class="sd">    Args:</span>
<span class="sd">        actors: Either a single AsyncVLLM instance or a list of Ray actors.</span>
<span class="sd">        strategy: Single strategy or sequence of strategies in fallback order.</span>
<span class="sd">            Available strategies:</span>

<span class="sd">            - &quot;prefix-aware&quot;: Route based on prompt prefix for cache locality</span>
<span class="sd">            - &quot;requests&quot;: Select actor with fewest pending requests</span>
<span class="sd">            - &quot;kv-cache&quot;: Select actor with lowest KV cache utilization</span>
<span class="sd">            - &quot;round-robin&quot;: Simple round-robin distribution</span>

<span class="sd">            Default: [&quot;prefix-aware&quot;, &quot;requests&quot;]</span>

<span class="sd">        prefix_length: Number of tokens/words to use for prefix routing (default: 8).</span>
<span class="sd">        overload_threshold: Multiplier for average load to consider actor overloaded (default: 1.5).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; service = AsyncVLLM.from_pretrained(&quot;Qwen/Qwen2.5-3B&quot;, num_replicas=3)</span>

<span class="sd">        &gt;&gt;&gt; # Simple strategy</span>
<span class="sd">        &gt;&gt;&gt; lb = LoadBalancer(service, &quot;requests&quot;)</span>
<span class="sd">        &gt;&gt;&gt; actor_idx = lb.select_actor()</span>

<span class="sd">        &gt;&gt;&gt; # Strategy hierarchy: try prefix-aware first, fall back to requests, then round-robin</span>
<span class="sd">        &gt;&gt;&gt; lb = LoadBalancer(service, [&quot;prefix-aware&quot;, &quot;requests&quot;, &quot;round-robin&quot;])</span>
<span class="sd">        &gt;&gt;&gt; actor_idx = lb.select_actor(prompt=&quot;Hello world&quot;)  # Uses prefix routing</span>
<span class="sd">        &gt;&gt;&gt; actor_idx = lb.select_actor()  # Falls back to requests (no prompt)</span>

<span class="sd">        &gt;&gt;&gt; # Custom configuration</span>
<span class="sd">        &gt;&gt;&gt; lb = LoadBalancer(</span>
<span class="sd">        ...     service,</span>
<span class="sd">        ...     [&quot;prefix-aware&quot;, &quot;kv-cache&quot;],</span>
<span class="sd">        ...     prefix_length=16,</span>
<span class="sd">        ...     overload_threshold=2.0</span>
<span class="sd">        ... )</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">actors</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="n">AsyncVLLM</span><span class="p">,</span>
        <span class="n">strategy</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;requests&quot;</span><span class="p">,</span> <span class="s2">&quot;kv-cache&quot;</span><span class="p">]</span>
        <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;prefix-aware&quot;</span><span class="p">,</span> <span class="s2">&quot;requests&quot;</span><span class="p">,</span> <span class="s2">&quot;kv-cache&quot;</span><span class="p">,</span> <span class="s2">&quot;round-robin&quot;</span><span class="p">]]</span>
        <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prefix_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">overload_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.5</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">strategy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">strategy</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;prefix-aware&quot;</span><span class="p">,</span> <span class="s2">&quot;requests&quot;</span><span class="p">]</span>
        <span class="c1"># Handle both AsyncVLLM instances and direct actor lists</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">actors</span><span class="p">,</span> <span class="s2">&quot;actors&quot;</span><span class="p">):</span>  <span class="c1"># AsyncVLLM instance</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">actors</span> <span class="o">=</span> <span class="n">actors</span><span class="o">.</span><span class="n">actors</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span> <span class="o">=</span> <span class="n">actors</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">actors</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>  <span class="c1"># Direct list of actors</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">actors</span> <span class="o">=</span> <span class="n">actors</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;actors must be either an AsyncVLLM instance or a list of actors&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No actors provided&quot;</span><span class="p">)</span>

        <span class="c1"># Handle both single strategy and strategy hierarchy</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">strategy</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strategies</span> <span class="o">=</span> <span class="p">[</span><span class="n">strategy</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strategies</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>

        <span class="c1"># Validate strategies</span>
        <span class="n">valid_strategies</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;prefix-aware&quot;</span><span class="p">,</span> <span class="s2">&quot;requests&quot;</span><span class="p">,</span> <span class="s2">&quot;kv-cache&quot;</span><span class="p">,</span> <span class="s2">&quot;round-robin&quot;</span><span class="p">}</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">strategies</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">s</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_strategies</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Invalid strategy &#39;</span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">&#39;. Must be one of </span><span class="si">{</span><span class="n">valid_strategies</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">strategies</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;At least one strategy must be provided&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">strategy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strategies</span><span class="p">[</span>
            <span class="mi">0</span>
        <span class="p">]</span>  <span class="c1"># Primary strategy for backward compatibility</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefix_length</span> <span class="o">=</span> <span class="n">prefix_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">overload_threshold</span> <span class="o">=</span> <span class="n">overload_threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_round_robin_index</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># For round-robin fallback</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_actor</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">request_context</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Select the optimal actor index based on the configured strategy hierarchy.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt: The input prompt (string or token list) for prefix-aware routing.</span>
<span class="sd">            request_context: Additional context for routing decisions.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: Index of the selected actor in the actors list.</span>

<span class="sd">        Raises:</span>
<span class="sd">            RuntimeError: If unable to gather metrics from actors.</span>
<span class="sd">            ValueError: If no actors are available.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No actors available for selection&quot;</span><span class="p">)</span>

        <span class="c1"># Try each strategy in order until one succeeds</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strategies</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Trying strategy </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strategies</span><span class="p">)</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">strategy</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

                <span class="k">if</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;prefix-aware&quot;</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">prompt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select_by_prefix_aware</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                            <span class="s2">&quot;No prompt provided for prefix-aware routing, trying next strategy&quot;</span>
                        <span class="p">)</span>
                        <span class="k">continue</span>

                <span class="k">elif</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;requests&quot;</span><span class="p">:</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select_by_requests</span><span class="p">()</span>

                <span class="k">elif</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;kv-cache&quot;</span><span class="p">:</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select_by_cache_usage</span><span class="p">()</span>

                <span class="k">elif</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;round-robin&quot;</span><span class="p">:</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select_round_robin</span><span class="p">()</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Unknown strategy: </span><span class="si">{</span><span class="n">strategy</span><span class="si">}</span><span class="s2">, trying next strategy&quot;</span>
                    <span class="p">)</span>
                    <span class="k">continue</span>

            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Strategy &#39;</span><span class="si">{</span><span class="n">strategy</span><span class="si">}</span><span class="s2">&#39; failed with error: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Trying next strategy...&quot;</span>
                <span class="p">)</span>
                <span class="k">continue</span>

        <span class="c1"># All strategies failed, final fallback to random</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;All strategies </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">strategies</span><span class="si">}</span><span class="s2"> failed. Falling back to random selection.&quot;</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_select_by_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Select actor with fewest pending requests.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use AsyncVLLM&#39;s built-in method to get request counts</span>
            <span class="n">request_counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span><span class="o">.</span><span class="n">get_num_unfinished_requests</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Query actors directly</span>
            <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">actor</span><span class="o">.</span><span class="n">get_num_unfinished_requests</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">actor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span>
            <span class="p">]</span>
            <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
            <span class="n">request_counts</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span>

        <span class="c1"># Find the actor with minimum pending requests</span>
        <span class="n">min_requests</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">request_counts</span><span class="p">)</span>
        <span class="n">min_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">request_counts</span><span class="p">)</span> <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="n">min_requests</span>
        <span class="p">]</span>

        <span class="c1"># If multiple actors have the same minimum count, choose randomly among them</span>
        <span class="n">selected_index</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">min_indices</span><span class="p">)</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;LoadBalancer (requests): Selected actor </span><span class="si">{</span><span class="n">selected_index</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;with </span><span class="si">{</span><span class="n">min_requests</span><span class="si">}</span><span class="s2"> pending requests. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Request counts: </span><span class="si">{</span><span class="n">request_counts</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">selected_index</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_select_by_cache_usage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Select actor with lowest KV cache utilization.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Use AsyncVLLM&#39;s built-in method to get cache usage</span>
            <span class="n">cache_usages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span><span class="o">.</span><span class="n">get_cache_usage</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Query actors directly</span>
            <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span><span class="n">actor</span><span class="o">.</span><span class="n">get_cache_usage</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">actor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">]</span>
            <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
            <span class="n">cache_usages</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span>

        <span class="c1"># Find the actor with minimum cache usage</span>
        <span class="n">min_usage</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">cache_usages</span><span class="p">)</span>
        <span class="n">min_indices</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">usage</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cache_usages</span><span class="p">)</span> <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">usage</span> <span class="o">-</span> <span class="n">min_usage</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span>
        <span class="p">]</span>

        <span class="c1"># If multiple actors have similar cache usage, choose randomly among them</span>
        <span class="n">selected_index</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">min_indices</span><span class="p">)</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;LoadBalancer (kv-cache): Selected actor </span><span class="si">{</span><span class="n">selected_index</span><span class="si">}</span><span class="s2"> &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;with </span><span class="si">{</span><span class="n">min_usage</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> cache usage. &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Cache usages: </span><span class="si">{</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">u</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">u</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">cache_usages</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">selected_index</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_select_by_prefix_aware</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Select actor based on prompt prefix for cache locality.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt: Input prompt as string or token list.</span>

<span class="sd">        Returns:</span>
<span class="sd">            int: Selected actor index.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If prefix cannot be extracted.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Extract prefix tokens</span>
            <span class="n">prefix_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extract_prefix_tokens</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">prefix_tokens</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not extract meaningful prefix tokens&quot;</span><span class="p">)</span>

            <span class="c1"># Create consistent hash from prefix</span>
            <span class="n">prefix_hash</span> <span class="o">=</span> <span class="nb">hash</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">prefix_tokens</span><span class="p">))</span>
            <span class="n">preferred_actor</span> <span class="o">=</span> <span class="n">prefix_hash</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span>

            <span class="c1"># Check if preferred actor is overloaded</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_actor_overloaded</span><span class="p">(</span><span class="n">preferred_actor</span><span class="p">):</span>
                <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Preferred actor </span><span class="si">{</span><span class="n">preferred_actor</span><span class="si">}</span><span class="s2"> is overloaded &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;(threshold: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">overload_threshold</span><span class="si">}</span><span class="s2">), falling back to load-based selection&quot;</span>
                <span class="p">)</span>
                <span class="c1"># Fall back to requests-based selection</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_select_by_requests</span><span class="p">()</span>

            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;LoadBalancer (prefix-aware): Selected actor </span><span class="si">{</span><span class="n">preferred_actor</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;for prefix hash </span><span class="si">{</span><span class="n">prefix_hash</span><span class="si">}</span><span class="s2"> (tokens: </span><span class="si">{</span><span class="n">prefix_tokens</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span><span class="si">}</span><span class="s2">...)&quot;</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">preferred_actor</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prefix-aware routing failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_select_round_robin</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Select actor using round-robin strategy.&quot;&quot;&quot;</span>
        <span class="n">selected</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round_robin_index</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_round_robin_index</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_round_robin_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">)</span>

        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LoadBalancer (round-robin): Selected actor </span><span class="si">{</span><span class="n">selected</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">selected</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_extract_prefix_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extract prefix tokens from prompt (string or token list).</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt: Input prompt.</span>

<span class="sd">        Returns:</span>
<span class="sd">            list[int]: Prefix tokens (up to self.prefix_length).</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If tokenization fails or prompt is invalid.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="c1"># Already tokenized</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">prompt</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Empty token list provided&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">prompt</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_length</span><span class="p">]</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># Need to tokenize - this requires access to tokenizer</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">prompt</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Empty or whitespace-only string provided&quot;</span><span class="p">)</span>

            <span class="c1"># Try to get tokenizer from AsyncVLLM instance</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># This is a simplistic approach - in practice you&#39;d want to cache the tokenizer</span>
                    <span class="c1"># For now, use a simple heuristic based on string content</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_simple_string_hash</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Could not tokenize string: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_simple_string_hash</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Fall back to simple string hashing</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_simple_string_hash</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported prompt type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_simple_string_hash</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create pseudo-tokens from string for prefix routing.</span>

<span class="sd">        This is a fallback when proper tokenization isn&#39;t available.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Use words as pseudo-tokens, limited to prefix_length</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_length</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">words</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No words found in text&quot;</span><span class="p">)</span>

        <span class="c1"># Convert words to integers using hash</span>
        <span class="n">pseudo_tokens</span> <span class="o">=</span> <span class="p">[</span>
            <span class="nb">abs</span><span class="p">(</span><span class="nb">hash</span><span class="p">(</span><span class="n">word</span><span class="p">))</span> <span class="o">%</span> <span class="mi">50000</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span>
        <span class="p">]</span>  <span class="c1"># Simulate vocab size</span>
        <span class="k">return</span> <span class="n">pseudo_tokens</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_is_actor_overloaded</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actor_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if an actor is overloaded compared to average load.</span>

<span class="sd">        Args:</span>
<span class="sd">            actor_index: Index of actor to check.</span>

<span class="sd">        Returns:</span>
<span class="sd">            bool: True if actor is overloaded.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">request_counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span><span class="o">.</span><span class="n">get_num_unfinished_requests</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">futures</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">actor</span><span class="o">.</span><span class="n">get_num_unfinished_requests</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">actor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span>
                <span class="p">]</span>
            <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
            <span class="n">request_counts</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">futures</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">request_counts</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>

            <span class="n">avg_requests</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">request_counts</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">request_counts</span><span class="p">)</span>
            <span class="n">actor_requests</span> <span class="o">=</span> <span class="n">request_counts</span><span class="p">[</span><span class="n">actor_index</span><span class="p">]</span>

            <span class="n">is_overloaded</span> <span class="o">=</span> <span class="n">actor_requests</span> <span class="o">&gt;</span> <span class="n">avg_requests</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">overload_threshold</span>

            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Actor </span><span class="si">{</span><span class="n">actor_index</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">actor_requests</span><span class="si">}</span><span class="s2"> requests, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;avg: </span><span class="si">{</span><span class="n">avg_requests</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, threshold: </span><span class="si">{</span><span class="n">avg_requests</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">overload_threshold</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;overloaded: </span><span class="si">{</span><span class="n">is_overloaded</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

            <span class="k">return</span> <span class="n">is_overloaded</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Could not check actor load: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>  <span class="c1"># Assume not overloaded if we can&#39;t check</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get current load balancing statistics for all actors.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict: Statistics including request counts and cache usage for all actors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;strategies&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">strategies</span><span class="p">,</span>
            <span class="s2">&quot;primary_strategy&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">strategy</span><span class="p">,</span>  <span class="c1"># For backward compatibility</span>
            <span class="s2">&quot;num_actors&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">actors</span><span class="p">),</span>
            <span class="s2">&quot;prefix_length&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix_length</span><span class="p">,</span>
            <span class="s2">&quot;overload_threshold&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">overload_threshold</span><span class="p">,</span>
            <span class="s2">&quot;round_robin_index&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round_robin_index</span><span class="p">,</span>
            <span class="s2">&quot;actor_stats&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="p">}</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">request_counts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span><span class="o">.</span><span class="n">get_num_unfinished_requests</span><span class="p">()</span>
                <span class="n">cache_usages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">async_vllm</span><span class="o">.</span><span class="n">get_cache_usage</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">request_futures</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">actor</span><span class="o">.</span><span class="n">get_num_unfinished_requests</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">actor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span>
                <span class="p">]</span>
                <span class="n">cache_futures</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">actor</span><span class="o">.</span><span class="n">get_cache_usage</span><span class="o">.</span><span class="n">remote</span><span class="p">()</span> <span class="k">for</span> <span class="n">actor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">actors</span>
                <span class="p">]</span>
            <span class="n">ray</span> <span class="o">=</span> <span class="n">_get_ray</span><span class="p">()</span>
            <span class="n">request_counts</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">request_futures</span><span class="p">)</span>
            <span class="n">cache_usages</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">cache_futures</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">requests</span><span class="p">,</span> <span class="n">cache_usage</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="nb">zip</span><span class="p">(</span><span class="n">request_counts</span><span class="p">,</span> <span class="n">cache_usages</span><span class="p">)</span>
            <span class="p">):</span>
                <span class="n">stats</span><span class="p">[</span><span class="s2">&quot;actor_stats&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;actor_index&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span>
                        <span class="s2">&quot;pending_requests&quot;</span><span class="p">:</span> <span class="n">requests</span><span class="p">,</span>
                        <span class="s2">&quot;cache_usage&quot;</span><span class="p">:</span> <span class="n">cache_usage</span><span class="p">,</span>
                    <span class="p">}</span>
                <span class="p">)</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error gathering load balancer stats: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">stats</span><span class="p">[</span><span class="s2">&quot;error&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">stats</span>


<div class="viewcode-block" id="make_async_vllm_engine"><a class="viewcode-back" href="../../../../../../reference/generated/torchrl.modules.llm.make_async_vllm_engine.html#torchrl.modules.llm.make_async_vllm_engine">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">make_async_vllm_engine</span><span class="p">(</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">num_devices</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_replicas</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="nb">compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">enable_fp32_output</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">tensor_parallel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">data_parallel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pipeline_parallel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncVLLM</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create an async vLLM engine service.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        model_name (str): The model name to pass to vLLM.</span>
<span class="sd">        num_devices (int, optional): Number of devices to use, per replica.</span>
<span class="sd">        num_replicas (int): Number of engine replicas to create.</span>
<span class="sd">        verbose (bool, optional): Whether to enable verbose logging with throughput statistics. Defaults to True.</span>
<span class="sd">        compile (bool, optional): Whether to enable model compilation for better performance. Defaults to True.</span>
<span class="sd">        enable_fp32_output (bool, optional): Whether to enable FP32 output for the final layer. Defaults to False.</span>
<span class="sd">            This can help with numerical stability for certain models. Requires model-specific support in</span>
<span class="sd">            torchrl.modules.llm.backends._models.</span>
<span class="sd">        tensor_parallel_size (int, optional): Number of devices to use, per replica. Defaults to None.</span>
<span class="sd">        data_parallel_size (int, optional): Number of data parallel groups to use. Defaults to None.</span>
<span class="sd">        pipeline_parallel_size (int, optional): Number of pipeline parallel groups to use. Defaults to None.</span>
<span class="sd">        **kwargs: Additional arguments passed to AsyncEngineArgs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        AsyncVLLM: The launched engine service.</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: If no CUDA devices are available.</span>
<span class="sd">        ValueError: If invalid device configuration is provided.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # Create a single-GPU async engine</span>
<span class="sd">        &gt;&gt;&gt; service = make_async_vllm_engine(&quot;Qwen/Qwen2.5-3B&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create a 2-GPU tensor parallel async engine with 2 replicas</span>
<span class="sd">        &gt;&gt;&gt; service = make_async_vllm_engine(&quot;Qwen/Qwen2.5-3B&quot;, num_devices=2, num_replicas=2)</span>
<span class="sd">        &gt;&gt;&gt; # Generate text</span>
<span class="sd">        &gt;&gt;&gt; result = service.generate(&quot;Hello, world!&quot;, sampling_params)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create with FP32 output enabled</span>
<span class="sd">        &gt;&gt;&gt; service = make_async_vllm_engine(&quot;Qwen/Qwen2.5-3B&quot;, enable_fp32_output=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_has_vllm</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
            <span class="s2">&quot;vllm is not installed. Please install it with `pip install vllm`.&quot;</span>
        <span class="p">)</span>

    <span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncEngineArgs</span>

    <span class="c1"># Set FP32 output environment variable if requested</span>
    <span class="k">if</span> <span class="n">enable_fp32_output</span><span class="p">:</span>
        <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;VLLM_ENABLE_FP32_OUTPUT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Enabled FP32 output for vLLM (VLLM_ENABLE_FP32_OUTPUT=1). &quot;</span>
            <span class="s2">&quot;This will use FP32 for the final output layer if the model supports it.&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Configure verbose logging if requested</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>

        <span class="c1"># Enable vLLM&#39;s throughput logging by setting the appropriate log level</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;vllm.engine.metrics&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;vllm&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

        <span class="c1"># vLLM logs throughput stats at INFO level every few seconds</span>
        <span class="c1"># The stats include: prompt throughput, generation throughput, running/pending requests, GPU KV cache usage</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;Enabled verbose vLLM logging - throughput statistics will be displayed&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Set tensor_parallel_size to num_devices if not set</span>
    <span class="k">if</span> <span class="n">tensor_parallel_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">num_devices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tensor_parallel_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tensor_parallel_size</span> <span class="o">=</span> <span class="n">num_devices</span>
    <span class="k">elif</span> <span class="n">num_devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tensor_parallel_size</span> <span class="o">!=</span> <span class="n">num_devices</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;tensor_parallel_size must be set to </span><span class="si">{</span><span class="n">num_devices</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">data_parallel_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">data_parallel_size</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">pipeline_parallel_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pipeline_parallel_size</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Create engine args</span>
    <span class="n">kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;distributed_executor_backend&quot;</span><span class="p">,</span> <span class="s2">&quot;ray&quot;</span><span class="p">)</span>
    <span class="c1"># Don&#39;t explicitly set enable_prefix_caching to avoid conflicts</span>
    <span class="n">kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;enable_prefix_caching&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Set compilation flag - this controls whether vLLM will compile the model for better performance</span>
    <span class="c1"># Disabled by default in GRPO since it can cause issues during training</span>
    <span class="k">if</span> <span class="s2">&quot;compilation_config&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">compile</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;compilation_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;compilation_config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>

    <span class="n">engine_args</span> <span class="o">=</span> <span class="n">AsyncEngineArgs</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="n">tensor_parallel_size</span><span class="p">,</span>
        <span class="n">data_parallel_size</span><span class="o">=</span><span class="n">data_parallel_size</span><span class="p">,</span>
        <span class="n">pipeline_parallel_size</span><span class="o">=</span><span class="n">pipeline_parallel_size</span><span class="p">,</span>
        <span class="n">worker_extension_cls</span><span class="o">=</span><span class="s2">&quot;torchrl.modules.llm.backends.vllm.vllm_async._AsyncvLLMWorker&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">engine_args</span><span class="p">,</span> <span class="n">num_replicas</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../../../" src="../../../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../../../" id="documentation_options" src="../../../../../../_static/documentation_options.js"></script>
         <script src="../../../../../../_static/jquery.js"></script>
         <script src="../../../../../../_static/underscore.js"></script>
         <script src="../../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../../../_static/doctools.js"></script>
         <script src="../../../../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>