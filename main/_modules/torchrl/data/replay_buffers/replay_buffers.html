


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchrl.data.replay_buffers.replay_buffers &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../../../../versions.html"><span style="font-size:110%">main (0.0.0+unknown) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/export.html">Exporting TorchRL modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/llm_browser.html">TorchRL LLM: Building Tool-Enabled Environments</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
      <li>torchrl.data.replay_buffers.replay_buffers</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
    
    
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchrl.data.replay_buffers.replay_buffers</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the MIT license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">collections</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">contextlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">multiprocessing</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">textwrap</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">threading</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">concurrent.futures</span><span class="w"> </span><span class="kn">import</span> <span class="n">ThreadPoolExecutor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torch.compiler</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_compiling</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torch._dynamo</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_compiling</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span><span class="p">,</span> <span class="n">wraps</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">TYPE_CHECKING</span><span class="p">,</span> <span class="n">TypeVar</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">is_tensor_collection</span><span class="p">,</span>
    <span class="n">is_tensorclass</span><span class="p">,</span>
    <span class="n">LazyStackedTensorDict</span><span class="p">,</span>
    <span class="n">NestedKey</span><span class="p">,</span>
    <span class="n">TensorDict</span><span class="p">,</span>
    <span class="n">TensorDictBase</span><span class="p">,</span>
    <span class="n">unravel_key</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict.nn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_set_dispatch_td_nn_modules</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">expand_as_right</span><span class="p">,</span> <span class="n">expand_right</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils._pytree</span><span class="w"> </span><span class="kn">import</span> <span class="n">tree_map</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl._utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">accept_remote_rref_udf_invocation</span><span class="p">,</span> <span class="n">RL_WARNINGS</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.replay_buffers.samplers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">PrioritizedSampler</span><span class="p">,</span>
    <span class="n">RandomSampler</span><span class="p">,</span>
    <span class="n">Sampler</span><span class="p">,</span>
    <span class="n">SamplerEnsemble</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.replay_buffers.storages</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_get_default_collate</span><span class="p">,</span>
    <span class="n">_stack_anything</span><span class="p">,</span>
    <span class="n">ListStorage</span><span class="p">,</span>
    <span class="n">Storage</span><span class="p">,</span>
    <span class="n">StorageEnsemble</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.replay_buffers.utils</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">_is_int</span><span class="p">,</span>
    <span class="n">_reduce</span><span class="p">,</span>
    <span class="n">_to_numpy</span><span class="p">,</span>
    <span class="n">_to_torch</span><span class="p">,</span>
    <span class="n">INT_CLASSES</span><span class="p">,</span>
    <span class="n">pin_memory_output</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.replay_buffers.writers</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">RoundRobinWriter</span><span class="p">,</span>
    <span class="n">TensorDictRoundRobinWriter</span><span class="p">,</span>
    <span class="n">Writer</span><span class="p">,</span>
    <span class="n">WriterEnsemble</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">DEVICE_TYPING</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.transforms.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">_InvertTransform</span><span class="p">,</span> <span class="n">Transform</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">TypeVar</span><span class="p">(</span><span class="s2">&quot;T&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Self</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">Self</span> <span class="o">=</span> <span class="n">T</span>


<span class="k">def</span><span class="w"> </span><span class="nf">_maybe_delay_init</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delayed_init</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">wrapper</span>


<div class="viewcode-block" id="ReplayBuffer"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">ReplayBuffer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A generic, composable replay buffer class.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        storage (Storage, Callable[[], Storage], optional): the storage to be used.</span>
<span class="sd">            If a callable is passed, it is used as constructor for the storage.</span>
<span class="sd">            If none is provided a default :class:`~torchrl.data.replay_buffers.ListStorage` with</span>
<span class="sd">            ``max_size`` of ``1_000`` will be created.</span>
<span class="sd">        sampler (Sampler, Callable[[], Sampler], optional): the sampler to be used.</span>
<span class="sd">            If a callable is passed, it is used as constructor for the sampler.</span>
<span class="sd">            If none is provided, a default :class:`~torchrl.data.replay_buffers.RandomSampler`</span>
<span class="sd">            will be used.</span>
<span class="sd">        writer (Writer, Callable[[], Writer], optional): the writer to be used.</span>
<span class="sd">            If a callable is passed, it is used as constructor for the writer.</span>
<span class="sd">            If none is provided a default :class:`~torchrl.data.replay_buffers.RoundRobinWriter`</span>
<span class="sd">            will be used.</span>
<span class="sd">        collate_fn (callable, optional): merges a list of samples to form a</span>
<span class="sd">            mini-batch of Tensor(s)/outputs.  Used when using batched</span>
<span class="sd">            loading from a map-style dataset. The default value will be decided</span>
<span class="sd">            based on the storage type.</span>
<span class="sd">        pin_memory (bool): whether pin_memory() should be called on the rb</span>
<span class="sd">            samples.</span>
<span class="sd">        prefetch (int, optional): number of next batches to be prefetched</span>
<span class="sd">            using multithreading. Defaults to None (no prefetching).</span>
<span class="sd">        transform (Transform or Callable[[Any], Any], optional): Transform to be executed when</span>
<span class="sd">            :meth:`sample` is called.</span>
<span class="sd">            To chain transforms use the :class:`~torchrl.envs.Compose` class.</span>
<span class="sd">            Transforms should be used with :class:`tensordict.TensorDict`</span>
<span class="sd">            content. A generic callable can also be passed if the replay buffer</span>
<span class="sd">            is used with PyTree structures (see example below).</span>
<span class="sd">            Unlike storages, writers and samplers, transform constructors must</span>
<span class="sd">            be passed as separate keyword argument :attr:`transform_factory`,</span>
<span class="sd">            as it is impossible to distinguish a constructor from a transform.</span>
<span class="sd">        transform_factory (Callable[[], Callable], optional): a factory for the</span>
<span class="sd">            transform. Exclusive with :attr:`transform`.</span>
<span class="sd">        batch_size (int, optional): the batch size to be used when sample() is</span>
<span class="sd">            called.</span>

<span class="sd">            .. note::</span>
<span class="sd">              The batch-size can be specified at construction time via the</span>
<span class="sd">              ``batch_size`` argument, or at sampling time. The former should</span>
<span class="sd">              be preferred whenever the batch-size is consistent across the</span>
<span class="sd">              experiment. If the batch-size is likely to change, it can be</span>
<span class="sd">              passed to the :meth:`sample` method. This option is</span>
<span class="sd">              incompatible with prefetching (since this requires to know the</span>
<span class="sd">              batch-size in advance) as well as with samplers that have a</span>
<span class="sd">              ``drop_last`` argument.</span>

<span class="sd">        dim_extend (int, optional): indicates the dim to consider for</span>
<span class="sd">            extension when calling :meth:`extend`. Defaults to ``storage.ndim-1``.</span>
<span class="sd">            When using ``dim_extend &gt; 0``, we recommend using the ``ndim``</span>
<span class="sd">            argument in the storage instantiation if that argument is</span>
<span class="sd">            available, to let storages know that the data is</span>
<span class="sd">            multi-dimensional and keep consistent notions of storage-capacity</span>
<span class="sd">            and batch-size during sampling.</span>

<span class="sd">            .. note:: This argument has no effect on :meth:`add` and</span>
<span class="sd">                therefore should be used with caution when both :meth:`add`</span>
<span class="sd">                and :meth:`extend` are used in a codebase. For example:</span>

<span class="sd">                    &gt;&gt;&gt; data = torch.zeros(3, 4)</span>
<span class="sd">                    &gt;&gt;&gt; rb = ReplayBuffer(</span>
<span class="sd">                    ...     storage=LazyTensorStorage(10, ndim=2),</span>
<span class="sd">                    ...     dim_extend=1)</span>
<span class="sd">                    &gt;&gt;&gt; # these two approaches are equivalent:</span>
<span class="sd">                    &gt;&gt;&gt; for d in data.unbind(1):</span>
<span class="sd">                    ...     rb.add(d)</span>
<span class="sd">                    &gt;&gt;&gt; rb.extend(data)</span>

<span class="sd">        generator (torch.Generator, optional): a generator to use for sampling.</span>
<span class="sd">            Using a dedicated generator for the replay buffer can allow a fine-grained control</span>
<span class="sd">            over seeding, for instance keeping the global seed different but the RB seed identical</span>
<span class="sd">            for distributed jobs.</span>
<span class="sd">            Defaults to ``None`` (global default generator).</span>

<span class="sd">            .. warning:: As of now, the generator has no effect on the transforms.</span>
<span class="sd">        shared (bool, optional): whether the buffer will be shared using multiprocessing or not.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        compilable (bool, optional): whether the writer is compilable.</span>
<span class="sd">            If ``True``, the writer cannot be shared between multiple processes.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        delayed_init (bool, optional): whether to initialize storage, writer, sampler and transform</span>
<span class="sd">            the first time the buffer is used rather than during construction.</span>
<span class="sd">            This is useful when the replay buffer needs to be pickled and sent to remote workers,</span>
<span class="sd">            particularly when using transforms with modules that require gradients.</span>
<span class="sd">            If not specified, defaults to ``True`` when ``transform_factory`` is provided,</span>
<span class="sd">            and ``False`` otherwise.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data import ReplayBuffer, ListStorage</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt; rb = ReplayBuffer(</span>
<span class="sd">        ...     storage=ListStorage(max_size=1000),</span>
<span class="sd">        ...     batch_size=5,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; # populate the replay buffer and get the item indices</span>
<span class="sd">        &gt;&gt;&gt; data = range(10)</span>
<span class="sd">        &gt;&gt;&gt; indices = rb.extend(data)</span>
<span class="sd">        &gt;&gt;&gt; # sample will return as many elements as specified in the constructor</span>
<span class="sd">        &gt;&gt;&gt; sample = rb.sample()</span>
<span class="sd">        &gt;&gt;&gt; print(sample)</span>
<span class="sd">        tensor([4, 9, 3, 0, 3])</span>
<span class="sd">        &gt;&gt;&gt; # Passing the batch-size to the sample method overrides the one in the constructor</span>
<span class="sd">        &gt;&gt;&gt; sample = rb.sample(batch_size=3)</span>
<span class="sd">        &gt;&gt;&gt; print(sample)</span>
<span class="sd">        tensor([9, 7, 3])</span>
<span class="sd">        &gt;&gt;&gt; # one cans sample using the ``sample`` method or iterate over the buffer</span>
<span class="sd">        &gt;&gt;&gt; for i, batch in enumerate(rb):</span>
<span class="sd">        ...     print(i, batch)</span>
<span class="sd">        ...     if i == 3:</span>
<span class="sd">        ...         break</span>
<span class="sd">        0 tensor([7, 3, 1, 6, 6])</span>
<span class="sd">        1 tensor([9, 8, 6, 6, 8])</span>
<span class="sd">        2 tensor([4, 3, 6, 9, 1])</span>
<span class="sd">        3 tensor([4, 4, 1, 9, 9])</span>

<span class="sd">    Replay buffers accept *any* kind of data. Not all storage types</span>
<span class="sd">    will work, as some expect numerical data only, but the default</span>
<span class="sd">    :class:`~torchrl.data.ListStorage` will:</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt; buffer = ReplayBuffer(storage=ListStorage(100), collate_fn=lambda x: x)</span>
<span class="sd">        &gt;&gt;&gt; indices = buffer.extend([&quot;a&quot;, 1, None])</span>
<span class="sd">        &gt;&gt;&gt; buffer.sample(3)</span>
<span class="sd">        [None, &#39;a&#39;, None]</span>

<span class="sd">    The :class:`~torchrl.data.replay_buffers.TensorStorage`, :class:`~torchrl.data.replay_buffers.LazyMemmapStorage`</span>
<span class="sd">    and :class:`~torchrl.data.replay_buffers.LazyTensorStorage` also work</span>
<span class="sd">    with any PyTree structure (a PyTree is a nested structure of arbitrary depth made of dicts,</span>
<span class="sd">    lists or tuples where the leaves are tensors) provided that it only contains</span>
<span class="sd">    tensor data.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from torch.utils._pytree import tree_map</span>
<span class="sd">        &gt;&gt;&gt; def transform(x):</span>
<span class="sd">        ...     # Zeros all the data in the pytree</span>
<span class="sd">        ...     return tree_map(lambda y: y * 0, x)</span>
<span class="sd">        &gt;&gt;&gt; rb = ReplayBuffer(storage=LazyMemmapStorage(100), transform=transform)</span>
<span class="sd">        &gt;&gt;&gt; data = {</span>
<span class="sd">        ...     &quot;a&quot;: torch.randn(3),</span>
<span class="sd">        ...     &quot;b&quot;: {&quot;c&quot;: (torch.zeros(2), [torch.ones(1)])},</span>
<span class="sd">        ...     30: -torch.ones(()),</span>
<span class="sd">        ... }</span>
<span class="sd">        &gt;&gt;&gt; rb.add(data)</span>
<span class="sd">        &gt;&gt;&gt; # The sample has a similar structure to the data (with a leading dimension of 10 for each tensor)</span>
<span class="sd">        &gt;&gt;&gt; s = rb.sample(10)</span>
<span class="sd">        &gt;&gt;&gt; # let&#39;s check that our transform did its job:</span>
<span class="sd">        &gt;&gt;&gt; def assert0(x):</span>
<span class="sd">        &gt;&gt;&gt;     assert (x == 0).all()</span>
<span class="sd">        &gt;&gt;&gt; tree_map(assert0, s)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Storage</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sampler</span><span class="p">:</span> <span class="n">Sampler</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Sampler</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">writer</span><span class="p">:</span> <span class="n">Writer</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Writer</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">prefetch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">Transform</span> <span class="o">|</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa-F821</span>
        <span class="n">transform_factory</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Transform</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">]</span>
        <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa-F821</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dim_extend</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">checkpointer</span><span class="p">:</span> <span class="n">StorageCheckpointerBase</span>  <span class="c1"># noqa: F821</span>
        <span class="o">|</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">StorageCheckpointerBase</span><span class="p">]</span>  <span class="c1"># noqa: F821</span>
        <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: F821</span>
        <span class="n">generator</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shared</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">compilable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">delayed_init</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_delayed_init</span> <span class="o">=</span> <span class="n">delayed_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Store init parameters for potential delayed initialization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_storage</span> <span class="o">=</span> <span class="n">storage</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_sampler</span> <span class="o">=</span> <span class="n">sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_writer</span> <span class="o">=</span> <span class="n">writer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_collate_fn</span> <span class="o">=</span> <span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_transform</span> <span class="o">=</span> <span class="n">transform</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_transform_factory</span> <span class="o">=</span> <span class="n">transform_factory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_checkpointer</span> <span class="o">=</span> <span class="n">checkpointer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_generator</span> <span class="o">=</span> <span class="n">generator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_compilable</span> <span class="o">=</span> <span class="n">compilable</span>

        <span class="k">if</span> <span class="n">transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">transform_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;transform and transform_factory are mutually exclusive. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Got transform=</span><span class="si">{</span><span class="n">transform</span><span class="si">}</span><span class="s2"> and transform_factory=</span><span class="si">{</span><span class="n">transform_factory</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Auto-detect delayed_init when transform_factory is provided</span>
        <span class="k">if</span> <span class="n">transform_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">delayed_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">delayed_init</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">elif</span> <span class="n">delayed_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">delayed_init</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># Update _delayed_init after auto-detection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_delayed_init</span> <span class="o">=</span> <span class="n">delayed_init</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory</span> <span class="o">=</span> <span class="n">pin_memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">prefetch</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_cap</span> <span class="o">=</span> <span class="n">prefetch</span> <span class="ow">or</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_queue</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>

        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">prefetch</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Dynamic batch-size specification is incompatible &quot;</span>
                <span class="s2">&quot;with multithreaded sampling. &quot;</span>
                <span class="s2">&quot;When using prefetch, the batch-size must be specified in &quot;</span>
                <span class="s2">&quot;advance. &quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">dim_extend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dim_extend</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dim_extend must be a positive value.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dim_extend</span> <span class="o">=</span> <span class="n">dim_extend</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_cap</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_executor</span> <span class="o">=</span> <span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_cap</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">shared</span> <span class="ow">and</span> <span class="n">prefetch</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot share prefetched replay buffers.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">shared</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">share</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_futures_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">()</span>

        <span class="c1"># If not delayed, initialize immediately</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_delayed_init</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the replay buffer components.</span>

<span class="sd">        This method is called either immediately during __init__ (if delayed_init=False)</span>
<span class="sd">        or on first use of the buffer (if delayed_init=True).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Initialize storage</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_make_storage</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_init_storage</span><span class="p">,</span> <span class="n">compilable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_compilable</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

            <span class="c1"># Initialize sampler</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_make_sampler</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_sampler</span><span class="p">)</span>

            <span class="c1"># Initialize writer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_make_writer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_writer</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">register_storage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">)</span>

            <span class="c1"># Initialize collate function</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_get_collate_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_collate_fn</span><span class="p">)</span>

            <span class="c1"># Initialize transform</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_make_transform</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_init_transform</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_transform_factory</span>
            <span class="p">)</span>

            <span class="c1"># Check batch_size compatibility with sampler</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="p">,</span> <span class="s2">&quot;drop_last&quot;</span><span class="p">)</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">drop_last</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Samplers with drop_last=True must work with a predictable batch-size. &quot;</span>
                    <span class="s2">&quot;Please pass the batch-size to the ReplayBuffer constructor.&quot;</span>
                <span class="p">)</span>

            <span class="c1"># Set dim_extend properly now that storage is initialized</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_extend</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">ndim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_dim_extend</span> <span class="o">=</span> <span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_dim_extend</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># Set checkpointer and generator</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">checkpointer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_checkpointer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_rng</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_generator</span><span class="p">)</span>

            <span class="c1"># Initialize prioritized sampler if needed</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_initialize_prioritized_sampler</span><span class="p">()</span>

            <span class="c1"># Remove init parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_storage</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_sampler</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_writer</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_collate_fn</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_transform</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_transform_factory</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_checkpointer</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_generator</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_compilable</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">raise</span> <span class="n">e</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">initialized</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Whether the replay buffer has been initialized.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_initialize_prioritized_sampler</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize priority trees for existing data when using PrioritizedSampler.</span>

<span class="sd">        This method ensures that when a PrioritizedSampler is used with storage that</span>
<span class="sd">        already contains data, the priority trees are properly populated with default</span>
<span class="sd">        priorities for all existing entries.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.samplers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PrioritizedSampler</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="p">,</span> <span class="n">PrioritizedSampler</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Set default priorities for all existing data</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
            <span class="n">default_priorities</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
                <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">),),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">default_priority</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">update_priority</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">default_priorities</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_maybe_make_storage</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Storage</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="n">compilable</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Storage</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">storage</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ListStorage</span><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="mi">1_000</span><span class="p">,</span> <span class="n">compilable</span><span class="o">=</span><span class="n">compilable</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">Storage</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">storage</span>
        <span class="k">elif</span> <span class="nb">callable</span><span class="p">(</span><span class="n">storage</span><span class="p">):</span>
            <span class="n">storage</span> <span class="o">=</span> <span class="n">storage</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">Storage</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;storage must be either a Storage or a callable returning a storage instance.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">storage</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_maybe_make_sampler</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">sampler</span><span class="p">:</span> <span class="n">Sampler</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Sampler</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sampler</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">RandomSampler</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">Sampler</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">sampler</span>
        <span class="k">elif</span> <span class="nb">callable</span><span class="p">(</span><span class="n">sampler</span><span class="p">):</span>
            <span class="n">sampler</span> <span class="o">=</span> <span class="n">sampler</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampler</span><span class="p">,</span> <span class="n">Sampler</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;sampler must be either a Sampler or a callable returning a sampler instance.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">sampler</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_maybe_make_writer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">writer</span><span class="p">:</span> <span class="n">Writer</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Writer</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Writer</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">writer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">RoundRobinWriter</span><span class="p">()</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">writer</span><span class="p">,</span> <span class="n">Writer</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">writer</span>
        <span class="k">elif</span> <span class="nb">callable</span><span class="p">(</span><span class="n">writer</span><span class="p">):</span>
            <span class="n">writer</span> <span class="o">=</span> <span class="n">writer</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">writer</span><span class="p">,</span> <span class="n">Writer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;writer must be either a Writer or a callable returning a writer instance.&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">writer</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_maybe_make_transform</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">Transform</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Transform</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">transform_factory</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transform</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.transforms.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
            <span class="n">_CallableTransform</span><span class="p">,</span>
            <span class="n">Compose</span><span class="p">,</span>
            <span class="n">Transform</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">transform_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;transform and transform_factory cannot be used simultaneously&quot;</span>
                <span class="p">)</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="n">transform_factory</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">()</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">Compose</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">Transform</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">transform</span><span class="p">):</span>
                <span class="n">transform</span> <span class="o">=</span> <span class="n">_CallableTransform</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">Transform</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;transform must be either a Transform instance or a callable.&quot;</span>
                <span class="p">)</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
        <span class="n">transform</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">transform</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">share</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shared</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared</span> <span class="o">=</span> <span class="n">shared</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_write_lock</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_write_lock</span> <span class="o">=</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_rng</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">generator</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">generator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">generator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">generator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">_rng</span> <span class="o">=</span> <span class="n">generator</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dim_extend</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_extend</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">batch_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The batch size of the replay buffer.</span>

<span class="sd">        The batch size can be overriden by setting the `batch_size` parameter in the :meth:`sample` method.</span>

<span class="sd">        It defines both the number of samples returned by :meth:`sample` and the number of samples that are</span>
<span class="sd">        yielded by the :class:`ReplayBuffer` iterator.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span>

    <span class="nd">@dim_extend</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dim_extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_dim_extend&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_extend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dim_extend</span> <span class="o">!=</span> <span class="n">value</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;dim_extend cannot be reset. Please create a new replay buffer.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">ndim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_dim_extend</span> <span class="o">=</span> <span class="n">value</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_tensor_collection</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">data</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_extend</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_extend</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_collate_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">collate_fn</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">collate_fn</span>
            <span class="k">if</span> <span class="n">collate_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">_get_default_collate</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">,</span> <span class="n">_is_tensordict</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">TensorDictReplayBuffer</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

<div class="viewcode-block" id="ReplayBuffer.set_storage"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.set_storage">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_storage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span><span class="p">,</span> <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets a new storage in the replay buffer and returns the previous storage.</span>

<span class="sd">        Args:</span>
<span class="sd">            storage (Storage): the new storage for the buffer.</span>
<span class="sd">            collate_fn (callable, optional): if provided, the collate_fn is set to this</span>
<span class="sd">                value. Otherwise it is reset to a default value.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">prev_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span> <span class="o">=</span> <span class="n">storage</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_get_collate_fn</span><span class="p">(</span><span class="n">collate_fn</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">prev_storage</span></div>

<div class="viewcode-block" id="ReplayBuffer.set_writer"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.set_writer">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_writer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">writer</span><span class="p">:</span> <span class="n">Writer</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets a new writer in the replay buffer and returns the previous writer.&quot;&quot;&quot;</span>
        <span class="n">prev_writer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span> <span class="o">=</span> <span class="n">writer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">register_storage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prev_writer</span></div>

<div class="viewcode-block" id="ReplayBuffer.set_sampler"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.set_sampler">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_sampler</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sampler</span><span class="p">:</span> <span class="n">Sampler</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets a new sampler in the replay buffer and returns the previous sampler.&quot;&quot;&quot;</span>
        <span class="n">prev_sampler</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span> <span class="o">=</span> <span class="n">sampler</span>
        <span class="k">return</span> <span class="n">prev_sampler</span></div>

    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
        <span class="c1"># To access properties in remote settings, see RayReplayBuffer.write_count for instance</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="c1"># To set properties in remote settings</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>  <span class="c1"># explicit return for remote calls</span>

    <span class="nd">@property</span>
    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">write_count</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The total number of items written so far in the buffer through add and extend.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">_write_count</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">Compose</span>

        <span class="n">storage</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;storage=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_storage&#39;</span><span class="p">,</span><span class="w"> </span><span class="kc">None</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">writer</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;writer=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_writer&#39;</span><span class="p">,</span><span class="w"> </span><span class="kc">None</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sampler=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_sampler&#39;</span><span class="p">,</span><span class="w"> </span><span class="kc">None</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_transform&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">,</span> <span class="n">Compose</span><span class="p">)</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_transform&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;transform=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_transform&#39;</span><span class="p">,</span><span class="w"> </span><span class="kc">None</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span>
            <span class="p">)</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="si">}</span><span class="s2">, &quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;batch_size=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_batch_size&#39;</span><span class="p">,</span><span class="w"> </span><span class="kc">None</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="p">)</span>
        <span class="n">collate_fn</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;collate_fn=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_collate_fn&#39;</span><span class="p">,</span><span class="w"> </span><span class="kc">None</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="se">\n</span><span class="si">{</span><span class="n">storage</span><span class="si">}</span><span class="s2">, </span><span class="se">\n</span><span class="si">{</span><span class="n">sampler</span><span class="si">}</span><span class="s2">, </span><span class="se">\n</span><span class="si">{</span><span class="n">writer</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">transform</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">, </span><span class="se">\n</span><span class="si">{</span><span class="n">collate_fn</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="nd">@_maybe_delay_init</span>
    <span class="nd">@pin_memory_output</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">NestedKey</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="n">unravel_key</span><span class="p">(</span><span class="n">index</span><span class="p">)):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">[:][</span><span class="n">index</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="p">[:][</span><span class="n">index</span><span class="p">]</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">_to_numpy</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_extend</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_extend</span> <span class="o">+</span> <span class="p">(</span><span class="n">index</span><span class="p">,)</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">INT_CLASSES</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">data</span><span class="o">.</span><span class="n">unlock_</span><span class="p">()</span> <span class="k">if</span> <span class="n">is_tensor_collection</span><span class="p">(</span>
                <span class="n">data</span>
            <span class="p">)</span> <span class="k">else</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">():</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">data</span>

    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="n">unravel_key</span><span class="p">(</span><span class="n">index</span><span class="p">)):</span>
            <span class="bp">self</span><span class="p">[:][</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">[:][</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">return</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">_to_numpy</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">):</span>
            <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_extend</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_extend</span> <span class="o">+</span> <span class="p">(</span><span class="n">index</span><span class="p">,)</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">return</span>

    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;_storage&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;_sampler&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;_writer&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;_transforms&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s2">&quot;_batch_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">,</span>
            <span class="s2">&quot;_rng&quot;</span><span class="p">:</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">}</span>

    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_storage&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_sampler&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_writer&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_transforms&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;_batch_size&quot;</span><span class="p">]</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_rng&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rng</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">rng</span>
            <span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">rng</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_rng</span><span class="p">(</span><span class="n">generator</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

<div class="viewcode-block" id="ReplayBuffer.dumps"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.dumps">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Saves the replay buffer on disk at the specified path.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (Path or str): path where to save the replay buffer.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import tempfile</span>
<span class="sd">            &gt;&gt;&gt; import tqdm</span>
<span class="sd">            &gt;&gt;&gt; from torchrl.data import LazyMemmapStorage, TensorDictReplayBuffer</span>
<span class="sd">            &gt;&gt;&gt; from torchrl.data.replay_buffers.samplers import PrioritizedSampler, RandomSampler</span>
<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">            &gt;&gt;&gt; # Build and populate the replay buffer</span>
<span class="sd">            &gt;&gt;&gt; S = 1_000_000</span>
<span class="sd">            &gt;&gt;&gt; sampler = PrioritizedSampler(S, 1.1, 1.0)</span>
<span class="sd">            &gt;&gt;&gt; # sampler = RandomSampler()</span>
<span class="sd">            &gt;&gt;&gt; storage = LazyMemmapStorage(S)</span>
<span class="sd">            &gt;&gt;&gt; rb = TensorDictReplayBuffer(storage=storage, sampler=sampler)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; for _ in tqdm.tqdm(range(100)):</span>
<span class="sd">            ...     td = TensorDict({&quot;obs&quot;: torch.randn(100, 3, 4), &quot;next&quot;: {&quot;obs&quot;: torch.randn(100, 3, 4)}, &quot;td_error&quot;: torch.rand(100)}, [100])</span>
<span class="sd">            ...     rb.extend(td)</span>
<span class="sd">            ...     sample = rb.sample(32)</span>
<span class="sd">            ...     rb.update_tensordict_priority(sample)</span>
<span class="sd">            &gt;&gt;&gt; # save and load the buffer</span>
<span class="sd">            &gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:</span>
<span class="sd">            ...     rb.dumps(tmpdir)</span>
<span class="sd">            ...</span>
<span class="sd">            ...     sampler = PrioritizedSampler(S, 1.1, 1.0)</span>
<span class="sd">            ...     # sampler = RandomSampler()</span>
<span class="sd">            ...     storage = LazyMemmapStorage(S)</span>
<span class="sd">            ...     rb_load = TensorDictReplayBuffer(storage=storage, sampler=sampler)</span>
<span class="sd">            ...     rb_load.loads(tmpdir)</span>
<span class="sd">            ...     assert len(rb) == len(rb_load)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="n">path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;storage&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sampler&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;writer&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rng_state</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">(</span>
                <span class="n">rng_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">rng_state</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;rng_state&quot;</span><span class="p">)</span>

        <span class="c1"># fall back on state_dict for transforms</span>
        <span class="n">transform_sd</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">transform_sd</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">transform_sd</span><span class="p">,</span> <span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;transform.t&quot;</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;buffer_metadata.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">({</span><span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">},</span> <span class="n">file</span><span class="p">)</span></div>

<div class="viewcode-block" id="ReplayBuffer.loads"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.loads">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">loads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Loads a replay buffer state at the given path.</span>

<span class="sd">        The buffer should have matching components and be saved using :meth:`dumps`.</span>

<span class="sd">        Args:</span>
<span class="sd">            path (Path or str): path where the replay buffer was saved.</span>

<span class="sd">        See :meth:`dumps` for more info.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;storage&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;sampler&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;writer&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;rng_state&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
            <span class="n">rng_state</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="o">.</span><span class="n">load_memmap</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;rng_state&quot;</span><span class="p">)</span>
            <span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">rng_state</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">rng</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">rng_state</span><span class="p">[</span><span class="s2">&quot;rng_state&quot;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
        <span class="c1"># fall back on state_dict for transforms</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;transform.t&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;transform.t&quot;</span><span class="p">))</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;buffer_metadata.json&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">metadata</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span></div>

<div class="viewcode-block" id="ReplayBuffer.save"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.save">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Alias for :meth:`dumps`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ReplayBuffer.dump"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.dump">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">dump</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Alias for :meth:`dumps`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ReplayBuffer.load"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.load">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Alias for :meth:`loads`.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="ReplayBuffer.register_save_hook"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.register_save_hook">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_save_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Any</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Registers a save hook for the storage.</span>

<span class="sd">        .. note:: Hooks are currently not serialized when saving a replay buffer: they must</span>
<span class="sd">            be manually re-initialized every time the buffer is created.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">register_save_hook</span><span class="p">(</span><span class="n">hook</span><span class="p">)</span></div>

<div class="viewcode-block" id="ReplayBuffer.register_load_hook"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.register_load_hook">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_load_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Any</span><span class="p">],</span> <span class="n">Any</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Registers a load hook for the storage.</span>

<span class="sd">        .. note:: Hooks are currently not serialized when saving a replay buffer: they must</span>
<span class="sd">            be manually re-initialized every time the buffer is created.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">register_load_hook</span><span class="p">(</span><span class="n">hook</span><span class="p">)</span></div>

<div class="viewcode-block" id="ReplayBuffer.add"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.add">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a single element to the replay buffer.</span>

<span class="sd">        Args:</span>
<span class="sd">            data (Any): data to be added to the replay buffer</span>

<span class="sd">        Returns:</span>
<span class="sd">            index where the data lives in the replay buffer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">_set_dispatch_td_nn_modules</span><span class="p">(</span><span class="n">is_tensor_collection</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
                <span class="n">make_none</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="c1"># Transforms usually expect a time batch dimension when called within a RB, so we unsqueeze the data temporarily</span>
                <span class="n">is_tc</span> <span class="o">=</span> <span class="n">is_tensor_collection</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="n">cm</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_tc</span> <span class="k">else</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="n">new_data</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">with</span> <span class="n">cm</span> <span class="k">as</span> <span class="n">data_unsq</span><span class="p">:</span>
                    <span class="n">data_unsq_r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">data_unsq</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">is_tc</span> <span class="ow">and</span> <span class="n">data_unsq_r</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="c1"># this is a no-op whenever the result matches the input</span>
                        <span class="n">new_data</span> <span class="o">=</span> <span class="n">data_unsq_r</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">make_none</span> <span class="o">=</span> <span class="n">data_unsq_r</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">new_data</span> <span class="k">if</span> <span class="n">new_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">data</span>
                <span class="k">if</span> <span class="n">make_none</span><span class="p">:</span>
                    <span class="n">data</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">RL_WARNINGS</span> <span class="ow">and</span> <span class="n">is_tensor_collection</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="ow">and</span> <span class="n">data</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Using `add()` with a TensorDict that has batch_size=</span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Use `extend()` to add multiple elements, or `add()` with a single element (batch_size=torch.Size([])). &quot;</span>
                <span class="s2">&quot;You can silence this warning by setting the `RL_WARNINGS` environment variable to `&#39;0&#39;`.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_lock</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">index</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_extend</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">update_priority</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">is_comp</span> <span class="o">=</span> <span class="n">is_compiling</span><span class="p">()</span>
        <span class="n">nc</span> <span class="o">=</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_comp</span> <span class="k">else</span> <span class="n">nc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_lock</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_comp</span> <span class="k">else</span> <span class="n">nc</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_extend</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">index</span>

<div class="viewcode-block" id="ReplayBuffer.extend"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.extend">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">extend</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">update_priority</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extends the replay buffer with one or more elements contained in an iterable.</span>

<span class="sd">        If present, the inverse transforms will be called.`</span>

<span class="sd">        Args:</span>
<span class="sd">            data (iterable): collection of data to be added to the replay</span>
<span class="sd">                buffer.</span>

<span class="sd">        Keyword Args:</span>
<span class="sd">            update_priority (bool, optional): Whether to update the priority of the data. Defaults to True.</span>
<span class="sd">                Without effect in this class. See :meth:`~torchrl.data.TensorDictReplayBuffer.extend` for more details.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Indices of the data added to the replay buffer.</span>

<span class="sd">        .. warning:: :meth:`~torchrl.data.replay_buffers.ReplayBuffer.extend` can have an</span>
<span class="sd">          ambiguous signature when dealing with lists of values, which should be interpreted</span>
<span class="sd">          either as PyTree (in which case all elements in the list will be put in a slice</span>
<span class="sd">          in the stored PyTree in the storage) or a list of values to add one at a time.</span>
<span class="sd">          To solve this, TorchRL makes the clear-cut distinction between list and tuple:</span>
<span class="sd">          a tuple will be viewed as a PyTree, a list (at the root level) will be interpreted</span>
<span class="sd">          as a stack of values to add one at a time to the buffer.</span>
<span class="sd">          For :class:`~torchrl.data.replay_buffers.ListStorage` instances, only</span>
<span class="sd">          unbound elements can be provided (no PyTrees).</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">update_priority</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;update_priority is not supported in this class. See :meth:`~torchrl.data.TensorDictReplayBuffer.extend` for more details.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">_set_dispatch_td_nn_modules</span><span class="p">(</span><span class="n">is_tensor_collection</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_extend</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">update_priority</span><span class="o">=</span><span class="n">update_priority</span><span class="p">)</span></div>

    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">update_priority</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">priority</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">priority</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_extend</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">priority</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">priority</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose</span><span class="p">(</span><span class="n">priority</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="c1"># priority = priority.flatten()</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_lock</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">update_priority</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">priority</span><span class="p">,</span> <span class="n">storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">)</span>

    <span class="nd">@pin_memory_output</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="n">is_comp</span> <span class="o">=</span> <span class="n">is_compiling</span><span class="p">()</span>
        <span class="n">nc</span> <span class="o">=</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_comp</span> <span class="k">else</span> <span class="n">nc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_lock</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_comp</span> <span class="k">else</span> <span class="n">nc</span><span class="p">:</span>
            <span class="n">index</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="n">info</span><span class="p">[</span><span class="s2">&quot;index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">INT_CLASSES</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">):</span>
            <span class="n">is_td</span> <span class="o">=</span> <span class="n">is_tensor_collection</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">data</span><span class="o">.</span><span class="n">unlock_</span><span class="p">()</span> <span class="k">if</span> <span class="n">is_td</span> <span class="k">else</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">(),</span> <span class="n">_set_dispatch_td_nn_modules</span><span class="p">(</span>
                <span class="n">is_td</span>
            <span class="p">):</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">info</span>

<div class="viewcode-block" id="ReplayBuffer.empty"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.empty">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">empty</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">empty_write_count</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Empties the replay buffer and reset cursor to 0.</span>

<span class="sd">        Args:</span>
<span class="sd">            empty_write_count (bool, optional): Whether to empty the write_count attribute. Defaults to `True`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="o">.</span><span class="n">_empty</span><span class="p">(</span><span class="n">empty_write_count</span><span class="o">=</span><span class="n">empty_write_count</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">_empty</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">_empty</span><span class="p">()</span></div>

<div class="viewcode-block" id="ReplayBuffer.sample"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.sample">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">return_info</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Samples a batch of data from the replay buffer.</span>

<span class="sd">        Uses Sampler to sample indices, and retrieves them from Storage.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_size (int, optional): size of data to be collected. If none</span>
<span class="sd">                is provided, this method will sample a batch-size as indicated</span>
<span class="sd">                by the sampler.</span>
<span class="sd">            return_info (bool): whether to return info. If True, the result</span>
<span class="sd">                is a tuple (data, info). If False, the result is the data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A batch of data selected in the replay buffer.</span>
<span class="sd">            A tuple containing this batch and info if return_info flag is set to True.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">batch_size</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span>
        <span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Got conflicting batch_sizes in constructor (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="si">}</span><span class="s2">) &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;and `sample` (</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">). Refer to the ReplayBuffer documentation &quot;</span>
                <span class="s2">&quot;for a proper usage of the batch-size arguments. &quot;</span>
                <span class="s2">&quot;The batch-size provided to the sample method &quot;</span>
                <span class="s2">&quot;will prevail.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span>
        <span class="k">elif</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;batch_size not specified. You can specify the batch_size when &quot;</span>
                <span class="s2">&quot;constructing the replay buffer, or pass it to the sample method. &quot;</span>
                <span class="s2">&quot;Refer to the ReplayBuffer documentation &quot;</span>
                <span class="s2">&quot;for a proper usage of the batch-size arguments.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch</span><span class="p">:</span>
            <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sample</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_futures_lock</span><span class="p">:</span>
                <span class="k">while</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_queue</span><span class="p">)</span>
                    <span class="o">&lt;</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">_remaining_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_cap</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">ran_out</span>
                <span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_queue</span><span class="p">):</span>
                    <span class="n">fut</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sample</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_queue</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fut</span><span class="p">)</span>
                <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_queue</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">return_info</span><span class="p">:</span>
            <span class="n">out</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">result</span>
            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="o">.</span><span class="n">device</span>
                <span class="n">info</span> <span class="o">=</span> <span class="n">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;to&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">info</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>

    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">mark_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">mark_update</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">storage</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">)</span>

<div class="viewcode-block" id="ReplayBuffer.append_transform"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.append_transform">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">append_transform</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">transform</span><span class="p">:</span> <span class="n">Transform</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">invert</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># noqa-F821</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ReplayBuffer</span><span class="p">:</span>  <span class="c1"># noqa: D417</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Appends transform at the end.</span>

<span class="sd">        Transforms are applied in order when `sample` is called.</span>

<span class="sd">        Args:</span>
<span class="sd">            transform (Transform): The transform to be appended</span>

<span class="sd">        Keyword Args:</span>
<span class="sd">            invert (bool, optional): if ``True``, the transform will be inverted (forward calls will be called</span>
<span class="sd">                during writing and inverse calls during reading). Defaults to ``False``.</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; rb = ReplayBuffer(storage=LazyMemmapStorage(10), batch_size=4)</span>
<span class="sd">            &gt;&gt;&gt; data = TensorDict({&quot;a&quot;: torch.zeros(10)}, [10])</span>
<span class="sd">            &gt;&gt;&gt; def t(data):</span>
<span class="sd">            ...     data += 1</span>
<span class="sd">            ...     return data</span>
<span class="sd">            &gt;&gt;&gt; rb.append_transform(t, invert=True)</span>
<span class="sd">            &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">            &gt;&gt;&gt; assert (data == 1).all()</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.transforms.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">_CallableTransform</span><span class="p">,</span> <span class="n">Transform</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">Transform</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">callable</span><span class="p">(</span><span class="n">transform</span><span class="p">):</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="n">_CallableTransform</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">invert</span><span class="p">:</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="n">_InvertTransform</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
        <span class="n">transform</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="ReplayBuffer.insert_transform"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.insert_transform">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">insert_transform</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">Transform</span><span class="p">,</span>  <span class="c1"># noqa-F821</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">invert</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ReplayBuffer</span><span class="p">:</span>  <span class="c1"># noqa: D417</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Inserts transform.</span>

<span class="sd">        Transforms are executed in order when `sample` is called.</span>

<span class="sd">        Args:</span>
<span class="sd">            index (int): Position to insert the transform.</span>
<span class="sd">            transform (Transform): The transform to be appended</span>

<span class="sd">        Keyword Args:</span>
<span class="sd">            invert (bool, optional): if ``True``, the transform will be inverted (forward calls will be called</span>
<span class="sd">                during writing and inverse calls during reading). Defaults to ``False``.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">transform</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">invert</span><span class="p">:</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="n">_InvertTransform</span><span class="p">(</span><span class="n">transform</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">transform</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="n">_iterator</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="ReplayBuffer.next"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.ReplayBuffer.html#torchrl.data.ReplayBuffer.next">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">next</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the next item in the replay buffer.</span>

<span class="sd">        This method is used to iterate over the replay buffer in contexts where __iter__ is not available,</span>
<span class="sd">        such as :class:`~torchrl.data.replay_buffers.RayReplayBuffer`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span><span class="p">)</span>
            <span class="c1"># if any, we don&#39;t want the device ref to be passed in distributed settings</span>
            <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">clear_device_</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">out</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_iterator</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">return</span> <span class="kc">None</span></div>

    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">ran_out</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">ran_out</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot iterate over the replay buffer. &quot;</span>
                <span class="s2">&quot;Batch_size was not specified during construction of the replay buffer.&quot;</span>
            <span class="p">)</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">ran_out</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_queue</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_rng&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rng_state</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">(</span>
                <span class="n">rng_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_rng&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rng_state</span>
        <span class="n">_replay_lock</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_replay_lock&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">_futures_lock</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_futures_lock&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_replay_lock</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_replay_lock_placeholder&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">_futures_lock</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_futures_lock_placeholder&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
        <span class="n">rngstate</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="s2">&quot;_rng&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">rngstate</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_rng&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">rngstate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">rng</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">rngstate</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">rng</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">rngstate</span><span class="p">[</span><span class="s2">&quot;rng_state&quot;</span><span class="p">])</span>

        <span class="k">if</span> <span class="s2">&quot;_replay_lock_placeholder&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_replay_lock_placeholder&quot;</span><span class="p">)</span>
            <span class="n">_replay_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">()</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_replay_lock&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_replay_lock</span>
        <span class="k">if</span> <span class="s2">&quot;_futures_lock_placeholder&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="n">state</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;_futures_lock_placeholder&quot;</span><span class="p">)</span>
            <span class="n">_futures_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">RLock</span><span class="p">()</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;_futures_lock&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_futures_lock</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rngstate</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_rng</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">sampler</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Sampler</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The sampler of the replay buffer.</span>

<span class="sd">        The sampler must be an instance of :class:`~torchrl.data.replay_buffers.Sampler`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span>

    <span class="nd">@property</span>
    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">writer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Writer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The writer of the replay buffer.</span>

<span class="sd">        The writer must be an instance of :class:`~torchrl.data.replay_buffers.Writer`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span>

    <span class="nd">@property</span>
    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">storage</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Storage</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The storage of the replay buffer.</span>

<span class="sd">        The storage must be an instance of :class:`~torchrl.data.replay_buffers.Storage`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span>

    <span class="nd">@property</span>
    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Transform</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The transform of the replay buffer.</span>

<span class="sd">        The transform must be an instance of :class:`~torchrl.envs.transforms.Transform`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span></div>


<div class="viewcode-block" id="PrioritizedReplayBuffer"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.PrioritizedReplayBuffer.html#torchrl.data.PrioritizedReplayBuffer">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">PrioritizedReplayBuffer</span><span class="p">(</span><span class="n">ReplayBuffer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Prioritized replay buffer.</span>

<span class="sd">    All arguments are keyword-only arguments.</span>

<span class="sd">    Presented in &quot;Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2015.</span>
<span class="sd">    Prioritized experience replay.&quot; (https://arxiv.org/abs/1511.05952)</span>

<span class="sd">    Args:</span>
<span class="sd">        alpha (:obj:`float`): exponent α determines how much prioritization is used,</span>
<span class="sd">            with α = 0 corresponding to the uniform case.</span>
<span class="sd">        beta (:obj:`float`): importance sampling negative exponent.</span>
<span class="sd">        eps (:obj:`float`): delta added to the priorities to ensure that the buffer</span>
<span class="sd">            does not contain null priorities.</span>
<span class="sd">        storage (Storage, optional): the storage to be used. If none is provided</span>
<span class="sd">            a default :class:`~torchrl.data.replay_buffers.ListStorage` with</span>
<span class="sd">            ``max_size`` of ``1_000`` will be created.</span>
<span class="sd">        sampler (Sampler, optional): the sampler to be used. If none is provided,</span>
<span class="sd">            a default :class:`~torchrl.data.replay_buffers.PrioritizedSampler` with</span>
<span class="sd">            ``alpha``, ``beta``, and ``eps`` will be created.</span>
<span class="sd">        collate_fn (callable, optional): merges a list of samples to form a</span>
<span class="sd">            mini-batch of Tensor(s)/outputs.  Used when using batched</span>
<span class="sd">            loading from a map-style dataset. The default value will be decided</span>
<span class="sd">            based on the storage type.</span>
<span class="sd">        pin_memory (bool): whether pin_memory() should be called on the rb</span>
<span class="sd">            samples.</span>
<span class="sd">        prefetch (int, optional): number of next batches to be prefetched</span>
<span class="sd">            using multithreading. Defaults to None (no prefetching).</span>
<span class="sd">        transform (Transform, optional): Transform to be executed when</span>
<span class="sd">            sample() is called.</span>
<span class="sd">            To chain transforms use the :class:`~torchrl.envs.Compose` class.</span>
<span class="sd">            Transforms should be used with :class:`tensordict.TensorDict`</span>
<span class="sd">            content. If used with other structures, the transforms should be</span>
<span class="sd">            encoded with a ``&quot;data&quot;`` leading key that will be used to</span>
<span class="sd">            construct a tensordict from the non-tensordict content.</span>
<span class="sd">        batch_size (int, optional): the batch size to be used when sample() is</span>
<span class="sd">            called.</span>

<span class="sd">            .. note:: The batch-size can be specified at construction time via the</span>
<span class="sd">              ``batch_size`` argument, or at sampling time. The former should</span>
<span class="sd">              be preferred whenever the batch-size is consistent across the</span>
<span class="sd">              experiment. If the batch-size is likely to change, it can be</span>
<span class="sd">              passed to the :meth:`sample` method. This option is</span>
<span class="sd">              incompatible with prefetching (since this requires to know the</span>
<span class="sd">              batch-size in advance) as well as with samplers that have a</span>
<span class="sd">              ``drop_last`` argument.</span>

<span class="sd">        dim_extend (int, optional): indicates the dim to consider for</span>
<span class="sd">            extension when calling :meth:`extend`. Defaults to ``storage.ndim-1``.</span>
<span class="sd">            When using ``dim_extend &gt; 0``, we recommend using the ``ndim``</span>
<span class="sd">            argument in the storage instantiation if that argument is</span>
<span class="sd">            available, to let storages know that the data is</span>
<span class="sd">            multi-dimensional and keep consistent notions of storage-capacity</span>
<span class="sd">            and batch-size during sampling.</span>

<span class="sd">            .. note:: This argument has no effect on :meth:`add` and</span>
<span class="sd">                therefore should be used with caution when both :meth:`add`</span>
<span class="sd">                and :meth:`extend` are used in a codebase. For example:</span>

<span class="sd">                    &gt;&gt;&gt; data = torch.zeros(3, 4)</span>
<span class="sd">                    &gt;&gt;&gt; rb = ReplayBuffer(</span>
<span class="sd">                    ...     storage=LazyTensorStorage(10, ndim=2),</span>
<span class="sd">                    ...     dim_extend=1)</span>
<span class="sd">                    &gt;&gt;&gt; # these two approaches are equivalent:</span>
<span class="sd">                    &gt;&gt;&gt; for d in data.unbind(1):</span>
<span class="sd">                    ...     rb.add(d)</span>
<span class="sd">                    &gt;&gt;&gt; rb.extend(data)</span>

<span class="sd">        delayed_init (bool, optional): whether to initialize storage, writer, sampler and transform</span>
<span class="sd">            the first time the buffer is used rather than during construction.</span>
<span class="sd">            This is useful when the replay buffer needs to be pickled and sent to remote workers,</span>
<span class="sd">            particularly when using transforms with modules that require gradients.</span>
<span class="sd">            If not specified, defaults to ``True`` when ``transform_factory`` is provided,</span>
<span class="sd">            and ``False`` otherwise.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Generic prioritized replay buffers (ie. non-tensordict backed) require</span>
<span class="sd">        calling :meth:`~.sample` with the ``return_info`` argument set to</span>
<span class="sd">        ``True`` to have access to the indices, and hence update the priority.</span>
<span class="sd">        Using :class:`tensordict.TensorDict` and the related</span>
<span class="sd">        :class:`~torchrl.data.TensorDictPrioritizedReplayBuffer` simplifies this</span>
<span class="sd">        process.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data import ListStorage, PrioritizedReplayBuffer</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; rb = PrioritizedReplayBuffer(alpha=0.7, beta=0.9, storage=ListStorage(10))</span>
<span class="sd">        &gt;&gt;&gt; data = range(10)</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">        &gt;&gt;&gt; sample = rb.sample(3)</span>
<span class="sd">        &gt;&gt;&gt; print(sample)</span>
<span class="sd">        tensor([1, 0, 1])</span>
<span class="sd">        &gt;&gt;&gt; # get the info to find what the indices are</span>
<span class="sd">        &gt;&gt;&gt; sample, info = rb.sample(5, return_info=True)</span>
<span class="sd">        &gt;&gt;&gt; print(sample, info)</span>
<span class="sd">        tensor([2, 7, 4, 3, 5]) {&#39;_weight&#39;: array([1., 1., 1., 1., 1.], dtype=float32), &#39;index&#39;: array([2, 7, 4, 3, 5])}</span>
<span class="sd">        &gt;&gt;&gt; # update priority</span>
<span class="sd">        &gt;&gt;&gt; priority = torch.ones(5) * 5</span>
<span class="sd">        &gt;&gt;&gt; rb.update_priority(info[&quot;index&quot;], priority)</span>
<span class="sd">        &gt;&gt;&gt; # and now a new sample, the weights should be updated</span>
<span class="sd">        &gt;&gt;&gt; sample, info = rb.sample(5, return_info=True)</span>
<span class="sd">        &gt;&gt;&gt; print(sample, info)</span>
<span class="sd">        tensor([2, 5, 2, 2, 5]) {&#39;_weight&#39;: array([0.36278465, 0.36278465, 0.36278465, 0.36278465, 0.36278465],</span>
<span class="sd">              dtype=float32), &#39;index&#39;: array([2, 5, 2, 2, 5])}</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
        <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sampler</span><span class="p">:</span> <span class="n">Sampler</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">prefetch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">Transform</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa-F821</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dim_extend</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">delayed_init</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">storage</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">storage</span> <span class="o">=</span> <span class="n">ListStorage</span><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="mi">1_000</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sampler</span> <span class="o">=</span> <span class="n">PrioritizedSampler</span><span class="p">(</span><span class="n">storage</span><span class="o">.</span><span class="n">max_size</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">storage</span><span class="o">=</span><span class="n">storage</span><span class="p">,</span>
            <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span>
            <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">,</span>
            <span class="n">prefetch</span><span class="o">=</span><span class="n">prefetch</span><span class="p">,</span>
            <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">dim_extend</span><span class="o">=</span><span class="n">dim_extend</span><span class="p">,</span>
            <span class="n">delayed_init</span><span class="o">=</span><span class="n">delayed_init</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="TensorDictReplayBuffer"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">TensorDictReplayBuffer</span><span class="p">(</span><span class="n">ReplayBuffer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;TensorDict-specific wrapper around the :class:`~torchrl.data.ReplayBuffer` class.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        storage (Storage, Callable[[], Storage], optional): the storage to be used.</span>
<span class="sd">            If a callable is passed, it is used as constructor for the storage.</span>
<span class="sd">            If none is provided a default :class:`~torchrl.data.replay_buffers.ListStorage` with</span>
<span class="sd">            ``max_size`` of ``1_000`` will be created.</span>
<span class="sd">        sampler (Sampler, Callable[[], Sampler], optional): the sampler to be used.</span>
<span class="sd">            If a callable is passed, it is used as constructor for the sampler.</span>
<span class="sd">            If none is provided, a default :class:`~torchrl.data.replay_buffers.RandomSampler`</span>
<span class="sd">            will be used.</span>
<span class="sd">        writer (Writer, Callable[[], Writer], optional): the writer to be used.</span>
<span class="sd">            If a callable is passed, it is used as constructor for the writer.</span>
<span class="sd">            If none is provided a default :class:`~torchrl.data.replay_buffers.TensorDictRoundRobinWriter`</span>
<span class="sd">            will be used.</span>
<span class="sd">        collate_fn (callable, optional): merges a list of samples to form a</span>
<span class="sd">            mini-batch of Tensor(s)/outputs.  Used when using batched</span>
<span class="sd">            loading from a map-style dataset. The default value will be decided</span>
<span class="sd">            based on the storage type.</span>
<span class="sd">        pin_memory (bool): whether pin_memory() should be called on the rb</span>
<span class="sd">            samples.</span>
<span class="sd">        prefetch (int, optional): number of next batches to be prefetched</span>
<span class="sd">            using multithreading. Defaults to None (no prefetching).</span>
<span class="sd">        transform (Transform or Callable[[Any], Any], optional): Transform to be executed when</span>
<span class="sd">            :meth:`sample` is called.</span>
<span class="sd">            To chain transforms use the :class:`~torchrl.envs.Compose` class.</span>
<span class="sd">            Transforms should be used with :class:`tensordict.TensorDict`</span>
<span class="sd">            content. A generic callable can also be passed if the replay buffer</span>
<span class="sd">            is used with PyTree structures (see example below).</span>
<span class="sd">            Unlike storages, writers and samplers, transform constructors must</span>
<span class="sd">            be passed as separate keyword argument :attr:`transform_factory`,</span>
<span class="sd">            as it is impossible to distinguish a constructor from a transform.</span>
<span class="sd">        transform_factory (Callable[[], Callable], optional): a factory for the</span>
<span class="sd">            transform. Exclusive with :attr:`transform`.</span>
<span class="sd">        batch_size (int, optional): the batch size to be used when sample() is</span>
<span class="sd">            called.</span>

<span class="sd">            .. note::</span>
<span class="sd">              The batch-size can be specified at construction time via the</span>
<span class="sd">              ``batch_size`` argument, or at sampling time. The former should</span>
<span class="sd">              be preferred whenever the batch-size is consistent across the</span>
<span class="sd">              experiment. If the batch-size is likely to change, it can be</span>
<span class="sd">              passed to the :meth:`~.sample` method. This option is</span>
<span class="sd">              incompatible with prefetching (since this requires to know the</span>
<span class="sd">              batch-size in advance) as well as with samplers that have a</span>
<span class="sd">              ``drop_last`` argument.</span>

<span class="sd">        priority_key (str, optional): the key at which priority is assumed to</span>
<span class="sd">            be stored within TensorDicts added to this ReplayBuffer.</span>
<span class="sd">            This is to be used when the sampler is of type</span>
<span class="sd">            :class:`~torchrl.data.PrioritizedSampler`.</span>
<span class="sd">            Defaults to ``&quot;td_error&quot;``.</span>
<span class="sd">        dim_extend (int, optional): indicates the dim to consider for</span>
<span class="sd">            extension when calling :meth:`~.extend`. Defaults to ``storage.ndim-1``.</span>
<span class="sd">            When using ``dim_extend &gt; 0``, we recommend using the ``ndim``</span>
<span class="sd">            argument in the storage instantiation if that argument is</span>
<span class="sd">            available, to let storages know that the data is</span>
<span class="sd">            multi-dimensional and keep consistent notions of storage-capacity</span>
<span class="sd">            and batch-size during sampling.</span>

<span class="sd">            .. note:: This argument has no effect on :meth:`~.add` and</span>
<span class="sd">                therefore should be used with caution when both :meth:`~.add`</span>
<span class="sd">                and :meth:`~.extend` are used in a codebase. For example:</span>

<span class="sd">                    &gt;&gt;&gt; data = torch.zeros(3, 4)</span>
<span class="sd">                    &gt;&gt;&gt; rb = ReplayBuffer(</span>
<span class="sd">                    ...     storage=LazyTensorStorage(10, ndim=2),</span>
<span class="sd">                    ...     dim_extend=1)</span>
<span class="sd">                    &gt;&gt;&gt; # these two approaches are equivalent:</span>
<span class="sd">                    &gt;&gt;&gt; for d in data.unbind(1):</span>
<span class="sd">                    ...     rb.add(d)</span>
<span class="sd">                    &gt;&gt;&gt; rb.extend(data)</span>

<span class="sd">        generator (torch.Generator, optional): a generator to use for sampling.</span>
<span class="sd">            Using a dedicated generator for the replay buffer can allow a fine-grained control</span>
<span class="sd">            over seeding, for instance keeping the global seed different but the RB seed identical</span>
<span class="sd">            for distributed jobs.</span>
<span class="sd">            Defaults to ``None`` (global default generator).</span>

<span class="sd">            .. warning:: As of now, the generator has no effect on the transforms.</span>
<span class="sd">        shared (bool, optional): whether the buffer will be shared using multiprocessing or not.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        compilable (bool, optional): whether the writer is compilable.</span>
<span class="sd">            If ``True``, the writer cannot be shared between multiple processes.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        delayed_init (bool, optional): whether to initialize storage, writer, sampler and transform</span>
<span class="sd">            the first time the buffer is used rather than during construction.</span>
<span class="sd">            This is useful when the replay buffer needs to be pickled and sent to remote workers,</span>
<span class="sd">            particularly when using transforms with modules that require gradients.</span>
<span class="sd">            If not specified, defaults to ``True`` when ``transform_factory`` is provided,</span>
<span class="sd">            and ``False`` otherwise.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data import LazyTensorStorage, TensorDictReplayBuffer</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; rb = TensorDictReplayBuffer(storage=LazyTensorStorage(10), batch_size=5)</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict({&quot;a&quot;: torch.ones(10, 3), (&quot;b&quot;, &quot;c&quot;): torch.zeros(10, 1, 1)}, [10])</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">        &gt;&gt;&gt; sample = rb.sample(3)</span>
<span class="sd">        &gt;&gt;&gt; # samples keep track of the index</span>
<span class="sd">        &gt;&gt;&gt; print(sample)</span>
<span class="sd">        TensorDict(</span>
<span class="sd">            fields={</span>
<span class="sd">                a: Tensor(shape=torch.Size([3, 3]), device=cpu, dtype=torch.float32, is_shared=False),</span>
<span class="sd">                b: TensorDict(</span>
<span class="sd">                    fields={</span>
<span class="sd">                        c: Tensor(shape=torch.Size([3, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False)},</span>
<span class="sd">                    batch_size=torch.Size([3]),</span>
<span class="sd">                    device=cpu,</span>
<span class="sd">                    is_shared=False),</span>
<span class="sd">                index: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.int32, is_shared=False)},</span>
<span class="sd">            batch_size=torch.Size([3]),</span>
<span class="sd">            device=cpu,</span>
<span class="sd">            is_shared=False)</span>
<span class="sd">        &gt;&gt;&gt; # we can iterate over the buffer</span>
<span class="sd">        &gt;&gt;&gt; for i, data in enumerate(rb):</span>
<span class="sd">        ...     print(i, data)</span>
<span class="sd">        ...     if i == 2:</span>
<span class="sd">        ...         break</span>
<span class="sd">        0 TensorDict(</span>
<span class="sd">            fields={</span>
<span class="sd">                a: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),</span>
<span class="sd">                b: TensorDict(</span>
<span class="sd">                    fields={</span>
<span class="sd">                        c: Tensor(shape=torch.Size([5, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False)},</span>
<span class="sd">                    batch_size=torch.Size([5]),</span>
<span class="sd">                    device=cpu,</span>
<span class="sd">                    is_shared=False),</span>
<span class="sd">                index: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int32, is_shared=False)},</span>
<span class="sd">            batch_size=torch.Size([5]),</span>
<span class="sd">            device=cpu,</span>
<span class="sd">            is_shared=False)</span>
<span class="sd">        1 TensorDict(</span>
<span class="sd">            fields={</span>
<span class="sd">                a: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),</span>
<span class="sd">                b: TensorDict(</span>
<span class="sd">                    fields={</span>
<span class="sd">                        c: Tensor(shape=torch.Size([5, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False)},</span>
<span class="sd">                    batch_size=torch.Size([5]),</span>
<span class="sd">                    device=cpu,</span>
<span class="sd">                    is_shared=False),</span>
<span class="sd">                index: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int32, is_shared=False)},</span>
<span class="sd">            batch_size=torch.Size([5]),</span>
<span class="sd">            device=cpu,</span>
<span class="sd">            is_shared=False)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">priority_key</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;td_error&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">writer</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;writer&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">writer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;writer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
                <span class="n">TensorDictRoundRobinWriter</span><span class="p">,</span> <span class="n">compilable</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;compilable&quot;</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">priority_key</span> <span class="o">=</span> <span class="n">priority_key</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_priority_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">priority_key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># We have to flatten the priority otherwise we&#39;ll be aggregating</span>
            <span class="c1"># the priority across batches</span>
            <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">priority</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">default_priority</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">priority</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">priority</span> <span class="o">=</span> <span class="n">_reduce</span><span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Found a priority key of size&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">priority_key</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> but expected &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;scalar value&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">priority</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_priority_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">priority_key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">priority</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">default_priority</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">tensordict</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">tensordict</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">priority</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="c1"># We have to flatten the priority otherwise we&#39;ll be aggregating</span>
            <span class="c1"># the priority across batches</span>
            <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">priority</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">priority</span> <span class="o">=</span> <span class="n">_reduce</span><span class="p">(</span><span class="n">priority</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">priority</span>

<div class="viewcode-block" id="TensorDictReplayBuffer.add"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer.add">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">_set_dispatch_td_nn_modules</span><span class="p">(</span><span class="n">is_tensor_collection</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

        <span class="n">index</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_add</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">is_tensor_collection</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_set_index_in_td</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">update_tensordict_priority</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">index</span></div>

<div class="viewcode-block" id="TensorDictReplayBuffer.extend"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer.extend">[docs]</a>    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">extend</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensordicts</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">update_priority</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Extends the replay buffer with a batch of data.</span>

<span class="sd">        Args:</span>
<span class="sd">            tensordicts (TensorDictBase): The data to extend the replay buffer with.</span>

<span class="sd">        Keyword Args:</span>
<span class="sd">            update_priority (bool, optional): Whether to update the priority of the data. Defaults to True.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The indices of the data that were added to the replay buffer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensordicts</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> only accepts TensorDictBase subclasses. tensorclasses &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;and other types are not compatible with that class. &quot;</span>
                <span class="s2">&quot;Please use a regular `ReplayBuffer` instead.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tensordicts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">tensordicts</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensordicts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

        <span class="n">index</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_extend</span><span class="p">(</span><span class="n">tensordicts</span><span class="p">)</span>

        <span class="c1"># TODO: to be usable directly, the indices should be flipped but the issue</span>
        <span class="c1">#  is that just doing this results in indices that are not sorted like the original data</span>
        <span class="c1">#  so the actually indices will have to be used on the _storage directly (not on the buffer)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_index_in_td</span><span class="p">(</span><span class="n">tensordicts</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">update_priority</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">update_priority</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">update_priority</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">vector</span> <span class="o">=</span> <span class="n">tensordicts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">priority_key</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">update_priority</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">vector</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;Failed to update priority of extended data. You can try to set update_priority=False in the extend method and update the priority manually.&quot;</span>
                <span class="p">)</span> <span class="kn">from</span><span class="w"> </span><span class="nn">e</span>
        <span class="k">return</span> <span class="n">index</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_set_index_in_td</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">index</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="n">_is_int</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">tensordict</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">dim</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
                    <span class="c1"># if index has 2 dims and is in a non-zero format</span>
                    <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">dim</span><span class="p">])</span>
                    <span class="k">break</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;could not find how to reshape index with shape </span><span class="si">{</span><span class="n">index</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> to fit in tensordict with shape </span><span class="si">{</span><span class="n">tensordict</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="n">tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="n">expand_as_right</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">))</span>

    <span class="nd">@_maybe_delay_init</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">update_tensordict_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="p">,</span> <span class="n">PrioritizedSampler</span><span class="p">):</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
            <span class="n">priority</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_priority_vector</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">priority</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_priority_item</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">while</span> <span class="n">index</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">priority</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="c1"># reduce index</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_priority</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">priority</span><span class="p">)</span>

<div class="viewcode-block" id="TensorDictReplayBuffer.sample"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.TensorDictReplayBuffer.html#torchrl.data.TensorDictReplayBuffer.sample">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_info</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">include_info</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Samples a batch of data from the replay buffer.</span>

<span class="sd">        Uses Sampler to sample indices, and retrieves them from Storage.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_size (int, optional): size of data to be collected. If none</span>
<span class="sd">                is provided, this method will sample a batch-size as indicated</span>
<span class="sd">                by the sampler.</span>
<span class="sd">            return_info (bool): whether to return info. If True, the result</span>
<span class="sd">                is a tuple (data, info). If False, the result is the data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tensordict containing a batch of data selected in the replay buffer.</span>
<span class="sd">            A tuple containing this tensordict and info if return_info flag is set to True.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">include_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;include_info is going to be deprecated soon.&quot;</span>
                <span class="s2">&quot;The default behavior has changed to `include_info=True` &quot;</span>
                <span class="s2">&quot;to avoid bugs linked to wrongly preassigned values in the &quot;</span>
                <span class="s2">&quot;output tensordict.&quot;</span>
            <span class="p">)</span>

        <span class="n">data</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">return_info</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">is_tc</span> <span class="o">=</span> <span class="n">is_tensor_collection</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_tc</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_tensorclass</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="ow">and</span> <span class="n">include_info</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">is_locked</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">is_locked</span>
            <span class="k">if</span> <span class="n">is_locked</span><span class="p">:</span>
                <span class="n">data</span><span class="o">.</span><span class="n">unlock_</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">info</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;index&quot;</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                    <span class="n">val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">val</span> <span class="o">=</span> <span class="n">_to_torch</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">val</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="n">data</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                        <span class="n">val</span> <span class="o">=</span> <span class="n">expand_as_right</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
                    <span class="n">data</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">RuntimeError</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s2">&quot;Failed to set the metadata (e.g., indices or weights) in the sampled tensordict within TensorDictReplayBuffer.sample. &quot;</span>
                        <span class="s2">&quot;This is probably caused by a shape mismatch (one of the transforms has probably modified &quot;</span>
                        <span class="s2">&quot;the shape of the output tensordict). &quot;</span>
                        <span class="s2">&quot;You can always recover these items from the `sample` method from a regular ReplayBuffer &quot;</span>
                        <span class="s2">&quot;instance with the &#39;return_info&#39; flag set to True.&quot;</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="n">is_locked</span><span class="p">:</span>
                <span class="n">data</span><span class="o">.</span><span class="n">lock_</span><span class="p">()</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="n">is_tc</span> <span class="ow">and</span> <span class="n">include_info</span> <span class="ow">in</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Cannot include info in non-tensordict data&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">return_info</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">info</span>
        <span class="k">return</span> <span class="n">data</span></div>

    <span class="nd">@pin_memory_output</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="n">is_comp</span> <span class="o">=</span> <span class="n">is_compiling</span><span class="p">()</span>
        <span class="n">nc</span> <span class="o">=</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_replay_lock</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_comp</span> <span class="k">else</span> <span class="n">nc</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_write_lock</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_comp</span> <span class="k">else</span> <span class="n">nc</span><span class="p">:</span>
            <span class="n">index</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
            <span class="n">info</span><span class="p">[</span><span class="s2">&quot;index&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">INT_CLASSES</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">data</span><span class="o">.</span><span class="n">unlock_</span><span class="p">(),</span> <span class="n">_set_dispatch_td_nn_modules</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
                <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">info</span></div>


<div class="viewcode-block" id="TensorDictPrioritizedReplayBuffer"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.TensorDictPrioritizedReplayBuffer.html#torchrl.data.TensorDictPrioritizedReplayBuffer">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">TensorDictPrioritizedReplayBuffer</span><span class="p">(</span><span class="n">TensorDictReplayBuffer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;TensorDict-specific wrapper around the :class:`~torchrl.data.PrioritizedReplayBuffer` class.</span>

<span class="sd">    This class returns tensordicts with a new key ``&quot;index&quot;`` that represents</span>
<span class="sd">    the index of each element in the replay buffer. It also provides the</span>
<span class="sd">    :meth:`~.update_tensordict_priority` method that only requires for the</span>
<span class="sd">    tensordict to be passed to it with its new priority value.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        alpha (:obj:`float`): exponent α determines how much prioritization is used,</span>
<span class="sd">            with α = 0 corresponding to the uniform case.</span>
<span class="sd">        beta (:obj:`float`): importance sampling negative exponent.</span>
<span class="sd">        eps (:obj:`float`): delta added to the priorities to ensure that the buffer</span>
<span class="sd">            does not contain null priorities.</span>
<span class="sd">        storage (Storage, Callable[[], Storage], optional): the storage to be used.</span>
<span class="sd">            If a callable is passed, it is used as constructor for the storage.</span>
<span class="sd">            If none is provided a default :class:`~torchrl.data.replay_buffers.ListStorage` with</span>
<span class="sd">            ``max_size`` of ``1_000`` will be created.</span>
<span class="sd">        collate_fn (callable, optional): merges a list of samples to form a</span>
<span class="sd">            mini-batch of Tensor(s)/outputs.  Used when using batched</span>
<span class="sd">            loading from a map-style dataset. The default value will be decided</span>
<span class="sd">            based on the storage type.</span>
<span class="sd">        pin_memory (bool): whether pin_memory() should be called on the rb</span>
<span class="sd">            samples.</span>
<span class="sd">        prefetch (int, optional): number of next batches to be prefetched</span>
<span class="sd">            using multithreading. Defaults to None (no prefetching).</span>
<span class="sd">        transform (Transform or Callable[[Any], Any], optional): Transform to be executed when</span>
<span class="sd">            :meth:`sample` is called.</span>
<span class="sd">            To chain transforms use the :class:`~torchrl.envs.Compose` class.</span>
<span class="sd">            Transforms should be used with :class:`tensordict.TensorDict`</span>
<span class="sd">            content. A generic callable can also be passed if the replay buffer</span>
<span class="sd">            is used with PyTree structures (see example below).</span>
<span class="sd">            Unlike storages, writers and samplers, transform constructors must</span>
<span class="sd">            be passed as separate keyword argument :attr:`transform_factory`,</span>
<span class="sd">            as it is impossible to distinguish a constructor from a transform.</span>
<span class="sd">        transform_factory (Callable[[], Callable], optional): a factory for the</span>
<span class="sd">            transform. Exclusive with :attr:`transform`.</span>
<span class="sd">        batch_size (int, optional): the batch size to be used when sample() is</span>
<span class="sd">            called.</span>

<span class="sd">            .. note::</span>
<span class="sd">              The batch-size can be specified at construction time via the</span>
<span class="sd">              ``batch_size`` argument, or at sampling time. The former should</span>
<span class="sd">              be preferred whenever the batch-size is consistent across the</span>
<span class="sd">              experiment. If the batch-size is likely to change, it can be</span>
<span class="sd">              passed to the :meth:`~.sample` method. This option is</span>
<span class="sd">              incompatible with prefetching (since this requires to know the</span>
<span class="sd">              batch-size in advance) as well as with samplers that have a</span>
<span class="sd">              ``drop_last`` argument.</span>

<span class="sd">        priority_key (str, optional): the key at which priority is assumed to</span>
<span class="sd">            be stored within TensorDicts added to this ReplayBuffer.</span>
<span class="sd">            This is to be used when the sampler is of type</span>
<span class="sd">            :class:`~torchrl.data.PrioritizedSampler`.</span>
<span class="sd">            Defaults to ``&quot;td_error&quot;``.</span>
<span class="sd">        reduction (str, optional): the reduction method for multidimensional</span>
<span class="sd">            tensordicts (ie stored trajectories). Can be one of &quot;max&quot;, &quot;min&quot;,</span>
<span class="sd">            &quot;median&quot; or &quot;mean&quot;.</span>
<span class="sd">        dim_extend (int, optional): indicates the dim to consider for</span>
<span class="sd">            extension when calling :meth:`~.extend`. Defaults to ``storage.ndim-1``.</span>
<span class="sd">            When using ``dim_extend &gt; 0``, we recommend using the ``ndim``</span>
<span class="sd">            argument in the storage instantiation if that argument is</span>
<span class="sd">            available, to let storages know that the data is</span>
<span class="sd">            multi-dimensional and keep consistent notions of storage-capacity</span>
<span class="sd">            and batch-size during sampling.</span>

<span class="sd">            .. note:: This argument has no effect on :meth:`~.add` and</span>
<span class="sd">                therefore should be used with caution when both :meth:`~.add`</span>
<span class="sd">                and :meth:`~.extend` are used in a codebase. For example:</span>

<span class="sd">                    &gt;&gt;&gt; data = torch.zeros(3, 4)</span>
<span class="sd">                    &gt;&gt;&gt; rb = ReplayBuffer(</span>
<span class="sd">                    ...     storage=LazyTensorStorage(10, ndim=2),</span>
<span class="sd">                    ...     dim_extend=1)</span>
<span class="sd">                    &gt;&gt;&gt; # these two approaches are equivalent:</span>
<span class="sd">                    &gt;&gt;&gt; for d in data.unbind(1):</span>
<span class="sd">                    ...     rb.add(d)</span>
<span class="sd">                    &gt;&gt;&gt; rb.extend(data)</span>

<span class="sd">        generator (torch.Generator, optional): a generator to use for sampling.</span>
<span class="sd">            Using a dedicated generator for the replay buffer can allow a fine-grained control</span>
<span class="sd">            over seeding, for instance keeping the global seed different but the RB seed identical</span>
<span class="sd">            for distributed jobs.</span>
<span class="sd">            Defaults to ``None`` (global default generator).</span>

<span class="sd">            .. warning:: As of now, the generator has no effect on the transforms.</span>
<span class="sd">        shared (bool, optional): whether the buffer will be shared using multiprocessing or not.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        compilable (bool, optional): whether the writer is compilable.</span>
<span class="sd">            If ``True``, the writer cannot be shared between multiple processes.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        delayed_init (bool, optional): whether to initialize storage, writer, sampler and transform</span>
<span class="sd">            the first time the buffer is used rather than during construction.</span>
<span class="sd">            This is useful when the replay buffer needs to be pickled and sent to remote workers,</span>
<span class="sd">            particularly when using transforms with modules that require gradients.</span>
<span class="sd">            If not specified, defaults to ``True`` when ``transform_factory`` is provided,</span>
<span class="sd">            and ``False`` otherwise.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data import LazyTensorStorage, TensorDictPrioritizedReplayBuffer</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; torch.manual_seed(0)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; rb = TensorDictPrioritizedReplayBuffer(alpha=0.7, beta=1.1, storage=LazyTensorStorage(10), batch_size=5)</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict({&quot;a&quot;: torch.ones(10, 3), (&quot;b&quot;, &quot;c&quot;): torch.zeros(10, 3, 1)}, [10])</span>
<span class="sd">        &gt;&gt;&gt; rb.extend(data)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;len of rb&quot;, len(rb))</span>
<span class="sd">        len of rb 10</span>
<span class="sd">        &gt;&gt;&gt; sample = rb.sample(5)</span>
<span class="sd">        &gt;&gt;&gt; print(sample)</span>
<span class="sd">        TensorDict(</span>
<span class="sd">            fields={</span>
<span class="sd">                _weight: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.float32, is_shared=False),</span>
<span class="sd">                a: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),</span>
<span class="sd">                b: TensorDict(</span>
<span class="sd">                    fields={</span>
<span class="sd">                        c: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},</span>
<span class="sd">                    batch_size=torch.Size([5]),</span>
<span class="sd">                    device=cpu,</span>
<span class="sd">                    is_shared=False),</span>
<span class="sd">                index: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False)},</span>
<span class="sd">            batch_size=torch.Size([5]),</span>
<span class="sd">            device=cpu,</span>
<span class="sd">            is_shared=False)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;index&quot;, sample[&quot;index&quot;])</span>
<span class="sd">        index tensor([9, 5, 2, 2, 7])</span>
<span class="sd">        &gt;&gt;&gt; # give a high priority to these samples...</span>
<span class="sd">        &gt;&gt;&gt; sample.set(&quot;td_error&quot;, 100*torch.ones(sample.shape))</span>
<span class="sd">        &gt;&gt;&gt; # and update priority</span>
<span class="sd">        &gt;&gt;&gt; rb.update_tensordict_priority(sample)</span>
<span class="sd">        &gt;&gt;&gt; # the new sample should have a high overlap with the previous one</span>
<span class="sd">        &gt;&gt;&gt; sample = rb.sample(5)</span>
<span class="sd">        &gt;&gt;&gt; print(sample)</span>
<span class="sd">        TensorDict(</span>
<span class="sd">            fields={</span>
<span class="sd">                _weight: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.float32, is_shared=False),</span>
<span class="sd">                a: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),</span>
<span class="sd">                b: TensorDict(</span>
<span class="sd">                    fields={</span>
<span class="sd">                        c: Tensor(shape=torch.Size([5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},</span>
<span class="sd">                    batch_size=torch.Size([5]),</span>
<span class="sd">                    device=cpu,</span>
<span class="sd">                    is_shared=False),</span>
<span class="sd">                index: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False)},</span>
<span class="sd">            batch_size=torch.Size([5]),</span>
<span class="sd">            device=cpu,</span>
<span class="sd">            is_shared=False)</span>
<span class="sd">        &gt;&gt;&gt; print(&quot;index&quot;, sample[&quot;index&quot;])</span>
<span class="sd">        index tensor([2, 5, 5, 9, 7])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">priority_key</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;td_error&quot;</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">storage</span><span class="p">:</span> <span class="n">Storage</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">prefetch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">Transform</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa-F821</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dim_extend</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">generator</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shared</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">compilable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_make_storage</span><span class="p">(</span><span class="n">storage</span><span class="p">,</span> <span class="n">compilable</span><span class="o">=</span><span class="n">compilable</span><span class="p">)</span>
        <span class="n">sampler</span> <span class="o">=</span> <span class="n">PrioritizedSampler</span><span class="p">(</span>
            <span class="n">storage</span><span class="o">.</span><span class="n">max_size</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">priority_key</span><span class="o">=</span><span class="n">priority_key</span><span class="p">,</span>
            <span class="n">storage</span><span class="o">=</span><span class="n">storage</span><span class="p">,</span>
            <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span>
            <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">,</span>
            <span class="n">prefetch</span><span class="o">=</span><span class="n">prefetch</span><span class="p">,</span>
            <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">dim_extend</span><span class="o">=</span><span class="n">dim_extend</span><span class="p">,</span>
            <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
            <span class="n">shared</span><span class="o">=</span><span class="n">shared</span><span class="p">,</span>
            <span class="n">compilable</span><span class="o">=</span><span class="n">compilable</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="RemoteTensorDictReplayBuffer"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.RemoteTensorDictReplayBuffer.html#torchrl.data.RemoteTensorDictReplayBuffer">[docs]</a><span class="nd">@accept_remote_rref_udf_invocation</span>
<span class="k">class</span><span class="w"> </span><span class="nc">RemoteTensorDictReplayBuffer</span><span class="p">(</span><span class="n">TensorDictReplayBuffer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A remote invocation friendly ReplayBuffer class. Public methods can be invoked by remote agents using `torch.rpc` or called locally as normal.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<div class="viewcode-block" id="RemoteTensorDictReplayBuffer.sample"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.RemoteTensorDictReplayBuffer.html#torchrl.data.RemoteTensorDictReplayBuffer.sample">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">include_info</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">return_info</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">include_info</span><span class="o">=</span><span class="n">include_info</span><span class="p">,</span> <span class="n">return_info</span><span class="o">=</span><span class="n">return_info</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="RemoteTensorDictReplayBuffer.add"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.RemoteTensorDictReplayBuffer.html#torchrl.data.RemoteTensorDictReplayBuffer.add">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></div>

<div class="viewcode-block" id="RemoteTensorDictReplayBuffer.extend"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.RemoteTensorDictReplayBuffer.html#torchrl.data.RemoteTensorDictReplayBuffer.extend">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">extend</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensordicts</span><span class="p">:</span> <span class="nb">list</span> <span class="o">|</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">update_priority</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">tensordicts</span><span class="p">,</span> <span class="n">update_priority</span><span class="o">=</span><span class="n">update_priority</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">update_priority</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">priority</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">update_priority</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">priority</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update_tensordict_priority</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">update_tensordict_priority</span><span class="p">(</span><span class="n">data</span><span class="p">)</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">InPlaceSampler</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;[Deprecated] A sampler to write tennsordicts in-place.&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">DEVICE_TYPING</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;This class has been removed without replacement. In-place sampling should be avoided.&quot;</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">stack_tensors</span><span class="p">(</span><span class="n">list_of_tensor_iterators</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Zips a list of iterables containing tensor-like objects and stacks the resulting lists of tensors together.</span>

<span class="sd">    Args:</span>
<span class="sd">        list_of_tensor_iterators (list): Sequence containing similar iterators,</span>
<span class="sd">            where each element of the nested iterator is a tensor whose</span>
<span class="sd">            shape match the tensor of other iterators that have the same index.</span>

<span class="sd">    Returns:</span>
<span class="sd">         Tuple of stacked tensors.</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; list_of_tensor_iterators = [[torch.ones(3), torch.zeros(1,2)]</span>
<span class="sd">         ...     for _ in range(4)]</span>
<span class="sd">         &gt;&gt;&gt; stack_tensors(list_of_tensor_iterators)</span>
<span class="sd">         (tensor([[1., 1., 1.],</span>
<span class="sd">                 [1., 1., 1.],</span>
<span class="sd">                 [1., 1., 1.],</span>
<span class="sd">                 [1., 1., 1.]]), tensor([[[0., 0.]],</span>
<span class="sd">         &lt;BLANKLINE&gt;</span>
<span class="sd">                 [[0., 0.]],</span>
<span class="sd">         &lt;BLANKLINE&gt;</span>
<span class="sd">                 [[0., 0.]],</span>
<span class="sd">         &lt;BLANKLINE&gt;</span>
<span class="sd">                 [[0., 0.]]]))</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensors</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">list_of_tensor_iterators</span><span class="p">))</span>


<div class="viewcode-block" id="ReplayBufferEnsemble"><a class="viewcode-back" href="../../../../reference/generated/torchrl.data.replay_buffers.ReplayBufferEnsemble.html#torchrl.data.ReplayBufferEnsemble">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">ReplayBufferEnsemble</span><span class="p">(</span><span class="n">ReplayBuffer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An ensemble of replay buffers.</span>

<span class="sd">    This class allows to read and sample from multiple replay buffers at once.</span>
<span class="sd">    It automatically composes ensemble of storages (:class:`~torchrl.data.replay_buffers.storages.StorageEnsemble`),</span>
<span class="sd">    writers (:class:`~torchrl.data.replay_buffers.writers.WriterEnsemble`) and</span>
<span class="sd">    samplers (:class:`~torchrl.data.replay_buffers.samplers.SamplerEnsemble`).</span>

<span class="sd">    .. note::</span>
<span class="sd">      Writing directly to this class is forbidden, but it can be indexed to retrieve</span>
<span class="sd">      the nested nested-buffer and extending it.</span>

<span class="sd">    There are two distinct ways of constructing a :class:`~torchrl.data.ReplayBufferEnsemble`:</span>
<span class="sd">    one can either pass a list of replay buffers, or directly pass the components</span>
<span class="sd">    (storage, writers and samplers) like it is done for other replay buffer subclasses.</span>

<span class="sd">    Args:</span>
<span class="sd">        rbs (sequence of ReplayBuffer instances, optional): the replay buffers to ensemble.</span>
<span class="sd">        storages (StorageEnsemble, optional): the ensemble of storages, if the replay</span>
<span class="sd">            buffers are not passed.</span>
<span class="sd">        samplers (SamplerEnsemble, optional): the ensemble of samplers, if the replay</span>
<span class="sd">            buffers are not passed.</span>
<span class="sd">        writers (WriterEnsemble, optional): the ensemble of writers, if the replay</span>
<span class="sd">            buffers are not passed.</span>
<span class="sd">        transform (Transform, optional): if passed, this will be the transform</span>
<span class="sd">            of the ensemble of replay buffers. Individual transforms for each</span>
<span class="sd">            replay buffer is retrieved from its parent replay buffer, or directly</span>
<span class="sd">            written in the :class:`~torchrl.data.replay_buffers.storages.StorageEnsemble`</span>
<span class="sd">            object.</span>
<span class="sd">        batch_size (int, optional): the batch-size to use during sampling.</span>
<span class="sd">        collate_fn (callable, optional): the function to use to collate the</span>
<span class="sd">            data after each individual collate_fn has been called and the data</span>
<span class="sd">            is placed in a list (along with the buffer id).</span>
<span class="sd">        collate_fns (list of callables, optional): collate_fn of each nested</span>
<span class="sd">            replay buffer. Retrieved from the :class:`~ReplayBuffer` instances</span>
<span class="sd">            if not provided.</span>
<span class="sd">        p (list of float or Tensor, optional): a list of floating numbers</span>
<span class="sd">            indicating the relative weight of each replay buffer. Can also</span>
<span class="sd">            be passed to torchrl.data.replay_buffers.samplers.SamplerEnsemble`</span>
<span class="sd">            if the buffer is built explicitly.</span>
<span class="sd">        sample_from_all (bool, optional): if ``True``, each dataset will be sampled</span>
<span class="sd">            from. This is not compatible with the ``p`` argument. Defaults to ``False``.</span>
<span class="sd">            Can also be passed to torchrl.data.replay_buffers.samplers.SamplerEnsemble`</span>
<span class="sd">            if the buffer is built explicitly.</span>
<span class="sd">        num_buffer_sampled (int, optional): the number of buffers to sample.</span>
<span class="sd">            if ``sample_from_all=True``, this has no effect, as it defaults to the</span>
<span class="sd">            number of buffers. If ``sample_from_all=False``, buffers will be</span>
<span class="sd">            sampled according to the probabilities ``p``. Can also</span>
<span class="sd">            be passed to torchrl.data.replay_buffers.samplers.SamplerEnsemble`</span>
<span class="sd">            if the buffer is built explicitly.</span>
<span class="sd">        generator (torch.Generator, optional): a generator to use for sampling.</span>
<span class="sd">            Using a dedicated generator for the replay buffer can allow a fine-grained control</span>
<span class="sd">            over seeding, for instance keeping the global seed different but the RB seed identical</span>
<span class="sd">            for distributed jobs.</span>
<span class="sd">            Defaults to ``None`` (global default generator).</span>

<span class="sd">            .. warning:: As of now, the generator has no effect on the transforms.</span>

<span class="sd">        shared (bool, optional): whether the buffer will be shared using multiprocessing or not.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        delayed_init (bool, optional): whether to initialize storage, writer, sampler and transform</span>
<span class="sd">            the first time the buffer is used rather than during construction.</span>
<span class="sd">            This is useful when the replay buffer needs to be pickled and sent to remote workers,</span>
<span class="sd">            particularly when using transforms with modules that require gradients.</span>
<span class="sd">            If not specified, defaults to ``True`` when ``transform_factory`` is provided,</span>
<span class="sd">            and ``False`` otherwise.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.envs import Compose, ToTensorImage, Resize, RenameTransform</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data import TensorDictReplayBuffer, ReplayBufferEnsemble, LazyMemmapStorage</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt; rb0 = TensorDictReplayBuffer(</span>
<span class="sd">        ...     storage=LazyMemmapStorage(10),</span>
<span class="sd">        ...     transform=Compose(</span>
<span class="sd">        ...         ToTensorImage(in_keys=[&quot;pixels&quot;, (&quot;next&quot;, &quot;pixels&quot;)]),</span>
<span class="sd">        ...         Resize(32, in_keys=[&quot;pixels&quot;, (&quot;next&quot;, &quot;pixels&quot;)]),</span>
<span class="sd">        ...         RenameTransform([(&quot;some&quot;, &quot;key&quot;)], [&quot;renamed&quot;]),</span>
<span class="sd">        ...     ),</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rb1 = TensorDictReplayBuffer(</span>
<span class="sd">        ...     storage=LazyMemmapStorage(10),</span>
<span class="sd">        ...     transform=Compose(</span>
<span class="sd">        ...         ToTensorImage(in_keys=[&quot;pixels&quot;, (&quot;next&quot;, &quot;pixels&quot;)]),</span>
<span class="sd">        ...         Resize(32, in_keys=[&quot;pixels&quot;, (&quot;next&quot;, &quot;pixels&quot;)]),</span>
<span class="sd">        ...         RenameTransform([&quot;another_key&quot;], [&quot;renamed&quot;]),</span>
<span class="sd">        ...     ),</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rb = ReplayBufferEnsemble(</span>
<span class="sd">        ...     rb0,</span>
<span class="sd">        ...     rb1,</span>
<span class="sd">        ...     p=[0.5, 0.5],</span>
<span class="sd">        ...     transform=Resize(33, in_keys=[&quot;pixels&quot;], out_keys=[&quot;pixels33&quot;]),</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; print(rb)</span>
<span class="sd">        ReplayBufferEnsemble(</span>
<span class="sd">            storages=StorageEnsemble(</span>
<span class="sd">                storages=(&lt;torchrl.data.replay_buffers.storages.LazyMemmapStorage object at 0x13a2ef430&gt;, &lt;torchrl.data.replay_buffers.storages.LazyMemmapStorage object at 0x13a2f9310&gt;),</span>
<span class="sd">                transforms=[Compose(</span>
<span class="sd">                        ToTensorImage(keys=[&#39;pixels&#39;, (&#39;next&#39;, &#39;pixels&#39;)]),</span>
<span class="sd">                        Resize(w=32, h=32, interpolation=InterpolationMode.BILINEAR, keys=[&#39;pixels&#39;, (&#39;next&#39;, &#39;pixels&#39;)]),</span>
<span class="sd">                        RenameTransform(keys=[(&#39;some&#39;, &#39;key&#39;)])), Compose(</span>
<span class="sd">                        ToTensorImage(keys=[&#39;pixels&#39;, (&#39;next&#39;, &#39;pixels&#39;)]),</span>
<span class="sd">                        Resize(w=32, h=32, interpolation=InterpolationMode.BILINEAR, keys=[&#39;pixels&#39;, (&#39;next&#39;, &#39;pixels&#39;)]),</span>
<span class="sd">                        RenameTransform(keys=[&#39;another_key&#39;]))]),</span>
<span class="sd">            samplers=SamplerEnsemble(</span>
<span class="sd">                samplers=(&lt;torchrl.data.replay_buffers.samplers.RandomSampler object at 0x13a2f9220&gt;, &lt;torchrl.data.replay_buffers.samplers.RandomSampler object at 0x13a2f9f70&gt;)),</span>
<span class="sd">            writers=WriterEnsemble(</span>
<span class="sd">                writers=(&lt;torchrl.data.replay_buffers.writers.TensorDictRoundRobinWriter object at 0x13a2d9b50&gt;, &lt;torchrl.data.replay_buffers.writers.TensorDictRoundRobinWriter object at 0x13a2f95b0&gt;)),</span>
<span class="sd">        batch_size=None,</span>
<span class="sd">        transform=Compose(</span>
<span class="sd">                Resize(w=33, h=33, interpolation=InterpolationMode.BILINEAR, keys=[&#39;pixels&#39;])),</span>
<span class="sd">        collate_fn=&lt;built-in method stack of type object at 0x128648260&gt;)</span>
<span class="sd">        &gt;&gt;&gt; data0 = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         &quot;pixels&quot;: torch.randint(255, (10, 244, 244, 3)),</span>
<span class="sd">        ...         (&quot;next&quot;, &quot;pixels&quot;): torch.randint(255, (10, 244, 244, 3)),</span>
<span class="sd">        ...         (&quot;some&quot;, &quot;key&quot;): torch.randn(10),</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     batch_size=[10],</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; data1 = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         &quot;pixels&quot;: torch.randint(255, (10, 64, 64, 3)),</span>
<span class="sd">        ...         (&quot;next&quot;, &quot;pixels&quot;): torch.randint(255, (10, 64, 64, 3)),</span>
<span class="sd">        ...         &quot;another_key&quot;: torch.randn(10),</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     batch_size=[10],</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; rb[0].extend(data0)</span>
<span class="sd">        &gt;&gt;&gt; rb[1].extend(data1)</span>
<span class="sd">        &gt;&gt;&gt; for _ in range(2):</span>
<span class="sd">        ...     sample = rb.sample(10)</span>
<span class="sd">        ...     assert sample[&quot;next&quot;, &quot;pixels&quot;].shape == torch.Size([2, 5, 3, 32, 32])</span>
<span class="sd">        ...     assert sample[&quot;pixels&quot;].shape == torch.Size([2, 5, 3, 32, 32])</span>
<span class="sd">        ...     assert sample[&quot;pixels33&quot;].shape == torch.Size([2, 5, 3, 33, 33])</span>
<span class="sd">        ...     assert sample[&quot;renamed&quot;].shape == torch.Size([2, 5])</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_collate_fn_val</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">rbs</span><span class="p">,</span>
        <span class="n">storages</span><span class="p">:</span> <span class="n">StorageEnsemble</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">samplers</span><span class="p">:</span> <span class="n">SamplerEnsemble</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">writers</span><span class="p">:</span> <span class="n">WriterEnsemble</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">Transform</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># noqa: F821</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">collate_fns</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">p</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">sample_from_all</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_buffer_sampled</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">generator</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">shared</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>

        <span class="k">if</span> <span class="n">collate_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">collate_fn</span> <span class="o">=</span> <span class="n">_stack_anything</span>

        <span class="k">if</span> <span class="n">rbs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">storages</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">samplers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">writers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span>
            <span class="c1"># Ensure all replay buffers are initialized before creating ensemble</span>
            <span class="k">for</span> <span class="n">rb</span> <span class="ow">in</span> <span class="n">rbs</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="nb">hasattr</span><span class="p">(</span><span class="n">rb</span><span class="p">,</span> <span class="s2">&quot;_delayed_init&quot;</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="n">rb</span><span class="o">.</span><span class="n">_delayed_init</span>
                    <span class="ow">and</span> <span class="ow">not</span> <span class="n">rb</span><span class="o">.</span><span class="n">initialized</span>
                <span class="p">):</span>
                    <span class="n">rb</span><span class="o">.</span><span class="n">_init</span><span class="p">()</span>
            <span class="n">storages</span> <span class="o">=</span> <span class="n">StorageEnsemble</span><span class="p">(</span>
                <span class="o">*</span><span class="p">[</span><span class="n">rb</span><span class="o">.</span><span class="n">_storage</span> <span class="k">for</span> <span class="n">rb</span> <span class="ow">in</span> <span class="n">rbs</span><span class="p">],</span> <span class="n">transforms</span><span class="o">=</span><span class="p">[</span><span class="n">rb</span><span class="o">.</span><span class="n">_transform</span> <span class="k">for</span> <span class="n">rb</span> <span class="ow">in</span> <span class="n">rbs</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">samplers</span> <span class="o">=</span> <span class="n">SamplerEnsemble</span><span class="p">(</span>
                <span class="o">*</span><span class="p">[</span><span class="n">rb</span><span class="o">.</span><span class="n">_sampler</span> <span class="k">for</span> <span class="n">rb</span> <span class="ow">in</span> <span class="n">rbs</span><span class="p">],</span>
                <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
                <span class="n">sample_from_all</span><span class="o">=</span><span class="n">sample_from_all</span><span class="p">,</span>
                <span class="n">num_buffer_sampled</span><span class="o">=</span><span class="n">num_buffer_sampled</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">writers</span> <span class="o">=</span> <span class="n">WriterEnsemble</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">rb</span><span class="o">.</span><span class="n">_writer</span> <span class="k">for</span> <span class="n">rb</span> <span class="ow">in</span> <span class="n">rbs</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">collate_fns</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">collate_fns</span> <span class="o">=</span> <span class="p">[</span><span class="n">rb</span><span class="o">.</span><span class="n">_collate_fn</span> <span class="k">for</span> <span class="n">rb</span> <span class="ow">in</span> <span class="n">rbs</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">rbs</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">collate_fns</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">collate_fns</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">_get_default_collate</span><span class="p">(</span><span class="n">storage</span><span class="p">)</span> <span class="k">for</span> <span class="n">storage</span> <span class="ow">in</span> <span class="n">storages</span><span class="o">.</span><span class="n">_storages</span>
                <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_rbs</span> <span class="o">=</span> <span class="n">rbs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fns</span> <span class="o">=</span> <span class="n">collate_fns</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">storage</span><span class="o">=</span><span class="n">storages</span><span class="p">,</span>
            <span class="n">sampler</span><span class="o">=</span><span class="n">samplers</span><span class="p">,</span>
            <span class="n">writer</span><span class="o">=</span><span class="n">writers</span><span class="p">,</span>
            <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span>
            <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
            <span class="n">shared</span><span class="o">=</span><span class="n">shared</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">sample</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_sample</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">):</span>
            <span class="n">buffer_ids</span> <span class="o">=</span> <span class="n">info</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="s2">&quot;buffer_ids&quot;</span><span class="p">))</span>
            <span class="n">info</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
                <span class="p">(</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="s2">&quot;buffer_ids&quot;</span><span class="p">),</span> <span class="n">expand_right</span><span class="p">(</span><span class="n">buffer_ids</span><span class="p">,</span> <span class="n">sample</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">info</span><span class="p">,</span> <span class="n">LazyStackedTensorDict</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">_info</span><span class="p">,</span> <span class="n">_sample</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="n">info</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">stack_dim</span><span class="p">),</span> <span class="n">sample</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">stack_dim</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="n">_info</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">_sample</span><span class="o">.</span><span class="n">batch_size</span>
                <span class="n">info</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">info</span><span class="o">.</span><span class="n">tensordicts</span><span class="p">,</span> <span class="n">info</span><span class="o">.</span><span class="n">stack_dim</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">info</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">batch_size</span>
            <span class="n">sample</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">sample</span><span class="p">,</span> <span class="n">info</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_collate_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">new_collate</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
            <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_collate_fns</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">sample</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">sample</span><span class="p">)</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">]</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn_val</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">new_collate</span>

    <span class="nd">@_collate_fn</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_collate_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn_val</span> <span class="o">=</span> <span class="n">value</span>

    <span class="n">_INDEX_ERROR</span> <span class="o">=</span> <span class="s2">&quot;Expected an index of type torch.Tensor, range, np.ndarray, int, slice or ellipsis, got </span><span class="si">{}</span><span class="s2"> instead.&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="nb">tuple</span> <span class="o">|</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span> <span class="o">|</span> <span class="nb">list</span> <span class="o">|</span> <span class="nb">slice</span> <span class="o">|</span> <span class="bp">Ellipsis</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="c1"># accepts inputs:</span>
        <span class="c1"># (int | 1d tensor | 1d list | 1d array | slice | ellipsis | range, int | tensor | list | array | slice | ellipsis | range)</span>
        <span class="c1"># tensor</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">Ellipsis</span><span class="p">:</span>
                <span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
            <span class="n">rb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">rb</span> <span class="ow">is</span> <span class="bp">self</span><span class="p">:</span>
                    <span class="c1"># then index[0] is an ellipsis/slice(None)</span>
                    <span class="n">sample</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">storage</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">storage</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="o">.</span><span class="n">_storages</span><span class="p">)</span>
                    <span class="p">]</span>
                    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">rb</span><span class="p">,</span> <span class="n">ReplayBufferEnsemble</span><span class="p">):</span>
                    <span class="n">new_index</span> <span class="o">=</span> <span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">),</span> <span class="o">*</span><span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
                    <span class="k">return</span> <span class="n">rb</span><span class="p">[</span><span class="n">new_index</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">rb</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
            <span class="k">return</span> <span class="n">rb</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">slice</span><span class="p">)</span> <span class="ow">and</span> <span class="n">index</span> <span class="o">==</span> <span class="nb">slice</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">range</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)):</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Cannot index a </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> with tensor indices that have more than one dimension.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                    <span class="s2">&quot;A floating point index was received when an integer dtype was expected.&quot;</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rbs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">slice</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_INDEX_ERROR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">index</span><span class="p">)))</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rbs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_INDEX_ERROR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">index</span><span class="p">)))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rbs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
                <span class="n">rbs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_rbs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">index</span><span class="p">]</span>
                <span class="n">_collate_fns</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_collate_fns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">index</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># slice</span>
                    <span class="n">rbs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rbs</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
                    <span class="n">_collate_fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fns</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
                <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_INDEX_ERROR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">index</span><span class="p">)))</span>
            <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">_p</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="k">return</span> <span class="n">ReplayBufferEnsemble</span><span class="p">(</span>
                <span class="o">*</span><span class="n">rbs</span><span class="p">,</span>
                <span class="n">transform</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">,</span>
                <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn_val</span><span class="p">,</span>
                <span class="n">collate_fns</span><span class="o">=</span><span class="n">_collate_fns</span><span class="p">,</span>
                <span class="n">sample_from_all</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">sample_from_all</span><span class="p">,</span>
                <span class="n">num_buffer_sampled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">num_buffer_sampled</span><span class="p">,</span>
                <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">samplers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="n">writers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="n">storages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">_collate_fns</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_collate_fns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_collate_fns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_collate_fns</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">_p</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="k">except</span> <span class="ne">IndexError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_INDEX_ERROR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">index</span><span class="p">)))</span>

        <span class="k">return</span> <span class="n">ReplayBufferEnsemble</span><span class="p">(</span>
            <span class="n">samplers</span><span class="o">=</span><span class="n">samplers</span><span class="p">,</span>
            <span class="n">writers</span><span class="o">=</span><span class="n">writers</span><span class="p">,</span>
            <span class="n">storages</span><span class="o">=</span><span class="n">storages</span><span class="p">,</span>
            <span class="n">transform</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn_val</span><span class="p">,</span>
            <span class="n">collate_fns</span><span class="o">=</span><span class="n">_collate_fns</span><span class="p">,</span>
            <span class="n">sample_from_all</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">sample_from_all</span><span class="p">,</span>
            <span class="n">num_buffer_sampled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">num_buffer_sampled</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">storages</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;storages=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_storage</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">writers</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;writers=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_writer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">samplers</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;samplers=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;ReplayBufferEnsemble(</span><span class="se">\n</span><span class="si">{</span><span class="n">storages</span><span class="si">}</span><span class="s2">, </span><span class="se">\n</span><span class="si">{</span><span class="n">samplers</span><span class="si">}</span><span class="s2">, </span><span class="se">\n</span><span class="si">{</span><span class="n">writers</span><span class="si">}</span><span class="s2">, </span><span class="se">\n</span><span class="s2">batch_size=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="si">}</span><span class="s2">, </span><span class="se">\n</span><span class="s2">transform=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_transform</span><span class="si">}</span><span class="s2">, </span><span class="se">\n</span><span class="s2">collate_fn=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_collate_fn_val</span><span class="si">}</span><span class="s2">)&quot;</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>