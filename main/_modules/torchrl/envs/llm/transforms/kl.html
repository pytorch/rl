


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torchrl.envs.llm.transforms.kl &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../../../../../versions.html"><span style="font-size:110%">main (0.0.0+unknown) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/export.html">Exporting TorchRL modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/llm_browser.html">TorchRL LLM: Building Tool-Enabled Environments</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../reference/index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../reference/knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../../index.html">Module code</a> &gt;</li>
        
      <li>torchrl.envs.llm.transforms.kl</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
    
    
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torchrl.envs.llm.transforms.kl</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the MIT license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">warnings</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">nullcontext</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">copy</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">TYPE_CHECKING</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict</span><span class="w"> </span><span class="kn">import</span> <span class="n">NestedKey</span><span class="p">,</span> <span class="n">set_list_to_stack</span><span class="p">,</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">unravel_key</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_zip_strict</span><span class="p">,</span> <span class="n">is_seq_of_nested_key</span><span class="p">,</span> <span class="n">logger</span> <span class="k">as</span> <span class="n">torchrl_logger</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.utils.rnn</span><span class="w"> </span><span class="kn">import</span> <span class="n">pad_sequence</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Composite</span><span class="p">,</span> <span class="n">Unbounded</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.tensor_specs</span><span class="w"> </span><span class="kn">import</span> <span class="n">DEVICE_TYPING</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs</span><span class="w"> </span><span class="kn">import</span> <span class="n">EnvBase</span><span class="p">,</span> <span class="n">Transform</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.transforms.ray_service</span><span class="w"> </span><span class="kn">import</span> <span class="n">_RayServiceMetaClass</span><span class="p">,</span> <span class="n">RayTransform</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.transforms.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">Compose</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.transforms.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">_set_missing_tolerance</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules.llm.policies.common</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMWrapperBase</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>


<span class="k">class</span><span class="w"> </span><span class="nc">RayKLRewardTransform</span><span class="p">(</span><span class="n">RayTransform</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A Ray-based implementation of :class:`~torchrl.envs.llm.transforms.kl.KLRewardTransform`.</span>

<span class="sd">    This class creates a Ray remote actor from KLRewardTransform that can be shared across multiple workers.</span>
<span class="sd">    All method calls are delegated to the remote actor, ensuring that multiple environments can</span>
<span class="sd">    share the same KL computation resources.</span>

<span class="sd">    To avoid serialization issues with large models, this class supports model factories</span>
<span class="sd">    that create models on the remote actor rather than passing full models through Ray channels.</span>

<span class="sd">    Args:</span>
<span class="sd">        ref_model (LLMWrapperBase, optional): the reference model. Prefer using a model factory instead</span>
<span class="sd">            to avoid serialization issues.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        ref_model_factory (Callable[[], LLMWrapperBase], optional): A callable that returns a reference model.</span>
<span class="sd">            This allows for explicit resource control and avoids serialization issues.</span>
<span class="sd">        num_cpus (int, optional): Number of CPUs to allocate to the Ray actor. Defaults to 1.</span>
<span class="sd">        num_gpus (int, optional): Number of GPUs to allocate to the Ray actor. Defaults to 0.</span>
<span class="sd">        device (torch.device, optional): Device to use on the remote Ray actor for tensor operations.</span>
<span class="sd">            The local Ray transform will handle CPU serialization and device restoration automatically.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        actor_name (str, optional): Name of the Ray actor to use. If provided, the actor will be reused if it already exists.</span>
<span class="sd">        **kwargs: Additional keyword arguments to pass to KLRewardTransform.</span>

<span class="sd">    Note:</span>
<span class="sd">        When using model factories, the corresponding model argument (ref_model) should be None.</span>
<span class="sd">        Model factories are preferred for large models to avoid serialization overhead.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Option 1: Using model factory for explicit resource control</span>
<span class="sd">        &gt;&gt;&gt; def create_ref_model():</span>
<span class="sd">        ...     return TransformersWrapper(ref_model, tokenizer=tokenizer, generate=False, return_log_probs=True)</span>
<span class="sd">        &gt;&gt;&gt; transform = RayKLRewardTransform(</span>
<span class="sd">        ...     ref_model=None,</span>
<span class="sd">        ...     ref_model_factory=create_ref_model,</span>
<span class="sd">        ...     num_gpus=1,</span>
<span class="sd">        ...     device=torch.device(&quot;cuda&quot;)</span>
<span class="sd">        ... )</span>

<span class="sd">        &gt;&gt;&gt; # Option 2: Pass model directly (Ray handles serialization)</span>
<span class="sd">        &gt;&gt;&gt; transform = RayKLRewardTransform(ref_model=ref_model, device=torch.device(&quot;cuda&quot;))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ref_model</span><span class="p">:</span> <span class="n">LLMWrapperBase</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">ref_model_factory</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">LLMWrapperBase</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_cpus</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_gpus</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">DEVICE_TYPING</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">actor_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Validate arguments: model and factory should not both be provided</span>
        <span class="k">if</span> <span class="n">ref_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">ref_model_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot provide both &#39;ref_model&#39; and &#39;ref_model_factory&#39;. Choose one.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">ref_model</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">ref_model_factory</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Must provide exactly one of &#39;ref_model&#39; or &#39;ref_model_factory&#39;.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Store creation parameters for actor creation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ref_model</span> <span class="o">=</span> <span class="n">ref_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ref_model_factory</span> <span class="o">=</span> <span class="n">ref_model_factory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_creation_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="c1"># Store device separately for passing to remote actor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_remote_device</span> <span class="o">=</span> <span class="n">device</span>

        <span class="c1"># Default num_cpus</span>
        <span class="k">if</span> <span class="n">num_cpus</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_cpus</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># Call parent constructor without device (Ray transform handles CPU/device mapping)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">num_cpus</span><span class="o">=</span><span class="n">num_cpus</span><span class="p">,</span>
            <span class="n">num_gpus</span><span class="o">=</span><span class="n">num_gpus</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Don&#39;t store device locally</span>
            <span class="n">actor_name</span><span class="o">=</span><span class="n">actor_name</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_actor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create the remote KLRewardTransform actor.&quot;&quot;&quot;</span>
        <span class="c1"># Create the remote KLRewardTransform with resource specifications</span>
        <span class="n">RemoteKLRewardTransform</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
            <span class="n">num_cpus</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_cpus</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_gpus</span>
        <span class="p">)(</span><span class="n">KLRewardTransform</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">RemoteKLRewardTransform</span> <span class="o">=</span> <span class="n">RemoteKLRewardTransform</span><span class="o">.</span><span class="n">options</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_actor_name</span>
            <span class="p">)</span>

        <span class="c1"># Determine how to create model on the remote actor</span>
        <span class="n">ref_model_arg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ref_model</span>

        <span class="c1"># If we have factory, we&#39;ll pass it and set model to None</span>
        <span class="n">creation_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_creation_kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ref_model_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">creation_kwargs</span><span class="p">[</span><span class="s2">&quot;ref_model_factory&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ref_model_factory</span>
            <span class="n">ref_model_arg</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Pass device to the remote actor</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remote_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">creation_kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remote_device</span>

        <span class="c1"># Create the shared actor</span>
        <span class="n">actor</span> <span class="o">=</span> <span class="n">RemoteKLRewardTransform</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
            <span class="n">ref_model</span><span class="o">=</span><span class="n">ref_model_arg</span><span class="p">,</span> <span class="o">**</span><span class="n">creation_kwargs</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">actor</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;String representation.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_actor&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_actor</span><span class="o">.</span><span class="fm">__repr__</span><span class="o">.</span><span class="n">remote</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="s2">&quot;RayKLRewardTransform(actor=None)&quot;</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;RayKLRewardTransform(actor=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_actor&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>


<div class="viewcode-block" id="KLRewardTransform"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.KLRewardTransform.html#torchrl.envs.llm.transforms.KLRewardTransform">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">KLRewardTransform</span><span class="p">(</span><span class="n">Transform</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">_RayServiceMetaClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A legacy transform for computing KL divergence-based rewards.</span>

<span class="sd">    **Deprecated**: This transform is maintained for backward compatibility but is no longer</span>
<span class="sd">    the recommended approach. Use :class:`~torchrl.envs.llm.transforms.kl.RetrieveKL` instead,</span>
<span class="sd">    which provides better modularity and integration with the new wrapper design.</span>

<span class="sd">    **Recent Changes:**</span>
<span class="sd">    - **Legacy Status**: This transform is now considered legacy and may not work optimally</span>
<span class="sd">      with the new modular wrapper design.</span>
<span class="sd">    - **ChatHistory Integration**: Limited support for the new :class:`~torchrl.modules.llm.policies.ChatHistory` objects.</span>
<span class="sd">    - **Input Mode Support**: May not handle all input modes (`&quot;history&quot;`, `&quot;text&quot;`, `&quot;tokens&quot;`) consistently.</span>

<span class="sd">    **Recommendation**:</span>
<span class="sd">    Use :class:`~torchrl.envs.llm.transforms.kl.RetrieveKL` for new code, which provides:</span>
<span class="sd">    - Better integration with the new wrapper design</span>
<span class="sd">    - Consistent support for all input modes</span>
<span class="sd">    - Proper handling of ChatHistory objects</span>
<span class="sd">    - More modular and composable architecture</span>

<span class="sd">    Args:</span>
<span class="sd">        ref_model (LLMWrapperBase): the reference model.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        ref_model_factory (Callable[[], LLMWrapperBase], optional): A callable that returns a reference model.</span>
<span class="sd">        assistant_only (bool): whether to only compute KL on assistant tokens. Defaults to `True`.</span>
<span class="sd">        tokenizer (transformers.AutoTokenizer): the tokenizer to use. Defaults to `None`.</span>
<span class="sd">        detach (bool): whether to detach the KL from the computation graph. Defaults to `True`.</span>
<span class="sd">        device (torch.device): the device to cast the tensors to. This is not the device of the specs, but the device</span>
<span class="sd">            onto which the tensors will be moved. It allows to keep the model on a different device</span>
<span class="sd">            than the upcoming data. When using Ray service, this device will be used on the remote actor.</span>
<span class="sd">            Defaults to `None`.</span>
<span class="sd">        padding_side (str): the side of the padding when using pad_sequence. Defaults to `&quot;left&quot;`.</span>
<span class="sd">        use_ray_service (bool, optional): whether to use Ray service. Defaults to `False`.</span>
<span class="sd">        actor_name (str, optional): the name of the Ray actor to use. Defaults to `None`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Legacy usage (not recommended for new code)</span>
<span class="sd">        &gt;&gt;&gt; transform = KLRewardTransform(gen_model, ref_model)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Recommended approach using RetrieveKL</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.envs.llm.transforms.kl import RetrieveKL</span>
<span class="sd">        &gt;&gt;&gt; transform = RetrieveKL(gen_model, ref_model, assistant_only=True)</span>

<span class="sd">    .. seealso::</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.RetrieveKL`: The recommended transform for KL divergence computation.</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.RetrieveLogProb`: Base transform for retrieving log-probabilities.</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.KLComputation`: Transform for computing KL divergence between log-prob tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">DEFAULT_IN_KEYS</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span>
    <span class="n">_RayServiceClass</span> <span class="o">=</span> <span class="n">RayKLRewardTransform</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">ref_model</span><span class="p">:</span> <span class="n">LLMWrapperBase</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">ref_model_factory</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">LLMWrapperBase</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">coef</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">in_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">out_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">log_prob_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;log_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">),</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_to_reward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">assistant_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_side</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;left&quot;</span><span class="p">,</span>
        <span class="n">use_ray_service</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Handle model factory - create model if factory is provided</span>
        <span class="k">if</span> <span class="n">ref_model_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">ref_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot provide both &#39;ref_model&#39; and &#39;ref_model_factory&#39;. Choose one.&quot;</span>
                <span class="p">)</span>
            <span class="n">ref_model</span> <span class="o">=</span> <span class="n">ref_model_factory</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">ref_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Must provide exactly one of &#39;ref_model&#39; or &#39;ref_model_factory&#39;.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">in_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">in_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">DEFAULT_IN_KEYS</span>
        <span class="k">if</span> <span class="n">out_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">out_keys</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">in_keys</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_keys</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_keys</span><span class="p">):</span>
            <span class="n">out_keys</span> <span class="o">=</span> <span class="n">out_keys</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;kl_penalty&quot;</span><span class="p">,</span> <span class="s2">&quot;ref_log_probs&quot;</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_keys</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_keys</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The out_keys must have the same length as the in_keys (plus two additional optional kl entries for logging).&quot;</span>
            <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_keys</span><span class="o">=</span><span class="n">in_keys</span><span class="p">,</span> <span class="n">out_keys</span><span class="o">=</span><span class="n">out_keys</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_seq_of_nested_key</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_keys</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">is_seq_of_nested_key</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;invalid in_keys / out_keys:</span><span class="se">\n</span><span class="s2">in_keys=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">in_keys</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">out_keys=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_keys</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Only one in_key/out_key is allowed, got in_keys=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">in_keys</span><span class="si">}</span><span class="s2">, out_keys=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_out_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">unravel_key</span><span class="p">(</span><span class="n">out_key</span><span class="p">)</span> <span class="k">for</span> <span class="n">out_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_out_keys</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ref_model</span><span class="p">,</span> <span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The actor is configured to generate text, not compute the log-probs.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># update the in_keys for dispatch etc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_keys</span> <span class="o">+</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">in_keys</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">unravel_key</span><span class="p">(</span><span class="n">in_key</span><span class="p">)</span> <span class="k">for</span> <span class="n">in_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_keys</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_to_reward</span> <span class="o">=</span> <span class="n">add_to_reward</span>
        <span class="c1"># check that the model has parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">[</span><span class="s2">&quot;ref_model&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ref_model</span>

        <span class="c1"># self._buffers[&quot;actor_params&quot;] = params.clone().detach()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>

        <span class="c1"># find the sample log-prob key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_prob_full_key</span> <span class="o">=</span> <span class="n">log_prob_key</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assistant_only</span> <span class="o">=</span> <span class="n">assistant_only</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="n">padding_side</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;coef&quot;</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
        <span class="c1"># sanity check for the ref_model</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ref_model</span><span class="p">,</span> <span class="s2">&quot;input_mode&quot;</span><span class="p">,</span> <span class="s2">&quot;tokens&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;tokens&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The ref_model must be configured to use tokens as input. Please set the `input_mode` argument to `tokens`.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">pad_output</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># We need pad_output to match the pad_output of the inference model</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="o">.</span><span class="n">pad_output</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tokenizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tokenizer</span>
        <span class="k">if</span> <span class="n">tokenizer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tokenizer</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="o">.</span><span class="n">tokenizer</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;The ref_model does not have a tokenizer. Please pass the tokenizer to the constructor.&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">set_container</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">container</span><span class="p">:</span> <span class="n">Transform</span> <span class="o">|</span> <span class="n">EnvBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">set_container</span><span class="p">(</span><span class="n">container</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">parent</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;parent&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">parent</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">action_keys</span> <span class="o">=</span> <span class="n">parent</span><span class="o">.</span><span class="n">action_keys</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">action_keys</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;More than one action_key found. Please pass the `action_key` argument directly to </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>
                <span class="n">action_key</span> <span class="o">=</span> <span class="n">action_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">action_key</span> <span class="o">=</span> <span class="n">action_key</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_reset</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">tensordict_reset</span><span class="p">:</span> <span class="n">TensorDictBase</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">_set_missing_tolerance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
            <span class="n">tensordict_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step</span><span class="p">(</span><span class="n">tensordict_reset</span><span class="p">,</span> <span class="n">tensordict_reset</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensordict_reset</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">action_key</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NestedKey</span><span class="p">:</span>
        <span class="c1"># Get the action from the base env (a ChatEnv).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">base_env</span><span class="o">.</span><span class="n">input_mode</span> <span class="o">==</span> <span class="s2">&quot;history&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;history&quot;</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">base_env</span><span class="o">.</span><span class="n">input_mode</span> <span class="o">==</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">base_env</span><span class="o">.</span><span class="n">input_mode</span> <span class="o">==</span> <span class="s2">&quot;tokens&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid input mode: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">base_env</span><span class="o">.</span><span class="n">input_mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">next_tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="n">original_device</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">original_device</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">device</span>
            <span class="n">tensordict</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">next_tensordict</span> <span class="o">=</span> <span class="n">next_tensordict</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># tensordict = self._get_text_response(tensordict, next_tensordict)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_key</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">response</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_tolerance</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Action with key </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">action_key</span><span class="si">}</span><span class="s2"> not found data </span><span class="si">{</span><span class="n">tensordict</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="c1"># being called after reset or without action, skipping</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;reward&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">next_tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">reward_spec</span><span class="o">.</span><span class="n">zero</span><span class="p">())</span>
            <span class="k">return</span> <span class="n">next_tensordict</span>

        <span class="c1"># We use the (&quot;tokens&quot;, &quot;full&quot;) key to get the log-probs of the reference model</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">():</span>
            <span class="n">td_input</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">ref_log_prob_td</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="p">(</span><span class="n">td_input</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_output</span><span class="p">:</span>
            <span class="n">ref_log_prob_padded</span> <span class="o">=</span> <span class="n">ref_log_prob_td</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_prob_full_key</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ref_log_prob_unpadded</span> <span class="o">=</span> <span class="n">ref_log_prob_td</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_prob_full_key</span><span class="p">,</span> <span class="n">as_list</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># type: ignore[misc]</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">assistant_only</span><span class="p">:</span>
            <span class="c1"># Get the assistant mask</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;masks&quot;</span><span class="p">,</span> <span class="s2">&quot;all_assistant_mask&quot;</span><span class="p">))</span>
            <span class="c1"># mask will often be None - fall back on prompt / response separation</span>
            <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_output</span><span class="p">:</span>
                    <span class="c1"># simple case: just take the prompt length</span>
                    <span class="n">prompt_length</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;prompt&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">mask</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;masks&quot;</span><span class="p">,</span> <span class="s2">&quot;all_attention_mask&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                    <span class="n">mask</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">prompt_length</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># simple case: just take the prompt length</span>
                    <span class="n">prompt_length</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;tokens&quot;</span><span class="p">,</span> <span class="s2">&quot;prompt&quot;</span><span class="p">),</span> <span class="n">as_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
                    <span class="p">]</span>
                    <span class="n">mask</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;masks&quot;</span><span class="p">,</span> <span class="s2">&quot;all_attention_mask&quot;</span><span class="p">),</span> <span class="n">as_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt_length</span><span class="p">)):</span>
                        <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                        <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">prompt_length</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="c1"># we want to keep the batch dimension</span>
            <span class="n">ref_log_prob_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_output</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ref_log_prob_padded</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
                    <span class="n">ref_log_prob_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">ref_log_prob_padded</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ref_log_prob_unpadded</span><span class="p">)):</span>
                    <span class="n">ref_log_prob_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">ref_log_prob_unpadded</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_output</span><span class="p">:</span>
                <span class="n">ref_log_prob</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
                    <span class="n">ref_log_prob_list</span><span class="p">,</span>
                    <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">padding_side</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ref_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">(</span>
                    <span class="n">ref_log_prob_list</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">strided</span>
                <span class="p">)</span>

        <span class="c1"># we obtain the current log-probs (already computed) from the current tensordict</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_output</span><span class="p">:</span>
            <span class="n">curr_log_prob_padded</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_prob_full_key</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">curr_log_prob_unpadded</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_prob_full_key</span><span class="p">,</span> <span class="n">as_list</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># type: ignore[misc]</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">assistant_only</span><span class="p">:</span>
            <span class="c1"># we want to keep the batch dimension</span>
            <span class="n">curr_log_prob_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_output</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">curr_log_prob_padded</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
                    <span class="n">curr_log_prob_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">curr_log_prob_padded</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">curr_log_prob_unpadded</span><span class="p">)):</span>
                    <span class="n">curr_log_prob_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">curr_log_prob_unpadded</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_output</span><span class="p">:</span>
                <span class="n">curr_log_prob</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
                    <span class="n">curr_log_prob_list</span><span class="p">,</span>
                    <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">padding_side</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">curr_log_prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">(</span>
                    <span class="n">curr_log_prob_list</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">strided</span>
                <span class="p">)</span>

        <span class="n">ref_log_prob</span> <span class="o">=</span> <span class="n">ref_log_prob</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">curr_log_prob</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># We want the log-probs to have a similar dim to the reward</span>
        <span class="n">curr_log_prob</span> <span class="o">=</span> <span class="n">curr_log_prob</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ref_log_prob</span> <span class="o">=</span> <span class="n">ref_log_prob</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ref_log_prob</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">ref_log_prob</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">curr_log_prob</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="c1"># Don&#39;t check shapes if nested</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;the log-probability tensor shapes must match, got cur_log_prob.shape=</span><span class="si">{</span><span class="n">curr_log_prob</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> and log_prob.shape=</span><span class="si">{</span><span class="n">ref_log_prob</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;One possible reason is that the padding token is identical to the eos token, which means that the eos_token log_prob is truncated from the &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;reference model output.&quot;</span>
                <span class="p">)</span>
        <span class="n">kl</span> <span class="o">=</span> <span class="n">curr_log_prob</span> <span class="o">-</span> <span class="n">ref_log_prob</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_to_reward</span><span class="p">:</span>
            <span class="n">reward_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">next_tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">reward_key</span><span class="p">)</span>
            <span class="c1"># we use the unbiased consistent estimator of the KL: log_p(x) - log_q(x) when x ~ p(x)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">reward</span><span class="o">.</span><span class="n">is_nested</span> <span class="ow">and</span> <span class="n">ref_log_prob</span><span class="o">.</span><span class="n">is_nested</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">nested_tensor</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">rew</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">lp</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">rew</span><span class="p">,</span> <span class="n">lp</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">ref_log_prob</span><span class="p">)],</span>
                    <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">reward</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">curr_log_prob</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;The number of dimensions of reward must be the same as the number of dimensions of the KL &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;term. Got ndim=</span><span class="si">{</span><span class="n">reward</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">curr_log_prob</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2"> respectively.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">reward</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef</span> <span class="o">*</span> <span class="n">kl</span>
            <span class="n">next_tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reward</span><span class="p">)</span>
        <span class="n">next_tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">kl</span><span class="p">)</span>
        <span class="n">next_tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">ref_log_prob</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">original_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">next_tensordict</span> <span class="o">=</span> <span class="n">next_tensordict</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">original_device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">next_tensordict</span>

<div class="viewcode-block" id="KLRewardTransform.forward"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.KLRewardTransform.html#torchrl.envs.llm.transforms.KLRewardTransform.forward">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="n">next_td</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">)</span>
        <span class="n">next_td</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step</span><span class="p">(</span><span class="n">tensordict</span><span class="p">,</span> <span class="n">next_td</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="n">next_td</span><span class="p">)</span></div>

<div class="viewcode-block" id="KLRewardTransform.transform_output_spec"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.KLRewardTransform.html#torchrl.envs.llm.transforms.KLRewardTransform.transform_output_spec">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">transform_output_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
        <span class="n">in_key</span> <span class="o">=</span> <span class="n">unravel_key</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">out_key</span> <span class="o">=</span> <span class="n">unravel_key</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">if</span> <span class="s2">&quot;full_observation_spec&quot;</span> <span class="ow">in</span> <span class="n">output_spec</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">observation_spec</span> <span class="o">=</span> <span class="n">output_spec</span><span class="p">[</span><span class="s2">&quot;full_observation_spec&quot;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">observation_spec</span> <span class="o">=</span> <span class="n">Composite</span><span class="p">(</span>
                <span class="n">shape</span><span class="o">=</span><span class="n">output_spec</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">output_spec</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">output_spec</span><span class="p">[</span><span class="s2">&quot;full_observation_spec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">observation_spec</span>

        <span class="k">if</span> <span class="n">in_key</span> <span class="o">==</span> <span class="s2">&quot;reward&quot;</span> <span class="ow">and</span> <span class="n">out_key</span> <span class="o">==</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span>
            <span class="n">parent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span>

            <span class="n">reward_keys</span> <span class="o">=</span> <span class="n">parent</span><span class="o">.</span><span class="n">reward_keys</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">reward_keys</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">reward_key</span> <span class="o">=</span> <span class="n">reward_keys</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="n">output_spec</span><span class="p">[</span><span class="s2">&quot;full_reward_spec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="k">elif</span> <span class="s2">&quot;reward&quot;</span> <span class="ow">in</span> <span class="n">reward_keys</span><span class="p">:</span>
                <span class="n">reward_key</span> <span class="o">=</span> <span class="s2">&quot;reward&quot;</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="n">output_spec</span><span class="p">[</span><span class="s2">&quot;full_reward_spec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="n">output_spec</span><span class="o">.</span><span class="n">shape</span>
                <span class="n">reward_key</span> <span class="o">=</span> <span class="s2">&quot;reward&quot;</span>
            <span class="c1"># For LLMs, the shape of the reward is (batch, -1, 1)</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">reward_spec</span> <span class="o">=</span> <span class="n">Unbounded</span><span class="p">(</span>
                <span class="n">device</span><span class="o">=</span><span class="n">output_spec</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">output_spec</span><span class="p">[</span><span class="s2">&quot;full_reward_spec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Composite</span><span class="p">(</span>
                <span class="p">{</span><span class="n">reward_key</span><span class="p">:</span> <span class="n">reward_spec</span><span class="p">},</span>
                <span class="n">shape</span><span class="o">=</span><span class="n">output_spec</span><span class="p">[</span><span class="s2">&quot;full_reward_spec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">in_key</span> <span class="o">==</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span>
            <span class="c1"># TODO: we should at least allow to make this a component of the reward specs, to avoid a call during reset</span>
            <span class="n">parent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span>
            <span class="n">reward_spec</span> <span class="o">=</span> <span class="n">output_spec</span><span class="p">[</span><span class="s2">&quot;full_reward_spec&quot;</span><span class="p">][</span><span class="n">parent</span><span class="o">.</span><span class="n">reward_key</span><span class="p">]</span>

            <span class="n">shape</span> <span class="o">=</span> <span class="n">output_spec</span><span class="p">[</span><span class="s2">&quot;full_reward_spec&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="c1"># For LLMs, the shape of the reward is (batch, -1, 1)</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">reward_spec</span> <span class="o">=</span> <span class="n">reward_spec</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">reward_spec</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>

            <span class="c1"># then we need to populate the output keys</span>
            <span class="n">observation_spec</span><span class="p">[</span><span class="n">out_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward_spec</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">observation_spec</span> <span class="o">=</span> <span class="n">output_spec</span><span class="p">[</span><span class="s2">&quot;full_observation_spec&quot;</span><span class="p">]</span>
            <span class="n">reward_spec</span> <span class="o">=</span> <span class="n">observation_spec</span><span class="p">[</span><span class="n">in_key</span><span class="p">]</span>

            <span class="n">shape</span> <span class="o">=</span> <span class="n">observation_spec</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">reward_spec</span> <span class="o">=</span> <span class="n">reward_spec</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">reward_spec</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>

            <span class="c1"># then we need to populate the output keys</span>
            <span class="n">observation_spec</span><span class="p">[</span><span class="n">out_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward_spec</span>

        <span class="n">observation_spec</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">out_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">reward_spec</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">output_spec</span></div></div>


<div class="viewcode-block" id="RetrieveLogProb"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveLogProb.html#torchrl.envs.llm.transforms.RetrieveLogProb">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">RetrieveLogProb</span><span class="p">(</span><span class="n">Transform</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A transform to retrieve log-probabilities from a model for KL divergence computation.</span>

<span class="sd">    This transform computes log-probabilities from a reference model, which can then be used</span>
<span class="sd">    to compute KL divergence with another model&#39;s log-probabilities. It&#39;s designed to work</span>
<span class="sd">    with the :class:`~torchrl.envs.llm.transforms.kl.RetrieveKL` and :class:`~torchrl.envs.llm.transforms.kl.KLComputation` transforms.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (LLMWrapperBase): the model to use to compute the log-probs.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        log_probs_full_key (NestedKey): the key where the log-probs are stored.</span>
<span class="sd">            If not provided, the key will be retrieved from the model&#39;s `log_probs_key` attribute</span>
<span class="sd">            (i.e., `(model.log_probs_key, &quot;full&quot;)`).</span>
<span class="sd">        assistant_only (bool): whether to zero out the log-probs of the non-assistant tokens (i.e., steps of history</span>
<span class="sd">            where the role is not `&quot;assistant&quot;`). Defaults to `True`.</span>

<span class="sd">            .. note:: When `assistant_only=True`, the model must have `input_mode=&#39;history&#39;` to properly identify</span>
<span class="sd">                assistant tokens. For other input modes (`&quot;text&quot;` or `&quot;tokens&quot;`), set `assistant_only=False`.</span>
<span class="sd">                This ensures users are conscious of the limitation that assistant token identification requires</span>
<span class="sd">                structured conversation history.</span>

<span class="sd">        tokenizer_kwargs (dict): the keyword arguments to pass to the tokenizer to be used to apply the chat template to the history when `assistant_only` is `True`.</span>
<span class="sd">            To control the tokenization in the ref_model, pass the tokenizer kwargs to the ref_model constructor.</span>
<span class="sd">            Defaults to `{&quot;return_assistant_tokens_mask&quot;: True, &quot;tokenize&quot;: True, &quot;return_dict&quot;: True, &quot;padding&quot;: False, &quot;add_generation_prompt&quot;: False}`.</span>
<span class="sd">        tokenizer (transformers.AutoTokenizer): the tokenizer to be used to tokenize the input and compute the assitant mask. If not provided, the tokenizer will be inferred from the `ref_model`.</span>
<span class="sd">        detach (bool): whether to exclude the log-probs from the gradient computation. Defaults to `True`.</span>
<span class="sd">        device (torch.device): the device to use for tensor creation. Defaults to `None`.</span>
<span class="sd">        padding_side (str): the side of the padding when using pad_sequence. Defaults to `&quot;left&quot;`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.llm import History</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.modules.llm import TransformersWrapper</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.modules.llm.policies import ChatHistory</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoTokenizer, OPTConfig, OPTForCausalLM</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict, set_list_to_stack</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Set up list to stack for History</span>
<span class="sd">        &gt;&gt;&gt; set_list_to_stack(True).set()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create chat data</span>
<span class="sd">        &gt;&gt;&gt; chats = [</span>
<span class="sd">        ...     [</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello, how are you?&quot;},</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;I&#39;m doing well, thank you!&quot;},</span>
<span class="sd">        ...     ],</span>
<span class="sd">        ...     [</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What&#39;s the weather like?&quot;},</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;I can&#39;t check the weather for you.&quot;},</span>
<span class="sd">        ...     ],</span>
<span class="sd">        ... ]</span>
<span class="sd">        &gt;&gt;&gt; history = History.from_chats(chats)</span>
<span class="sd">        &gt;&gt;&gt; print(f&quot;Created history with shape: {history.shape}&quot;)</span>
<span class="sd">        Created history with shape: torch.Size([2, 3])</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Setup tokenizer and model</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/opt-125m&quot;)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer.pad_token = tokenizer.eos_token</span>
<span class="sd">        &gt;&gt;&gt; model = OPTForCausalLM(OPTConfig()).eval()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create reference model</span>
<span class="sd">        &gt;&gt;&gt; ref_model = TransformersWrapper(</span>
<span class="sd">        ...     model,</span>
<span class="sd">        ...     tokenizer=tokenizer,</span>
<span class="sd">        ...     input_mode=&quot;history&quot;,</span>
<span class="sd">        ...     generate=False,</span>
<span class="sd">        ...     return_log_probs=True,</span>
<span class="sd">        ...     pad_output=True,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create the RetrieveLogProb transform</span>
<span class="sd">        &gt;&gt;&gt; transform = RetrieveLogProb(</span>
<span class="sd">        ...     ref_model,</span>
<span class="sd">        ...     assistant_only=True,</span>
<span class="sd">        ...     tokenizer=tokenizer,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Prepare data using ChatHistory</span>
<span class="sd">        &gt;&gt;&gt; chat_history = ChatHistory(full=history)</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict(history=chat_history, batch_size=(2,))</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Apply the transform to get reference log probabilities</span>
<span class="sd">        &gt;&gt;&gt; result = transform(data)</span>
<span class="sd">        &gt;&gt;&gt; log_probs_key = (ref_model.log_probs_key, &quot;full&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ref_log_probs = result.get(log_probs_key)</span>
<span class="sd">        &gt;&gt;&gt; print(f&quot;Log-probs shape: {ref_log_probs.shape}&quot;)</span>
<span class="sd">        Log-probs shape: torch.Size([2, 26])</span>

<span class="sd">    .. note::</span>
<span class="sd">        By default, the log-probabilities are stored as a list of tensors (one per sample, with variable length).</span>
<span class="sd">        Use `as_padded_tensor=True` in `.get()` to obtain a batchable tensor (with padding).</span>
<span class="sd">        The reference log probabilities are computed only for assistant tokens when `assistant_only=True`.</span>

<span class="sd">        **Input Mode Compatibility:**</span>
<span class="sd">        - When `assistant_only=True` (default), the model must have `input_mode=&#39;history&#39;` to properly identify assistant tokens.</span>
<span class="sd">        - When `assistant_only=False`, the transform works with any input mode (`&quot;history&quot;`, `&quot;text&quot;`, or `&quot;tokens&quot;`).</span>
<span class="sd">        - This design ensures users are conscious of the limitation that assistant token identification requires structured conversation history.</span>

<span class="sd">    .. seealso::</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.RetrieveKL`: A higher-level transform that combines two `RetrieveLogProb` instances with `KLComputation`.</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.KLComputation`: A transform that computes KL divergence between two log-prob tensors.</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.KLRewardTransform`: A legacy transform for KL reward computation (use `RetrieveKL` instead).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">LLMWrapperBase</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">log_probs_full_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">assistant_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">tokenizer_kwargs</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">detach</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">padding_side</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;left&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Set up keys</span>
        <span class="k">if</span> <span class="n">log_probs_full_key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">log_probs_full_key</span> <span class="o">=</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">log_probs_key</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">log_probs_full_key</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">log_probs_full_key</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;full&quot;</span>
        <span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The log_probs_full_key </span><span class="si">{</span><span class="n">log_probs_full_key</span><span class="si">}</span><span class="s2"> is not a tuple or does not end with &#39;full&#39;. &quot;</span>
                <span class="s2">&quot;This may cause issues with the KL computation. &quot;</span>
                <span class="s2">&quot;Please use a tuple with the log_probs_key and &#39;full&#39; as the last element.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_probs_full_key</span> <span class="o">=</span> <span class="n">log_probs_full_key</span>

        <span class="c1"># Set up input/output keys</span>
        <span class="n">in_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">in_keys</span><span class="p">)</span>
        <span class="n">out_keys</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">log_probs_full_key</span><span class="p">]</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_keys</span><span class="o">=</span><span class="n">in_keys</span><span class="p">,</span> <span class="n">out_keys</span><span class="o">=</span><span class="n">out_keys</span><span class="p">)</span>

        <span class="c1"># Store model and configuration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assistant_only</span> <span class="o">=</span> <span class="n">assistant_only</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">detach</span> <span class="o">=</span> <span class="n">detach</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="n">padding_side</span>

        <span class="c1"># Set up tokenizer kwargs</span>
        <span class="k">if</span> <span class="n">tokenizer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tokenizer_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">tokenizer_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;return_assistant_tokens_mask&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">tokenizer_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;tokenize&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">tokenizer_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;return_dict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">tokenizer_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;padding&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="n">tokenizer_kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s2">&quot;add_generation_prompt&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer_kwargs</span> <span class="o">=</span> <span class="n">tokenizer_kwargs</span>

        <span class="c1"># Validate model configuration (after setting assistant_only)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_model_config</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_validate_model_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">LLMWrapperBase</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Validate model configuration.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;return_log_probs&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The model must have `return_log_probs=True` to use the `RetrieveLogProb` transform.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The model must have `generate=False` to use the `RetrieveLogProb` transform.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Check input mode compatibility with assistant_only</span>
        <span class="n">input_mode</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;input_mode&quot;</span><span class="p">,</span> <span class="s2">&quot;history&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">assistant_only</span> <span class="ow">and</span> <span class="n">input_mode</span> <span class="o">!=</span> <span class="s2">&quot;history&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The model must have `input_mode=&#39;history&#39;` when `assistant_only=True`. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Current input_mode is &#39;</span><span class="si">{</span><span class="n">input_mode</span><span class="si">}</span><span class="s2">&#39;. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;To use input_mode &#39;</span><span class="si">{</span><span class="n">input_mode</span><span class="si">}</span><span class="s2">&#39;, set `assistant_only=False`.&quot;</span>
            <span class="p">)</span>

<div class="viewcode-block" id="RetrieveLogProb.forward"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveLogProb.html#torchrl.envs.llm.transforms.RetrieveLogProb.forward">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="n">next_td</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">)</span>
        <span class="n">next_is_none</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">next_td</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">next_is_none</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">next_td</span> <span class="o">=</span> <span class="n">tensordict</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step</span><span class="p">(</span><span class="n">tensordict</span><span class="p">,</span> <span class="n">next_td</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">next_is_none</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span>
        <span class="k">return</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_mask_assistant_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">td</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">lp_key</span><span class="p">:</span> <span class="n">NestedKey</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Mask log-probs to only include assistant tokens.</span>

<span class="sd">        Args:</span>
<span class="sd">            td: TensorDict containing the data</span>
<span class="sd">            lp_key: Key for log-probs in the TensorDict</span>

<span class="sd">        Returns:</span>
<span class="sd">            Masked log-probs tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">nullcontext</span><span class="p">():</span>
            <span class="c1"># Get assistant mask</span>
            <span class="n">assistant_masks</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="s2">&quot;masks&quot;</span><span class="p">,</span> <span class="s2">&quot;all_assistant_mask&quot;</span><span class="p">),</span> <span class="n">as_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
            <span class="n">log_probs</span> <span class="o">=</span> <span class="n">td</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">lp_key</span><span class="p">,</span> <span class="n">as_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
            <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">lp</span><span class="p">,</span> <span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">lp</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="n">_zip_strict</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">assistant_masks</span><span class="p">)</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">pad_output</span><span class="p">:</span>
                <span class="n">log_probs</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
                    <span class="n">log_probs</span><span class="p">,</span>
                    <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                    <span class="n">padding_side</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">as_nested_tensor</span><span class="p">(</span>
                    <span class="n">log_probs</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">layout</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">log_probs</span>

    <span class="nd">@set_list_to_stack</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">next_tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="c1"># Compute log-probs using the model</span>
        <span class="c1"># Use tensordict since we want to process the &quot;full&quot; entry</span>
        <span class="n">ref_td</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">tensordict</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">tmp_log_probs_key</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">log_probs_key</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">)</span>

        <span class="c1"># Apply assistant masking if requested</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">assistant_only</span><span class="p">:</span>
            <span class="n">log_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mask_assistant_tokens</span><span class="p">(</span><span class="n">ref_td</span><span class="p">,</span> <span class="n">tmp_log_probs_key</span><span class="p">)</span>
            <span class="n">ref_td</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">tmp_log_probs_key</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">)</span>

        <span class="c1"># Rename and store the log-probs</span>
        <span class="k">if</span> <span class="n">tmp_log_probs_key</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_probs_full_key</span><span class="p">:</span>
            <span class="n">ref_td</span><span class="o">.</span><span class="n">rename_key_</span><span class="p">(</span><span class="n">tmp_log_probs_key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_probs_full_key</span><span class="p">)</span>
        <span class="n">next_tensordict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">ref_td</span><span class="p">,</span> <span class="n">keys_to_update</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_probs_full_key</span><span class="p">,))</span>

        <span class="k">return</span> <span class="n">next_tensordict</span>

<div class="viewcode-block" id="RetrieveLogProb.transform_observation_spec"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveLogProb.html#torchrl.envs.llm.transforms.RetrieveLogProb.transform_observation_spec">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">transform_observation_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
        <span class="c1"># Add kl to observation spec</span>
        <span class="n">observation_spec</span><span class="p">[</span><span class="s2">&quot;kl_penalty&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">Unbounded</span><span class="p">(</span>
            <span class="n">device</span><span class="o">=</span><span class="n">observation_spec</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">observation_spec</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">observation_spec</span></div></div>


<span class="k">class</span><span class="w"> </span><span class="nc">RayRetrieveKL</span><span class="p">(</span><span class="n">RayTransform</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A Ray-based implementation of :class:`~torchrl.envs.llm.transforms.kl.RetrieveKL`.</span>

<span class="sd">    This class creates a Ray remote actor from RetrieveKL that can be shared across multiple workers.</span>
<span class="sd">    All method calls are delegated to the remote actor, ensuring that multiple environments can</span>
<span class="sd">    share the same KL computation resources.</span>

<span class="sd">    To avoid serialization issues with large models, this class supports model factories</span>
<span class="sd">    that create models on the remote actor rather than passing full models through Ray channels.</span>

<span class="sd">    Args:</span>
<span class="sd">        gen_model (LLMWrapperBase | Literal[&quot;from_collector&quot;]): the generation model, or &quot;from_collector&quot; for lazy initialization.</span>
<span class="sd">            Prefer using a model factory instead to avoid serialization issues.</span>
<span class="sd">        ref_model (LLMWrapperBase | None): the reference model. Prefer using a model factory instead</span>
<span class="sd">            to avoid serialization issues.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        gen_model_factory (Callable[[], LLMWrapperBase], optional): A callable that returns a generation model.</span>
<span class="sd">            This allows for explicit resource control and avoids serialization issues.</span>
<span class="sd">        ref_model_factory (Callable[[], LLMWrapperBase], optional): A callable that returns a reference model.</span>
<span class="sd">            This allows for explicit resource control and avoids serialization issues.</span>
<span class="sd">        num_cpus (int, optional): Number of CPUs to allocate to the Ray actor. Defaults to 1.</span>
<span class="sd">        num_gpus (int, optional): Number of GPUs to allocate to the Ray actor. Defaults to 0.</span>
<span class="sd">        device (torch.device, optional): Device to use on the remote Ray actor for tensor operations.</span>
<span class="sd">            The local Ray transform will handle CPU serialization and device restoration automatically.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        actor_name (str, optional): Name of the Ray actor to use. If provided, the actor will be reused if it already exists.</span>
<span class="sd">        **kwargs: Additional keyword arguments to pass to RetrieveKL.</span>

<span class="sd">    Note:</span>
<span class="sd">        When using model factories, the corresponding model arguments (gen_model, ref_model) should be None.</span>
<span class="sd">        Model factories are preferred for large models to avoid serialization overhead.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Option 1: Using model factories for explicit resource control</span>
<span class="sd">        &gt;&gt;&gt; def create_gen_model():</span>
<span class="sd">        ...     return TransformersWrapper(model, tokenizer=tokenizer, generate=False, return_log_probs=True)</span>
<span class="sd">        &gt;&gt;&gt; def create_ref_model():</span>
<span class="sd">        ...     return TransformersWrapper(ref_model, tokenizer=tokenizer, generate=False, return_log_probs=True)</span>
<span class="sd">        &gt;&gt;&gt; transform = RayRetrieveKL(</span>
<span class="sd">        ...     gen_model=None, ref_model=None,</span>
<span class="sd">        ...     gen_model_factory=create_gen_model,</span>
<span class="sd">        ...     ref_model_factory=create_ref_model,</span>
<span class="sd">        ...     num_gpus=1,</span>
<span class="sd">        ...     device=torch.device(&quot;cuda&quot;)</span>
<span class="sd">        ... )</span>

<span class="sd">        &gt;&gt;&gt; # Option 2: Pass models directly (Ray handles serialization)</span>
<span class="sd">        &gt;&gt;&gt; transform = RayRetrieveKL(gen_model=gen_model, ref_model=ref_model, device=torch.device(&quot;cuda&quot;))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">gen_model</span><span class="p">:</span> <span class="n">LLMWrapperBase</span> <span class="o">|</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;from_collector&quot;</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="s2">&quot;from_collector&quot;</span><span class="p">,</span>
        <span class="n">ref_model</span><span class="p">:</span> <span class="n">LLMWrapperBase</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">gen_model_factory</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">LLMWrapperBase</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ref_model_factory</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">LLMWrapperBase</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_cpus</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_gpus</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">DEVICE_TYPING</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">actor_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Validate arguments: models and factories should not both be provided</span>
        <span class="k">if</span> <span class="n">gen_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gen_model_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot provide both &#39;gen_model&#39; and &#39;gen_model_factory&#39;. Choose one.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">ref_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">ref_model_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot provide both &#39;ref_model&#39; and &#39;ref_model_factory&#39;. Choose one.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Store creation parameters for actor creation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gen_model</span> <span class="o">=</span> <span class="n">gen_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ref_model</span> <span class="o">=</span> <span class="n">ref_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gen_model_factory</span> <span class="o">=</span> <span class="n">gen_model_factory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ref_model_factory</span> <span class="o">=</span> <span class="n">ref_model_factory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_creation_kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
        <span class="c1"># Store device separately for passing to remote actor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_remote_device</span> <span class="o">=</span> <span class="n">device</span>

        <span class="c1"># Default num_cpus</span>
        <span class="k">if</span> <span class="n">num_cpus</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_cpus</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># Call parent constructor without device (Ray transform handles CPU/device mapping)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">num_cpus</span><span class="o">=</span><span class="n">num_cpus</span><span class="p">,</span>
            <span class="n">num_gpus</span><span class="o">=</span><span class="n">num_gpus</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Don&#39;t store device locally</span>
            <span class="n">actor_name</span><span class="o">=</span><span class="n">actor_name</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_create_actor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create the remote RetrieveKL actor.&quot;&quot;&quot;</span>
        <span class="c1"># Create the remote RetrieveKL with resource specifications</span>
        <span class="n">RemoteRetrieveKL</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
            <span class="n">num_cpus</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_cpus</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_gpus</span>
        <span class="p">)(</span><span class="n">RetrieveKL</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor_name</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">RemoteRetrieveKL</span> <span class="o">=</span> <span class="n">RemoteRetrieveKL</span><span class="o">.</span><span class="n">options</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_actor_name</span><span class="p">)</span>

        <span class="c1"># Determine how to create models on the remote actor</span>
        <span class="n">gen_model_arg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gen_model</span>
        <span class="n">ref_model_arg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ref_model</span>

        <span class="c1"># If we have factories, we&#39;ll pass them and set models to None</span>
        <span class="n">creation_kwargs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_creation_kwargs</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gen_model_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">creation_kwargs</span><span class="p">[</span><span class="s2">&quot;gen_model_factory&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gen_model_factory</span>
            <span class="n">gen_model_arg</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ref_model_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">creation_kwargs</span><span class="p">[</span><span class="s2">&quot;ref_model_factory&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ref_model_factory</span>
            <span class="n">ref_model_arg</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Pass device to the remote actor</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remote_device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">creation_kwargs</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_remote_device</span>

        <span class="c1"># Create the shared actor</span>
        <span class="n">actor</span> <span class="o">=</span> <span class="n">RemoteRetrieveKL</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span>
            <span class="n">gen_model</span><span class="o">=</span><span class="n">gen_model_arg</span><span class="p">,</span> <span class="n">ref_model</span><span class="o">=</span><span class="n">ref_model_arg</span><span class="p">,</span> <span class="o">**</span><span class="n">creation_kwargs</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">actor</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;String representation.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_actor&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_actor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_actor</span><span class="o">.</span><span class="fm">__repr__</span><span class="o">.</span><span class="n">remote</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="s2">&quot;RayRetrieveKL(actor=None)&quot;</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;RayRetrieveKL(actor=</span><span class="si">{</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_actor&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;None&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>


<div class="viewcode-block" id="RetrieveKL"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveKL.html#torchrl.envs.llm.transforms.RetrieveKL">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">RetrieveKL</span><span class="p">(</span><span class="n">Compose</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">_RayServiceMetaClass</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A transform to retrieve the KL divergence between two models&#39; log-probabilities.</span>

<span class="sd">    This transform combines two :class:`~torchrl.envs.llm.transforms.kl.RetrieveLogProb` instances</span>
<span class="sd">    with a :class:`~torchrl.envs.llm.transforms.kl.KLComputation` to compute KL divergence</span>
<span class="sd">    between a generation model and a reference model.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Both gen_model and ref_model must use the same pad_output value (True or False), otherwise KL computation will fail.</span>

<span class="sd">    Args:</span>
<span class="sd">        gen_model (LLMWrapperBase): the generation model, wrapped in such a way that it does not generate but computes the log-probs.</span>
<span class="sd">            In cases where the transform is used within a :class:`~torchrl.collectors.llm.LLMCollector` run on a remote worker, the</span>
<span class="sd">            policy may not be available ahead of time. In this case, the `gen_model` can be set to `&quot;from_collector&quot;` (default) to retrieve the</span>
<span class="sd">            policy from the collector. See :meth:`~torchrl.modules.llm.policies.LLMWrapperBase.get_new_version` for more details</span>
<span class="sd">            about generating a new version of the policy to gather the log-probs.</span>
<span class="sd">        ref_model (LLMWrapperBase): the reference model, wrapped in such a way that it does not generate but computes the log-probs.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        gen_model_factory (Callable[[], LLMWrapperBase], optional): A callable that returns a generation model.</span>
<span class="sd">            This allows for explicit resource control and avoids serialization issues when using Ray.</span>
<span class="sd">        ref_model_factory (Callable[[], LLMWrapperBase], optional): A callable that returns a reference model.</span>
<span class="sd">            This allows for explicit resource control and avoids serialization issues when using Ray.</span>
<span class="sd">        assistant_only (bool): whether to only retrieve the log-probs of the assistant tokens (i.e., steps of history</span>
<span class="sd">            where the role is `&quot;assistant&quot;`). Defaults to `True`.</span>

<span class="sd">            .. note:: When `assistant_only=True`, both models must have `input_mode=&#39;history&#39;` to properly identify assistant tokens.</span>
<span class="sd">                For other input modes (`&quot;text&quot;` or `&quot;tokens&quot;`), set `assistant_only=False`.</span>
<span class="sd">                This ensures users are conscious of the limitation that assistant token identification requires structured conversation history.</span>

<span class="sd">        gen_log_probs_full_key (str): the key where the log-probs of the generation model are stored. Defaults to `(&quot;log_probs&quot;, &quot;full&quot;)`.</span>
<span class="sd">        ref_log_probs_full_key (str): the key where the log-probs of the reference model are stored. Defaults to `(&quot;ref_log_probs&quot;, &quot;full&quot;)`.</span>
<span class="sd">        history_key (str): the key where the history is stored. Defaults to `&quot;history&quot;`.</span>
<span class="sd">        tokenizer_kwargs (dict): the keyword arguments to pass to the tokenizer to be used to apply the chat template to the history when `assistant_only` is `True`.</span>
<span class="sd">            To control the tokenization in the actor, pass the tokenizer kwargs to the actor constructor.</span>
<span class="sd">            Defaults to `{&quot;return_assistant_tokens_mask&quot;: True, &quot;tokenize&quot;: True, &quot;return_tensors&quot;: &quot;pt&quot;, &quot;padding&quot;: True, &quot;add_generation_prompt&quot;: False}`.</span>
<span class="sd">        detach (bool): whether to exclude the log-probs from the gradient computation. Defaults to `True`.</span>
<span class="sd">        device (torch.device): the device to cast the tensors to. This is not the device of the specs, but the device</span>
<span class="sd">            onto which the tensors will be moved. It allows to keep the model on a different device</span>
<span class="sd">            than the upcoming data itself. When using Ray service, this device will be used on the remote actor.</span>
<span class="sd">            Defaults to `None`.</span>
<span class="sd">        tokenizer (transformers.AutoTokenizer): the tokenizer to be used to tokenize the input and compute the assitant mask. If not provided, the tokenizer will be inferred from the `actor`.</span>
<span class="sd">        padding_side (str): the side of the padding when using pad_sequence. Defaults to `&quot;left&quot;`.</span>
<span class="sd">        kl_key (NestedKey): the key where the KL divergence is stored. Defaults to `&quot;kl_penalty&quot;`.</span>
<span class="sd">        add_to_reward (bool): whether to add the KL divergence to the reward. Defaults to `True`.</span>
<span class="sd">        coeff (float): the coefficient for the KL term when adding to reward. Defaults to `1.0`.</span>
<span class="sd">        padding_side (str): the side of the padding when using pad_sequence. Defaults to `&quot;left&quot;`.</span>
<span class="sd">        use_ray_service (bool, optional): if ``True``, returns a :class:`RayRetrieveKL` instance instead,</span>
<span class="sd">            which creates a Ray actor for shared KL computation across multiple environments.</span>
<span class="sd">            Defaults to ``False``.</span>
<span class="sd">        actor_name (str, optional): the name of the Ray actor to use. Defaults to `None`.</span>
<span class="sd">        **kwargs: additional arguments to pass to the `RetrieveLogProb` transform.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.data.llm import History</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.modules.llm import TransformersWrapper</span>
<span class="sd">        &gt;&gt;&gt; from torchrl.modules.llm.policies import ChatHistory</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoTokenizer, OPTConfig, OPTForCausalLM</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict, set_list_to_stack</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Set up list to stack for History</span>
<span class="sd">        &gt;&gt;&gt; set_list_to_stack(True).set()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create chat data</span>
<span class="sd">        &gt;&gt;&gt; chats = [</span>
<span class="sd">        ...     [</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello, how are you?&quot;},</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;I&#39;m doing well, thank you!&quot;},</span>
<span class="sd">        ...     ],</span>
<span class="sd">        ...     [</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What&#39;s the weather like?&quot;},</span>
<span class="sd">        ...         {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;I can&#39;t check the weather for you.&quot;},</span>
<span class="sd">        ...     ],</span>
<span class="sd">        ... ]</span>
<span class="sd">        &gt;&gt;&gt; history = History.from_chats(chats)</span>
<span class="sd">        &gt;&gt;&gt; print(f&quot;Created history with shape: {history.shape}&quot;)</span>
<span class="sd">        Created history with shape: torch.Size([2, 3])</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Setup tokenizer and model</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/opt-125m&quot;)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer.pad_token = tokenizer.eos_token</span>
<span class="sd">        &gt;&gt;&gt; model = OPTForCausalLM(OPTConfig()).eval()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create generation and reference models</span>
<span class="sd">        &gt;&gt;&gt; gen_model = TransformersWrapper(</span>
<span class="sd">        ...     model,</span>
<span class="sd">        ...     tokenizer=tokenizer,</span>
<span class="sd">        ...     input_mode=&quot;history&quot;,</span>
<span class="sd">        ...     generate=False,</span>
<span class="sd">        ...     return_log_probs=True,</span>
<span class="sd">        ...     pad_output=True,</span>
<span class="sd">        ...     log_probs_key=&quot;gen_log_probs&quot;,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; ref_model = TransformersWrapper(</span>
<span class="sd">        ...     model,</span>
<span class="sd">        ...     tokenizer=tokenizer,</span>
<span class="sd">        ...     input_mode=&quot;history&quot;,</span>
<span class="sd">        ...     generate=False,</span>
<span class="sd">        ...     return_log_probs=True,</span>
<span class="sd">        ...     pad_output=True,</span>
<span class="sd">        ...     log_probs_key=&quot;ref_log_probs&quot;,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create RetrieveKL transform</span>
<span class="sd">        &gt;&gt;&gt; transform = RetrieveKL(</span>
<span class="sd">        ...     gen_model=gen_model,</span>
<span class="sd">        ...     ref_model=ref_model,</span>
<span class="sd">        ...     assistant_only=True,</span>
<span class="sd">        ...     tokenizer=tokenizer,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Prepare data with next tensordict using ChatHistory</span>
<span class="sd">        &gt;&gt;&gt; chat_history = ChatHistory(full=history)</span>
<span class="sd">        &gt;&gt;&gt; next_td = TensorDict(history=chat_history, batch_size=(2,))</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict(history=chat_history, next=next_td, batch_size=(2,))</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Apply transform</span>
<span class="sd">        &gt;&gt;&gt; result = transform(data)</span>
<span class="sd">        &gt;&gt;&gt; kl = result[&quot;next&quot;].get(&quot;kl_penalty&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(f&quot;KL shape: {kl.shape}&quot;)</span>
<span class="sd">        KL shape: torch.Size([2, 26])</span>

<span class="sd">    Note:</span>
<span class="sd">        **Input Mode Compatibility:**</span>
<span class="sd">        - When `assistant_only=True`, both models must have `input_mode=&#39;history&#39;` to properly identify assistant tokens.</span>
<span class="sd">        - When `assistant_only=False`, the transform works with any input mode (`&quot;history&quot;`, `&quot;text&quot;`, or `&quot;tokens&quot;`).</span>
<span class="sd">        - This design ensures users are conscious of the limitation that assistant token identification requires structured conversation history.</span>

<span class="sd">    .. seealso::</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.RetrieveLogProb`: The base transform for retrieving log-probabilities from a single model.</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.KLComputation`: The transform that computes KL divergence between two log-prob tensors.</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.KLRewardTransform`: A legacy transform for KL reward computation (use `RetrieveKL` instead).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_RayServiceClass</span> <span class="o">=</span> <span class="n">RayRetrieveKL</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">gen_model</span><span class="p">:</span> <span class="n">LLMWrapperBase</span> <span class="o">|</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;from_collector&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;from_collector&quot;</span><span class="p">,</span>
        <span class="n">ref_model</span><span class="p">:</span> <span class="n">LLMWrapperBase</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">gen_model_factory</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">LLMWrapperBase</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">ref_model_factory</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">LLMWrapperBase</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">assistant_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">history_key</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;history&quot;</span><span class="p">,</span>
        <span class="n">tokenizer_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">detach</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoTokenizer</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">padding_side</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;left&quot;</span><span class="p">,</span>
        <span class="n">gen_log_probs_full_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;log_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">),</span>
        <span class="n">ref_log_probs_full_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;ref_log_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">),</span>
        <span class="n">kl_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">=</span> <span class="s2">&quot;kl_penalty&quot;</span><span class="p">,</span>
        <span class="n">add_to_reward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">coeff</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">use_ray_service</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># Handle model factories - create models if factories are provided</span>
        <span class="k">if</span> <span class="n">gen_model_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">gen_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gen_model</span> <span class="o">!=</span> <span class="s2">&quot;from_collector&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot provide both &#39;gen_model&#39; and &#39;gen_model_factory&#39;. Choose one.&quot;</span>
                <span class="p">)</span>
            <span class="n">gen_model</span> <span class="o">=</span> <span class="n">gen_model_factory</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">ref_model_factory</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">ref_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Cannot provide both &#39;ref_model&#39; and &#39;ref_model_factory&#39;. Choose one.&quot;</span>
                <span class="p">)</span>
            <span class="n">ref_model</span> <span class="o">=</span> <span class="n">ref_model_factory</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gen_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">gen_model</span> <span class="o">==</span> <span class="s2">&quot;from_collector&quot;</span><span class="p">:</span>
            <span class="c1"># Lazy init</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;ref_model&quot;</span><span class="p">:</span> <span class="n">ref_model</span><span class="p">,</span>
                <span class="s2">&quot;gen_model_factory&quot;</span><span class="p">:</span> <span class="n">gen_model_factory</span><span class="p">,</span>
                <span class="s2">&quot;ref_model_factory&quot;</span><span class="p">:</span> <span class="n">ref_model_factory</span><span class="p">,</span>
                <span class="s2">&quot;assistant_only&quot;</span><span class="p">:</span> <span class="n">assistant_only</span><span class="p">,</span>
                <span class="s2">&quot;history_key&quot;</span><span class="p">:</span> <span class="n">history_key</span><span class="p">,</span>
                <span class="s2">&quot;tokenizer_kwargs&quot;</span><span class="p">:</span> <span class="n">tokenizer_kwargs</span><span class="p">,</span>
                <span class="s2">&quot;detach&quot;</span><span class="p">:</span> <span class="n">detach</span><span class="p">,</span>
                <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span>
                <span class="s2">&quot;tokenizer&quot;</span><span class="p">:</span> <span class="n">tokenizer</span><span class="p">,</span>
                <span class="s2">&quot;gen_log_probs_full_key&quot;</span><span class="p">:</span> <span class="n">gen_log_probs_full_key</span><span class="p">,</span>
                <span class="s2">&quot;ref_log_probs_full_key&quot;</span><span class="p">:</span> <span class="n">ref_log_probs_full_key</span><span class="p">,</span>
                <span class="s2">&quot;kl_key&quot;</span><span class="p">:</span> <span class="n">kl_key</span><span class="p">,</span>
                <span class="s2">&quot;add_to_reward&quot;</span><span class="p">:</span> <span class="n">add_to_reward</span><span class="p">,</span>
                <span class="s2">&quot;coeff&quot;</span><span class="p">:</span> <span class="n">coeff</span><span class="p">,</span>
                <span class="s2">&quot;padding_side&quot;</span><span class="p">:</span> <span class="n">padding_side</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="k">return</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># Check pad_output consistency if both models are provided</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">gen_model</span><span class="p">,</span> <span class="s2">&quot;pad_output&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ref_model</span><span class="p">,</span> <span class="s2">&quot;pad_output&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">gen_model</span><span class="o">.</span><span class="n">pad_output</span> <span class="o">!=</span> <span class="n">ref_model</span><span class="o">.</span><span class="n">pad_output</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;pad_output mismatch: gen_model.pad_output=</span><span class="si">{</span><span class="n">gen_model</span><span class="o">.</span><span class="n">pad_output</span><span class="si">}</span><span class="s2">, &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;ref_model.pad_output=</span><span class="si">{</span><span class="n">ref_model</span><span class="o">.</span><span class="n">pad_output</span><span class="si">}</span><span class="s2">. &quot;</span>
                    <span class="s2">&quot;Both models must use the same padding strategy for KL computation.&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">gen_model</span><span class="p">,</span> <span class="s2">&quot;return_log_probs&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The generation model must have `return_log_probs=True` to use the `RetrieveKL` transform.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">gen_model</span><span class="p">,</span> <span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The generation model must have `generate=False` to use the `RetrieveKL` transform.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ref_model</span><span class="p">,</span> <span class="s2">&quot;return_log_probs&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The reference model must have `return_log_probs=True` to use the `RetrieveKL` transform.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ref_model</span><span class="p">,</span> <span class="s2">&quot;generate&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The reference model must have `generate=False` to use the `RetrieveKL` transform.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">gen_model</span><span class="p">,</span> <span class="s2">&quot;log_probs_key&quot;</span><span class="p">,</span> <span class="s2">&quot;gen_log_probs&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="nb">getattr</span><span class="p">(</span>
            <span class="n">ref_model</span><span class="p">,</span> <span class="s2">&quot;log_probs_key&quot;</span><span class="p">,</span> <span class="s2">&quot;log_probs&quot;</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The generation and reference models must have different `log_prob_key` values to use the `RetrieveKL` transform.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">gen_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;gen_model cannot be None when not using &#39;from_collector&#39;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ref_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;ref_model cannot be None&quot;</span><span class="p">)</span>

        <span class="n">t1</span> <span class="o">=</span> <span class="n">RetrieveLogProb</span><span class="p">(</span>
            <span class="n">gen_model</span><span class="p">,</span>
            <span class="n">log_probs_full_key</span><span class="o">=</span><span class="n">gen_log_probs_full_key</span><span class="p">,</span>
            <span class="n">assistant_only</span><span class="o">=</span><span class="n">assistant_only</span><span class="p">,</span>
            <span class="n">tokenizer_kwargs</span><span class="o">=</span><span class="n">tokenizer_kwargs</span><span class="p">,</span>
            <span class="n">detach</span><span class="o">=</span><span class="n">detach</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
            <span class="n">padding_side</span><span class="o">=</span><span class="n">padding_side</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">t2</span> <span class="o">=</span> <span class="n">RetrieveLogProb</span><span class="p">(</span>
            <span class="n">ref_model</span><span class="p">,</span>
            <span class="n">log_probs_full_key</span><span class="o">=</span><span class="n">ref_log_probs_full_key</span><span class="p">,</span>
            <span class="n">assistant_only</span><span class="o">=</span><span class="n">assistant_only</span><span class="p">,</span>
            <span class="n">tokenizer_kwargs</span><span class="o">=</span><span class="n">tokenizer_kwargs</span><span class="p">,</span>
            <span class="n">detach</span><span class="o">=</span><span class="n">detach</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
            <span class="n">padding_side</span><span class="o">=</span><span class="n">padding_side</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">t3</span> <span class="o">=</span> <span class="n">KLComputation</span><span class="p">(</span>
            <span class="n">gen_log_probs_full_key</span><span class="o">=</span><span class="n">gen_log_probs_full_key</span><span class="p">,</span>
            <span class="n">ref_log_probs_full_key</span><span class="o">=</span><span class="n">ref_log_probs_full_key</span><span class="p">,</span>
            <span class="n">kl_key</span><span class="o">=</span><span class="n">kl_key</span><span class="p">,</span>
            <span class="n">add_to_reward</span><span class="o">=</span><span class="n">add_to_reward</span><span class="p">,</span>
            <span class="n">coeff</span><span class="o">=</span><span class="n">coeff</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">,</span> <span class="n">t3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_deferred</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Initializing RetrieveKL transform&quot;</span><span class="p">)</span>
        <span class="n">container</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">container</span>
        <span class="k">if</span> <span class="n">container</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># also logging, since this will be sometimes hidden within the AttributeError</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The container is not set. Please set the container before calling this method.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The container is not set. Please set the container before calling this method.&quot;</span>
            <span class="p">)</span>
        <span class="n">container</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="n">collector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collector</span>
        <span class="k">if</span> <span class="n">collector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># also logging, since this will be sometimes hidden within the AttributeError</span>
            <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The collector is not set. Please set the collector before calling this method.&quot;</span>
            <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The collector is not set. Please set the collector before calling this method.&quot;</span>
            <span class="p">)</span>
        <span class="n">ref_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;ref_model&quot;</span><span class="p">]</span>
        <span class="n">pad_output</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ref_model</span><span class="p">,</span> <span class="s2">&quot;pad_output&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">gen_log_probs_full_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;gen_log_probs_full_key&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">gen_log_probs_full_key</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">gen_log_probs_full_key</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;full&quot;</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The gen_log_probs_full_key </span><span class="si">{</span><span class="n">gen_log_probs_full_key</span><span class="si">}</span><span class="s2"> is not a tuple or does not end with &#39;full&#39;. &quot;</span>
                <span class="s2">&quot;This may cause issues with the KL computation. &quot;</span>
                <span class="s2">&quot;Please use a tuple with the log_probs_key and &#39;full&#39; as the last element.&quot;</span>
            <span class="p">)</span>
        <span class="n">log_probs_key</span> <span class="o">=</span> <span class="n">gen_log_probs_full_key</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">gen_model</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">policy</span><span class="o">.</span><span class="n">get_new_version</span><span class="p">(</span>
            <span class="n">generate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">return_log_probs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">log_probs_key</span><span class="o">=</span><span class="n">log_probs_key</span><span class="p">,</span>
            <span class="n">input_mode</span><span class="o">=</span><span class="n">ref_model</span><span class="o">.</span><span class="n">input_mode</span><span class="p">,</span>
            <span class="n">input_key</span><span class="o">=</span><span class="p">(</span><span class="n">ref_model</span><span class="o">.</span><span class="n">input_mode</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">),</span>
            <span class="n">pad_output</span><span class="o">=</span><span class="n">pad_output</span><span class="p">,</span>  <span class="c1"># Pass pad_output from ref_model</span>
        <span class="p">)</span>
        <span class="c1"># Create the transforms manually instead of calling __init__</span>
        <span class="n">t1</span> <span class="o">=</span> <span class="n">RetrieveLogProb</span><span class="p">(</span>
            <span class="n">gen_model</span><span class="p">,</span>
            <span class="n">log_probs_full_key</span><span class="o">=</span><span class="n">gen_log_probs_full_key</span><span class="p">,</span>
            <span class="n">assistant_only</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;assistant_only&quot;</span><span class="p">],</span>
            <span class="n">tokenizer_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;tokenizer_kwargs&quot;</span><span class="p">],</span>
            <span class="n">detach</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;detach&quot;</span><span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">],</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;tokenizer&quot;</span><span class="p">],</span>
            <span class="n">padding_side</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;padding_side&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">ref_log_probs_full_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;ref_log_probs_full_key&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ref_log_probs_full_key</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span>
            <span class="ow">or</span> <span class="n">ref_log_probs_full_key</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;full&quot;</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The ref_log_probs_full_key </span><span class="si">{</span><span class="n">ref_log_probs_full_key</span><span class="si">}</span><span class="s2"> is not a tuple or does not end with &#39;full&#39;. &quot;</span>
                <span class="s2">&quot;This may cause issues with the KL computation. &quot;</span>
                <span class="s2">&quot;Please use a tuple with the log_probs_key and &#39;full&#39; as the last element.&quot;</span>
            <span class="p">)</span>
        <span class="n">t2</span> <span class="o">=</span> <span class="n">RetrieveLogProb</span><span class="p">(</span>
            <span class="n">ref_model</span><span class="p">,</span>
            <span class="n">log_probs_full_key</span><span class="o">=</span><span class="n">ref_log_probs_full_key</span><span class="p">,</span>
            <span class="n">assistant_only</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;assistant_only&quot;</span><span class="p">],</span>
            <span class="n">tokenizer_kwargs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;tokenizer_kwargs&quot;</span><span class="p">],</span>
            <span class="n">detach</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;detach&quot;</span><span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;device&quot;</span><span class="p">],</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;tokenizer&quot;</span><span class="p">],</span>
            <span class="n">padding_side</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;padding_side&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">t3</span> <span class="o">=</span> <span class="n">KLComputation</span><span class="p">(</span>
            <span class="n">gen_log_probs_full_key</span><span class="o">=</span><span class="n">gen_log_probs_full_key</span><span class="p">,</span>
            <span class="n">ref_log_probs_full_key</span><span class="o">=</span><span class="n">ref_log_probs_full_key</span><span class="p">,</span>
            <span class="n">kl_key</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;kl_key&quot;</span><span class="p">],</span>
            <span class="n">add_to_reward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;add_to_reward&quot;</span><span class="p">],</span>
            <span class="n">coeff</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span><span class="p">[</span><span class="s2">&quot;coeff&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="c1"># Replace the transforms in the Compose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">,</span> <span class="n">t3</span><span class="p">])</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">torchrl_logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Successfully initialized&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">next_tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_deferred</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_step</span><span class="p">(</span><span class="n">tensordict</span><span class="p">,</span> <span class="n">next_tensordict</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_reset</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">tensordict_reset</span><span class="p">:</span> <span class="n">TensorDictBase</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_deferred</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_reset</span><span class="p">(</span><span class="n">tensordict</span><span class="p">,</span> <span class="n">tensordict_reset</span><span class="p">)</span>

<div class="viewcode-block" id="RetrieveKL.forward"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveKL.html#torchrl.envs.llm.transforms.RetrieveKL.forward">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_deferred</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tensordict</span><span class="p">)</span></div>

<div class="viewcode-block" id="RetrieveKL.transform_observation_spec"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveKL.html#torchrl.envs.llm.transforms.RetrieveKL.transform_observation_spec">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">transform_observation_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_deferred</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transform_observation_spec</span><span class="p">(</span><span class="n">observation_spec</span><span class="p">)</span></div>

<div class="viewcode-block" id="RetrieveKL.transform_reward_spec"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveKL.html#torchrl.envs.llm.transforms.RetrieveKL.transform_reward_spec">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">transform_reward_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_deferred</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transform_reward_spec</span><span class="p">(</span><span class="n">reward_spec</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_inv_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_deferred</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_inv_call</span><span class="p">(</span><span class="n">tensordict</span><span class="p">)</span>

<div class="viewcode-block" id="RetrieveKL.transform_action_spec"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveKL.html#torchrl.envs.llm.transforms.RetrieveKL.transform_action_spec">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">transform_action_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_deferred</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transform_action_spec</span><span class="p">(</span><span class="n">action_spec</span><span class="p">)</span></div>

<div class="viewcode-block" id="RetrieveKL.transform_input_spec"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveKL.html#torchrl.envs.llm.transforms.RetrieveKL.transform_input_spec">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">transform_input_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_deferred</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transform_input_spec</span><span class="p">(</span><span class="n">input_spec</span><span class="p">)</span></div>

<div class="viewcode-block" id="RetrieveKL.transform_output_spec"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveKL.html#torchrl.envs.llm.transforms.RetrieveKL.transform_output_spec">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">transform_output_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_deferred</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transform_output_spec</span><span class="p">(</span><span class="n">output_spec</span><span class="p">)</span></div>

<div class="viewcode-block" id="RetrieveKL.transform_state_spec"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.RetrieveKL.html#torchrl.envs.llm.transforms.RetrieveKL.transform_state_spec">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">transform_state_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initialized</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_deferred</span><span class="p">()</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transform_state_spec</span><span class="p">(</span><span class="n">state_spec</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="KLComputation"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.KLComputation.html#torchrl.envs.llm.transforms.KLComputation">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">KLComputation</span><span class="p">(</span><span class="n">Transform</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A transform to compute KL divergence between two log-prob tensors and optionally add it to the reward.</span>

<span class="sd">    This transform computes KL divergence between generation and reference log-probabilities</span>
<span class="sd">    and can optionally subtract it from the reward (for KL penalty). It&#39;s designed to work</span>
<span class="sd">    with the :class:`~torchrl.envs.llm.transforms.kl.RetrieveLogProb` and :class:`~torchrl.envs.llm.transforms.kl.RetrieveKL` transforms.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Both input log-prob tensors must use the same padding strategy (pad_output) for correct KL computation.</span>

<span class="sd">    Args:</span>
<span class="sd">        gen_log_probs_full_key (NestedKey): the key where the generation model log-probs are stored.</span>
<span class="sd">            Defaults to `(&quot;gen_log_probs&quot;, &quot;full&quot;)`.</span>
<span class="sd">        ref_log_probs_full_key (NestedKey): the key where the reference model log-probs are stored.</span>
<span class="sd">            Defaults to `(&quot;ref_log_probs&quot;, &quot;full&quot;)`.</span>
<span class="sd">        kl_key (NestedKey): the key where the KL divergence is stored. Defaults to `&quot;kl_penalty&quot;`.</span>
<span class="sd">        add_to_reward (bool): whether to add the KL divergence to the reward. Defaults to `True`.</span>
<span class="sd">        coeff (float): the coefficient for the KL term when adding to reward. Defaults to `1.0`.</span>
<span class="sd">        padding_side (str): the side of the padding when using pad_sequence. Defaults to `&quot;left&quot;`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from tensordict import TensorDict</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create sample log-probs</span>
<span class="sd">        &gt;&gt;&gt; gen_log_probs = torch.randn(2, 10)  # 2 samples, 10 tokens each</span>
<span class="sd">        &gt;&gt;&gt; ref_log_probs = torch.randn(2, 10)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create data with next tensordict</span>
<span class="sd">        &gt;&gt;&gt; next_td = TensorDict(</span>
<span class="sd">        ...     {</span>
<span class="sd">        ...         (&quot;gen_log_probs&quot;, &quot;full&quot;): gen_log_probs,</span>
<span class="sd">        ...         (&quot;ref_log_probs&quot;, &quot;full&quot;): ref_log_probs,</span>
<span class="sd">        ...         &quot;reward&quot;: torch.randn(2, 10, 1),</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...     batch_size=(2,)</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; data = TensorDict(next=next_td, batch_size=(2,))</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Create KLComputation transform</span>
<span class="sd">        &gt;&gt;&gt; kl_transform = KLComputation(</span>
<span class="sd">        ...     gen_log_probs_key=(&quot;gen_log_probs&quot;, &quot;full&quot;),</span>
<span class="sd">        ...     ref_log_probs_key=(&quot;ref_log_probs&quot;, &quot;full&quot;),</span>
<span class="sd">        ...     kl_key=&quot;kl_penalty&quot;,</span>
<span class="sd">        ...     add_to_reward=True,</span>
<span class="sd">        ...     coef=1.0,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Apply transform</span>
<span class="sd">        &gt;&gt;&gt; result = kl_transform(data)</span>
<span class="sd">        &gt;&gt;&gt; kl = result[&quot;next&quot;].get(&quot;kl_penalty&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(f&quot;KL shape: {kl.shape}&quot;)</span>
<span class="sd">        KL shape: torch.Size([2, 10])</span>

<span class="sd">    .. seealso::</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.RetrieveLogProb`: The base transform for retrieving log-probabilities from a single model.</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.RetrieveKL`: A higher-level transform that combines two `RetrieveLogProb` instances with `KLComputation`.</span>
<span class="sd">        :class:`~torchrl.envs.llm.transforms.kl.KLRewardTransform`: A legacy transform for KL reward computation (use `RetrieveKL` instead).</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">gen_log_probs_full_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;log_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">),</span>
        <span class="n">ref_log_probs_full_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;ref_log_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">),</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">kl_key</span><span class="p">:</span> <span class="n">NestedKey</span> <span class="o">=</span> <span class="s2">&quot;kl_penalty&quot;</span><span class="p">,</span>
        <span class="n">add_to_reward</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">coeff</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">padding_side</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;left&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">in_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">gen_log_probs_full_key</span><span class="p">,</span> <span class="n">ref_log_probs_full_key</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">add_to_reward</span><span class="p">:</span>
            <span class="n">in_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>
        <span class="n">out_keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">kl_key</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">add_to_reward</span><span class="p">:</span>
            <span class="n">out_keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">in_keys</span><span class="o">=</span><span class="n">in_keys</span><span class="p">,</span> <span class="n">out_keys</span><span class="o">=</span><span class="n">out_keys</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gen_log_probs_full_key</span> <span class="o">=</span> <span class="n">gen_log_probs_full_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ref_log_probs_full_key</span> <span class="o">=</span> <span class="n">ref_log_probs_full_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kl_key</span> <span class="o">=</span> <span class="n">kl_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_to_reward</span> <span class="o">=</span> <span class="n">add_to_reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span> <span class="o">=</span> <span class="n">coeff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="n">padding_side</span>

<div class="viewcode-block" id="KLComputation.forward"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.KLComputation.html#torchrl.envs.llm.transforms.KLComputation.forward">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="n">next_td</span> <span class="o">=</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">)</span>
        <span class="n">has_next_td</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">next_td</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">next_td</span> <span class="o">=</span> <span class="n">tensordict</span>
            <span class="n">has_next_td</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">next_td</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_step</span><span class="p">(</span><span class="n">tensordict</span><span class="p">,</span> <span class="n">next_td</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">has_next_td</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;next&quot;</span><span class="p">,</span> <span class="n">next_td</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">next_td</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">_step</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span><span class="p">,</span> <span class="n">next_tensordict</span><span class="p">:</span> <span class="n">TensorDictBase</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDictBase</span><span class="p">:</span>
        <span class="c1"># Get log-probs</span>
        <span class="n">gen_log_probs</span> <span class="o">=</span> <span class="n">next_tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gen_log_probs_full_key</span><span class="p">,</span> <span class="n">as_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
        <span class="n">ref_log_probs</span> <span class="o">=</span> <span class="n">next_tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ref_log_probs_full_key</span><span class="p">,</span> <span class="n">as_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>

        <span class="k">if</span> <span class="n">gen_log_probs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">ref_log_probs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Log-probs not found. Expected keys: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">gen_log_probs_key</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">ref_log_probs_key</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Debug: Check lengths and shapes</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gen_log_probs</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ref_log_probs</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Batch size mismatch: gen_log_probs has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">gen_log_probs</span><span class="p">)</span><span class="si">}</span><span class="s2"> samples, ref_log_probs has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">ref_log_probs</span><span class="p">)</span><span class="si">}</span><span class="s2"> samples&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Check individual sequence lengths</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">gen_lp</span><span class="p">,</span> <span class="n">ref_lp</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">_zip_strict</span><span class="p">(</span><span class="n">gen_log_probs</span><span class="p">,</span> <span class="n">ref_log_probs</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">gen_lp</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">ref_lp</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Sample </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> has different shapes: gen_log_probs[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">].shape=</span><span class="si">{</span><span class="n">gen_lp</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, ref_log_probs[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">].shape=</span><span class="si">{</span><span class="n">ref_lp</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Compute KL divergence: KL(p||q) = E_p[log p - log q]</span>
        <span class="c1"># Here gen_log_probs = log p, ref_log_probs = log q</span>
        <span class="n">kl</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">gen_lp</span> <span class="o">-</span> <span class="n">ref_lp</span>
            <span class="k">for</span> <span class="n">gen_lp</span><span class="p">,</span> <span class="n">ref_lp</span> <span class="ow">in</span> <span class="n">_zip_strict</span><span class="p">(</span><span class="n">gen_log_probs</span><span class="p">,</span> <span class="n">ref_log_probs</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="n">kl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">as_nested_tensor</span><span class="p">(</span><span class="n">kl</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">)</span>

        <span class="n">next_tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kl_key</span><span class="p">,</span> <span class="n">kl</span><span class="p">)</span>

        <span class="c1"># Add to reward if requested</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_to_reward</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="n">next_tensordict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">as_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
            <span class="k">if</span> <span class="n">reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">kl</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;The rewards have shape </span><span class="si">{</span><span class="n">reward</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> but the kl has shape </span><span class="si">{</span><span class="n">kl</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;The rewards should have one more dimension than the KL.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">reward</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">r</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span> <span class="o">*</span> <span class="n">k</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">r</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">_zip_strict</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">kl</span><span class="p">)</span>
                    <span class="p">]</span>
                    <span class="n">next_tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
                        <span class="s2">&quot;reward&quot;</span><span class="p">,</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">nested</span><span class="o">.</span><span class="n">as_nested_tensor</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">strided</span><span class="p">),</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">reward</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">kl</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;The rewards have shape </span><span class="si">{</span><span class="n">reward</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> but the kl has shape </span><span class="si">{</span><span class="n">kl</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">. &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;The rewards should have one more dimension than the KL.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeff</span> <span class="o">*</span> <span class="n">kl</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">next_tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">next_tensordict</span>

<div class="viewcode-block" id="KLComputation.transform_observation_spec"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.KLComputation.html#torchrl.envs.llm.transforms.KLComputation.transform_observation_spec">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">transform_observation_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observation_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
        <span class="c1"># Add kl to observation spec</span>
        <span class="n">observation_spec</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">kl_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Unbounded</span><span class="p">(</span>
            <span class="n">device</span><span class="o">=</span><span class="n">observation_spec</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="n">observation_spec</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">observation_spec</span></div>

<div class="viewcode-block" id="KLComputation.transform_reward_spec"><a class="viewcode-back" href="../../../../../reference/generated/torchrl.envs.llm.transforms.KLComputation.html#torchrl.envs.llm.transforms.KLComputation.transform_reward_spec">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">transform_reward_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
        <span class="c1"># Optionally adjust reward spec if KL is added to reward</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_to_reward</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">reward_spec</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="c1"># For LLMs, the shape of the reward is (batch, -1, 1)</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">((</span><span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">reward_spec</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward_spec</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">reward_spec</span><span class="p">[</span><span class="s2">&quot;reward&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>
        <span class="k">return</span> <span class="n">reward_spec</span></div></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
         <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
         <script src="../../../../../_static/jquery.js"></script>
         <script src="../../../../../_static/underscore.js"></script>
         <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../../../../_static/doctools.js"></script>
         <script src="../../../../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>