


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>AsyncVLLM &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/pytorch.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-design.min.css" type="text/css" />
  <link rel="stylesheet" href="../../https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/features">Features</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   
  <div>

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../../versions.html"><span style="font-size:110%">main (0.10.0) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/export.html">Exporting TorchRL modules</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">

      <section data-toggle="wy-nav-shift" class="pytorch-content-wrap">
        <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
          <div class="pytorch-breadcrumbs-wrapper">
            















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>AsyncVLLM</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../_sources/reference/generated/torchrl.modules.llm.AsyncVLLM.rst.txt" rel="nofollow"><img src="../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
          </div>

          <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
            Shortcuts
          </div>
        </div>

        <div class="pytorch-content-left">
    
    
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" class="pytorch-article">
              
  <section id="asyncvllm">
<h1>AsyncVLLM<a class="headerlink" href="#asyncvllm" title="Link to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">torchrl.modules.llm.</span></span><span class="sig-name descname"><span class="pre">AsyncVLLM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">engine_args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">AsyncEngineArgs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_replicas</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_prefix_caching</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM" title="Link to this definition">¶</a></dt>
<dd><p>A service that manages multiple async vLLM engine actors for distributed inference.</p>
<p>This is the main entry point for async vLLM inference in TorchRL. It manages multiple
vLLM engine replicas running as Ray actors, providing load balancing, weight updates,
and a unified interface for text generation.</p>
<p>The service automatically handles Ray actor lifecycle management, GPU allocation through
placement groups, and provides both synchronous and asynchronous generation interfaces
that are compatible with the standard vLLM API.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>engine_args</strong> (<em>AsyncEngineArgs</em>) – Configuration for the vLLM engines.</p></li>
<li><p><strong>num_replicas</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of engine replicas to create. Defaults to 1.</p></li>
<li><p><strong>actor_class</strong> (<em>optional</em>) – Custom Ray actor class. Defaults to the internal actor implementation.</p></li>
<li><p><strong>enable_prefix_caching</strong> (<em>bool</em><em>, </em><em>optional</em>) – <p>Whether to enable prefix caching. Defaults to False.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>enable_prefix_caching is set to False by default, which is recommended if prompt log probs are needed.
Set it to True if prompt log probs are not needed.
See <a class="reference external" href="https://github.com/vllm-project/vllm/issues/8268">this issue</a> for more details.</p>
</div>
</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">torchrl.modules.llm</span> <span class="kn">import</span> <span class="n">AsyncVLLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">SamplingParams</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Simple usage - single GPU, single replica</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">service</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen2.5-3B&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Advanced usage - multi-GPU tensor parallel with multiple replicas</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">service</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;Qwen/Qwen2.5-7B&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Use 2 GPUs for tensor parallelism</span>
<span class="gp">... </span>    <span class="n">num_replicas</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># Create 2 replicas for higher throughput</span>
<span class="gp">... </span>    <span class="n">max_model_len</span><span class="o">=</span><span class="mi">4096</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Generate text</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">service</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Hello, world!&quot;</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Alternative: using AsyncEngineArgs directly for advanced configuration</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">AsyncEngineArgs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">engine_args</span> <span class="o">=</span> <span class="n">AsyncEngineArgs</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2.5-3B&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="mi">2</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">service</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">engine_args</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Architecture and Design</strong></p>
<p>The AsyncVLLM service implements a distributed inference architecture with the following key components:</p>
<ol class="arabic simple">
<li><p><strong>Ray Actor Management</strong>: Each replica runs as a separate Ray actor with dedicated GPU resources.
The service creates a placement group to ensure optimal GPU allocation and co-location of
tensor-parallel workers on the same node when possible.</p></li>
<li><p><strong>Load Balancing</strong>: Generation requests are distributed across replicas using random selection
by default, or can target specific replicas using the <cite>actor_index</cite> parameter.</p></li>
<li><p><strong>Weight Synchronization</strong>: The service supports weight updates across all replicas through
NCCL communication groups, enabling integration with distributed training workflows.</p></li>
<li><p><strong>Resource Management</strong>: Automatic GPU allocation and cleanup through Ray placement groups,
with proper shutdown procedures to prevent resource leaks.</p></li>
<li><p><strong>API Compatibility</strong>: Provides the same interface as vLLM’s synchronous <cite>LLM.generate()</cite>
method, making it a drop-in replacement for async workloads.</p></li>
</ol>
<p><strong>Ray Integration</strong></p>
<p>The service leverages Ray’s actor model for distributed execution. Each replica is an independent
Ray actor that can be scheduled on different nodes. The service handles actor lifecycle,
monitors readiness, and provides centralized access to all replicas.</p>
<p><strong>Performance Considerations</strong></p>
<ul class="simple">
<li><p>Prefix caching is enabled by default for better performance with repeated prompts</p></li>
<li><p>Tensor parallelism is supported for large models that don’t fit on single GPUs</p></li>
<li><p>Multiple replicas allow concurrent processing of different requests</p></li>
<li><p>Native vLLM batching is used within each replica for optimal throughput</p></li>
</ul>
<p><strong>Error Handling</strong></p>
<p>The service includes timeout support, graceful shutdown procedures, and best-effort
request cleanup on failures. Ray’s fault tolerance mechanisms provide additional
resilience for long-running inference workloads.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.collective_rpc">
<span class="sig-name descname"><span class="pre">collective_rpc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.collective_rpc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.collective_rpc" title="Link to this definition">¶</a></dt>
<dd><p>Forward an RPC to all actors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>method</strong> (<em>str</em>) – Method name to call.</p></li>
<li><p><strong>timeout</strong> (<em>float</em><em> | </em><em>None</em>) – Timeout for the RPC call.</p></li>
<li><p><strong>args</strong> (<em>tuple</em>) – Arguments to pass to the method.</p></li>
<li><p><strong>kwargs</strong> (<em>dict</em><em> | </em><em>None</em>) – Keyword arguments to pass to the method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Ray futures for all RPC calls.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[Any]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.create_load_balancer">
<span class="sig-name descname"><span class="pre">create_load_balancer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'requests'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'kv-cache'</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'prefix-aware'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'requests'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'kv-cache'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'round-robin'</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">LoadBalancer</span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.create_load_balancer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.create_load_balancer" title="Link to this definition">¶</a></dt>
<dd><p>Create a load balancer for this AsyncVLLM service.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>strategy</strong> – Load balancing strategy or sequence of strategies in fallback order.
Default: [“prefix-aware”, “requests”] - tries cache-aware routing first,
then load balancing. Single strategies: “requests”, “kv-cache”
Strategy sequences: [“prefix-aware”, “requests”, “round-robin”]</p></li>
<li><p><strong>**kwargs</strong> – Additional arguments passed to LoadBalancer constructor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Configured load balancer instance. This is stored in the AsyncVLLM instance.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>LoadBalancer</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">service</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen2.5-3B&quot;</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use smart defaults (prefix-aware -&gt; requests)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span> <span class="o">=</span> <span class="n">service</span><span class="o">.</span><span class="n">create_load_balancer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selected_actor_index</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">select_actor</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Simple single strategy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span> <span class="o">=</span> <span class="n">service</span><span class="o">.</span><span class="n">create_load_balancer</span><span class="p">(</span><span class="s2">&quot;requests&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selected_actor_index</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">select_actor</span><span class="p">()</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Custom strategy hierarchy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lb</span> <span class="o">=</span> <span class="n">service</span><span class="o">.</span><span class="n">create_load_balancer</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s2">&quot;prefix-aware&quot;</span><span class="p">,</span> <span class="s2">&quot;kv-cache&quot;</span><span class="p">,</span> <span class="s2">&quot;round-robin&quot;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">prefix_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">overload_threshold</span><span class="o">=</span><span class="mf">2.0</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selected_actor_index</span> <span class="o">=</span> <span class="n">lb</span><span class="o">.</span><span class="n">select_actor</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_devices</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_replicas</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_fp32_output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM" title="torchrl.modules.llm.backends.vllm.vllm_async.AsyncVLLM"><span class="pre">AsyncVLLM</span></a></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.from_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.from_pretrained" title="Link to this definition">¶</a></dt>
<dd><p>Create an AsyncVLLM instance from a pretrained model.</p>
<p>This is a convenience method that combines model loading and service launching
in a single call, similar to how other ML libraries work.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_name</strong> (<em>str</em>) – The model name to pass to vLLM.</p></li>
<li><p><strong>num_devices</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of devices to use, per replica.</p></li>
<li><p><strong>num_replicas</strong> (<em>int</em>) – Number of engine replicas to create.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to enable verbose logging with throughput statistics. Defaults to True.</p></li>
<li><p><strong>compile</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to enable model compilation for better performance. Defaults to True.</p></li>
<li><p><strong>enable_fp32_output</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to enable FP32 output for the final layer. Defaults to False.</p></li>
<li><p><strong>**kwargs</strong> – Additional arguments passed to AsyncEngineArgs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The launched async vLLM service.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM" title="torchrl.modules.llm.AsyncVLLM">AsyncVLLM</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Simple usage with defaults</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">service</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen2.5-3B&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Multi-GPU tensor parallel with multiple replicas</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">service</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;Qwen/Qwen2.5-7B&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">num_replicas</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_model_len</span><span class="o">=</span><span class="mi">4096</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Generate text</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">vllm</span> <span class="kn">import</span> <span class="n">SamplingParams</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">service</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Hello, world!&quot;</span><span class="p">,</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Enable FP32 output for better numerical stability</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">service</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;Qwen/Qwen2.5-3B&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">enable_fp32_output</span><span class="o">=</span><span class="kc">True</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sampling_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SamplingParams</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_token_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_tqdm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lora_request</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prompt_adapter_request</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">guided_options_request</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout_seconds</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">actor_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RequestOutput</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">RequestOutput</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.generate" title="Link to this definition">¶</a></dt>
<dd><p>Generate text using one of the actors with vLLM.LLM.generate interface.</p>
<p>This method provides the same interface as vLLM.LLM.generate for seamless
compatibility between sync and async engines. It can be used to generate text
within multiple threads / actors. If <cite>actor_index</cite> is not provided, the load balancer
will be used to select the actor.</p>
<p><cite>generate</cite> is a blocking method, so it will wait for the generation to complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prompts</strong> (<em>String</em><em>, </em><em>TokensPrompt</em><em>, or </em><em>list</em><em> of </em><em>these</em>) – Input prompts for generation.</p></li>
<li><p><strong>sampling_params</strong> (<em>SamplingParams</em>) – SamplingParams object for controlling generation behavior.</p></li>
<li><p><strong>prompt_token_ids</strong> (<em>list</em><em>[</em><em>int</em><em>] </em><em>| </em><em>list</em><em>[</em><em>list</em><em>[</em><em>int</em><em>]</em><em>]</em>) – Alternative to prompts - token IDs for generation.</p></li>
<li><p><strong>use_tqdm</strong> (<em>bool</em>) – Whether to show progress bar (not used in async engine).</p></li>
<li><p><strong>lora_request</strong> (<em>Any</em>) – LoRA request for adapter-based generation.</p></li>
<li><p><strong>prompt_adapter_request</strong> (<em>Any</em>) – Prompt adapter request.</p></li>
<li><p><strong>guided_options_request</strong> (<em>Any</em>) – Guided decoding options.</p></li>
<li><p><strong>timeout_seconds</strong> (<em>float</em><em> | </em><em>None</em>) – Timeout for generation in seconds.</p></li>
<li><p><strong>actor_index</strong> (<em>int</em><em> | </em><em>None</em>) – Specific actor to use (random if None).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Generated outputs from vLLM.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>RequestOutput | list[RequestOutput]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.get_cache_usage">
<span class="sig-name descname"><span class="pre">get_cache_usage</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.get_cache_usage"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.get_cache_usage" title="Link to this definition">¶</a></dt>
<dd><p>Get the KV cache usage for one or all actors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>actor_index</strong> (<em>int</em><em> | </em><em>None</em>) – Index of specific actor, or None for all actors.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Cache usage fraction for the specified actor,</dt><dd><p>or list of usage fractions for all actors if actor_index is None.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float | list[float]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.get_master_address">
<span class="sig-name descname"><span class="pre">get_master_address</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.get_master_address"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.get_master_address" title="Link to this definition">¶</a></dt>
<dd><p>Get the master address for weight synchronization.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.get_master_port">
<span class="sig-name descname"><span class="pre">get_master_port</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.get_master_port"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.get_master_port" title="Link to this definition">¶</a></dt>
<dd><p>Get the master port for weight synchronization.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.get_model_metadata">
<span class="sig-name descname"><span class="pre">get_model_metadata</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.9)"><span class="pre">dtype</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/size.html#torch.Size" title="(in PyTorch v2.9)"><span class="pre">Size</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.get_model_metadata"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.get_model_metadata" title="Link to this definition">¶</a></dt>
<dd><p>Get model parameter metadata.</p>
<p>Note: This requires the model to be loaded. For now, we return an empty dict
and expect the metadata to be provided externally during weight updates.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.get_num_unfinished_requests">
<span class="sig-name descname"><span class="pre">get_num_unfinished_requests</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">actor_index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.get_num_unfinished_requests"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.get_num_unfinished_requests" title="Link to this definition">¶</a></dt>
<dd><p>Get the number of unfinished requests for one or all actors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>actor_index</strong> (<em>int</em><em> | </em><em>None</em>) – Index of specific actor, or None for all actors.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>Number of unfinished requests for the specified actor,</dt><dd><p>or list of counts for all actors if actor_index is None.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int | list[int]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.get_random_actor_index">
<span class="sig-name descname"><span class="pre">get_random_actor_index</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.get_random_actor_index"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.get_random_actor_index" title="Link to this definition">¶</a></dt>
<dd><p>Get a random actor index.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.get_tp_size">
<span class="sig-name descname"><span class="pre">get_tp_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">int</span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.get_tp_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.get_tp_size" title="Link to this definition">¶</a></dt>
<dd><p>Get the tensor parallel size.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.init_weight_update_group">
<span class="sig-name descname"><span class="pre">init_weight_update_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">master_address</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">master_port</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.init_weight_update_group"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.init_weight_update_group" title="Link to this definition">¶</a></dt>
<dd><p>Forward the request to init NCCL weight update group to all actors.</p>
<p>This method initializes the weight update group for all vLLM workers.
The external trainer should be rank 0, and vLLM workers will be ranks 1+.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>master_address</strong> – Master address for NCCL communication.</p></li>
<li><p><strong>master_port</strong> – Master port for NCCL communication.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>List of Ray futures for the initialization calls.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The caller must wait on the returned futures (ray.get(refs)) to ensure
all workers have completed initialization before sending weights.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.launch">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">launch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">engine_args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">AsyncEngineArgs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_replicas</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM" title="torchrl.modules.llm.AsyncVLLM"><span class="pre">AsyncVLLM</span></a></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.launch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.launch" title="Link to this definition">¶</a></dt>
<dd><p>Launch a new AsyncVLLMEngineService.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>engine_args</strong> (<em>AsyncEngineArgs</em>) – Arguments for creating the AsyncLLMEngine instances.</p></li>
<li><p><strong>num_replicas</strong> (<em>int</em>) – Number of actor replicas to create.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The launched service.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>AsyncVLLMEngineService</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.shutdown">
<span class="sig-name descname"><span class="pre">shutdown</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.shutdown"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.shutdown" title="Link to this definition">¶</a></dt>
<dd><p>Shutdown all actors and clean up resources.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="torchrl.modules.llm.AsyncVLLM.update_weights">
<span class="sig-name descname"><span class="pre">update_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterator</span><span class="p"><span class="pre">[</span></span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../_modules/torchrl/modules/llm/backends/vllm/vllm_async.html#AsyncVLLM.update_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.modules.llm.AsyncVLLM.update_weights" title="Link to this definition">¶</a></dt>
<dd><p>Update model weights across all replicas using NCCL broadcast.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>weights</strong> – Iterator yielding (parameter_name, tensor) tuples</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


             </article>
             
            </div>
            <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">AsyncVLLM</a><ul>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM</span></code></a><ul>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.collective_rpc"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.collective_rpc()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.create_load_balancer"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.create_load_balancer()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.from_pretrained"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.from_pretrained()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.generate"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.generate()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.get_cache_usage"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.get_cache_usage()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.get_master_address"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.get_master_address()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.get_master_port"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.get_master_port()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.get_model_metadata"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.get_model_metadata()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.get_num_unfinished_requests"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.get_num_unfinished_requests()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.get_random_actor_index"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.get_random_actor_index()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.get_tp_size"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.get_tp_size()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.init_weight_update_group"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.init_weight_update_group()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.launch"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.launch()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.shutdown"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.shutdown()</span></code></a></li>
<li><a class="reference internal" href="#torchrl.modules.llm.AsyncVLLM.update_weights"><code class="docutils literal notranslate"><span class="pre">AsyncVLLM.update_weights()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>
  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'main',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../_static/design-tabs.js"></script>

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://shiftlab.github.io/pytorch/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://shiftlab.github.io/pytorch/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/">PyTorch</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/features">Features</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/blog/">Blog</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    mobileMenu.bind();
    mobileTOC.bind();
    pytorchAnchors.bind();

    $(window).on("load", function() {
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
    })

    // Add class to links that have code blocks, since we cannot create links in code blocks
    $("article.pytorch-article a span.pre").each(function(e) {
      $(this).closest("a").addClass("has-code");
    });
  </script>
</body>
</html>