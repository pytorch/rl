


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Things to consider when debugging RL &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/pytorch.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sphinx-design.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Working with gym" href="GYM.html" />
    <link rel="prev" title="Knowledge Base" href="../../knowledge_base.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/features">Features</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   
  <div>

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../../../versions.html"><span style="font-size:110%">main (0.10.0) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/export.html">Exporting TorchRL modules</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../index.html">API Reference</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">

      <section data-toggle="wy-nav-shift" class="pytorch-content-wrap">
        <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
          <div class="pytorch-breadcrumbs-wrapper">
            















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../knowledge_base.html">Knowledge Base</a> &gt;</li>
        
      <li>Things to consider when debugging RL</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../../_sources/reference/generated/knowledge_base/DEBUGGING_RL.rst.txt" rel="nofollow"><img src="../../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
          </div>

          <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
            Shortcuts
          </div>
        </div>

        <div class="pytorch-content-left">
    
    
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" class="pytorch-article">
              
  <section id="things-to-consider-when-debugging-rl">
<h1>Things to consider when debugging RL<a class="headerlink" href="#things-to-consider-when-debugging-rl" title="Link to this heading">¶</a></h1>
<section id="general">
<h2>General<a class="headerlink" href="#general" title="Link to this heading">¶</a></h2>
<section id="have-you-validated-your-algorithm-implementation-on-a-few-small-toy-problems-with-known-optimal-returns-e-g-gridworlds-mountaincar">
<h3>Have you validated your algorithm implementation on a few small, toy problems with known optimal returns e.g. gridworlds, mountaincar?<a class="headerlink" href="#have-you-validated-your-algorithm-implementation-on-a-few-small-toy-problems-with-known-optimal-returns-e-g-gridworlds-mountaincar" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: This will reveal any extreme bugs in your implementation.</p></li>
</ul>
</section>
<section id="have-you-visualized-your-agents">
<h3>Have you visualized your agents?<a class="headerlink" href="#have-you-visualized-your-agents" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: This will reveal things the learning curves won’t tell you (i.e., bug or exploit in a video game).</p></li>
</ul>
</section>
<section id="be-very-careful-with-any-data-augmentation">
<h3>Be very careful with any data augmentation.<a class="headerlink" href="#be-very-careful-with-any-data-augmentation" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: Data augmentation cannot be applied to RL in the same ways as CV since an agent needs to act based on the observation. As an example, flipping an image may correspondingly “flip” the appropriate action.</p></li>
</ul>
</section>
</section>
<section id="policy">
<h2>Policy<a class="headerlink" href="#policy" title="Link to this heading">¶</a></h2>
<section id="does-the-entropy-of-your-policy-converge-too-quickly-too-slowly-or-change-drastically">
<h3>Does the entropy of your policy converge too quickly, too slowly or change drastically?<a class="headerlink" href="#does-the-entropy-of-your-policy-converge-too-quickly-too-slowly-or-change-drastically" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: This can be algorithm dependent, but the entropy of the policy is roughly inversely related to the expected value of actions.</p></li>
<li><p>Prescription: Tuning the coefficient of an entropy bonus (i.e., beta in PPO) can help entropies that converge too quickly/slowly. Alternatively, reducing/increasing the magnitude of rewards may also help if converging too quickly/slowly. Entropy curves that step-change dramatically are usually downstream of an issue with the problem formulation (i.e., obs or action space), learning rate, gradient norms or a bug in the implementation.</p></li>
</ul>
</section>
</section>
<section id="rewards-beyond-going-up">
<h2>Rewards (beyond “going up”)<a class="headerlink" href="#rewards-beyond-going-up" title="Link to this heading">¶</a></h2>
<section id="is-the-agent-favoring-a-single-component-of-the-reward-function-i-e-velocity-vs-l2-action-magnitude">
<h3>Is the agent favoring a single component of the reward function (i.e. velocity vs L2 action magnitude)?<a class="headerlink" href="#is-the-agent-favoring-a-single-component-of-the-reward-function-i-e-velocity-vs-l2-action-magnitude" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: It may be the case that one of the components of the reward function is “easier” to optimize and so an agent will find the behavior as a local optima.</p></li>
<li><p>Prescription: In addition to tuning coefficients of reward components, it may also make sense to use the product of components instead of the sum. Tracking the stats w.r.t. each reward component may also yield insight. Alternatively, if some components are considered ‘auxiliary’, decaying the weight over time may be helpful.</p></li>
</ul>
</section>
<section id="is-the-task-horizon-extremely-long">
<h3>Is the task horizon extremely long?<a class="headerlink" href="#is-the-task-horizon-extremely-long" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: Credit assignment (i.e., attributing future/value rewards to past state/actions) becomes more difficult with the time between action and corresponding reward. In sparse reward environments, this can be a source of training inefficiency requiring many interactions with the environment.</p></li>
<li><p>Prescription: Adding intermediate rewards for behaviors that are instrumental to the final goal can greatly increase training speed (e.g., in a soccer environment, an intermediate reward for kicking the ball will increase the likelihood that an agent discovers scoring a goal is rewarding).  This may create undesired optima though as exploiting the intermediate reward may unintentionally be more valuable than the true reward or lead to undesired idiosyncratic behaviors. One can decay the value of this intermediate reward to zero using a step or reward based curriculum. Alternatively, if there are many subtasks, one can use a hierarchical or options based framework where individual policies are learned for different subtasks (e.g., kicking, passing, running) and then a higher level agent selects from these low level policies as its action space. Note, this issue may also fall under the “Exploration” section and require explicit exploration mechanisms such as the <a class="reference external" href="https://arxiv.org/pdf/1705.05363.pdf">Intrinsic Curiosity Module.</a></p></li>
</ul>
</section>
<section id="are-your-rewards-normalized-standardized">
<h3>Are your rewards normalized/standardized?<a class="headerlink" href="#are-your-rewards-normalized-standardized" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: Rewards of magnitudinally larger scale will dominate smaller rewards. Additionally, if per timestep rewards get really large, the targets for value functions will become huge as they are the sum of the per timestep rewards.</p></li>
<li><p>Prescription: In general, keeping rewards between [-1,1] is good practice. Alternatively, you can use running mean/std instance normalization (e.g., the TorchRL <a class="reference external" href="https://github.com/pytorch/rl/blob/20b6fc92574959b5edd0a7658e3d45ecadaef2eb/torchrl/envs/transforms/transforms.py#L2313">implementation</a> or the Gym <a class="reference external" href="https://github.com/openai/gym/blob/master/gym/wrappers/normalize.py">implementation</a>).</p></li>
</ul>
</section>
</section>
<section id="exploration">
<h2>Exploration<a class="headerlink" href="#exploration" title="Link to this heading">¶</a></h2>
<section id="is-value-loss-going-up-early-in-training">
<h3>Is value loss going up early in training?<a class="headerlink" href="#is-value-loss-going-up-early-in-training" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: Typically, at initialization value estimates are ~0.0. Early in training, an agent will likely be encountering new, unseen extrinsic as it explores and so the value estimates will be wrong and loss goes up.</p></li>
<li><p>Prescription: Increasing exploration via intrinsic rewards or entropy bonuses. Alternatively, making the reward function denser by adding intermediate rewards.</p></li>
</ul>
</section>
<section id="are-actions-roughly-uniformly-normally-random-early-in-training">
<h3>Are actions (roughly) uniformly/normally random early in training?<a class="headerlink" href="#are-actions-roughly-uniformly-normally-random-early-in-training" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: If no priors are used, a freshly initialized network should be near random. This is important for an agent to achieve proper exploration.</p></li>
<li><p>Prescription: Check the policy network is initialized appropriately and that policy entropy doesn’t drop really quickly.</p></li>
</ul>
</section>
<section id="are-intrinsic-rewards-decaying-as-learning-progresses-in-a-singleton-task">
<h3>Are intrinsic rewards decaying as learning progresses in a <a class="reference external" href="https://arxiv.org/pdf/2210.05805.pdf">singleton</a> task?<a class="headerlink" href="#are-intrinsic-rewards-decaying-as-learning-progresses-in-a-singleton-task" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: Intrinsic rewards are meant to encourage exploration, typically by some measure of novelty. As an agent explores, the value of additional exploration (or revisiting previously explored state-actions) is diminished as novelty decreases.  Ideally, as intrinsic reward starts to go down, extrinsic reward should start to increase.</p></li>
<li><p>Prescription: Intrinsic rewards should be normalized. If the intrinsic reward has gone to 0 but the agent has not learned anything, one can try slow the dynamics of the intrinsic module (i.e., reduce the learning rate of Random Network Distillation or add noise).</p></li>
</ul>
</section>
<section id="are-episodic-intrinsic-rewards-remaining-constant-or-increasing-as-learning-progresses-in-an-episodic-task">
<h3>Are <a class="reference external" href="https://arxiv.org/pdf/2210.05805.pdf">episodic</a> intrinsic rewards remaining constant or increasing as learning progresses in an episodic task?<a class="headerlink" href="#are-episodic-intrinsic-rewards-remaining-constant-or-increasing-as-learning-progresses-in-an-episodic-task" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: Intrinsic rewards are meant to encourage exploration, typically by some measure of novelty. In episodic tasks, since novelty may not decrease and exploratory behavior may actually improve, intrinsic rewards should remain constant or increase.</p></li>
<li><p>Prescription:  Extrinsic reward should of course also increase. If that is not the case, it could mean that the two objectives are misaligned and that there is a trade off between the two. If such a trade off is unavoidable, then the extrinsic reward needs to have priority over the episodic bonus. Some ways to achieve this are to use a decaying schedule on the episodic bonus, have separate explore (with episodic bonus only) and exploit (with extrinsic reward only) policies and use the explore policy to generate more diverse starting states for the exploit policy or use behavioral cloning to bootstrap training. Also, intrinsic rewards should be normalized.</p></li>
</ul>
</section>
</section>
<section id="environment-dynamics">
<h2>Environment Dynamics<a class="headerlink" href="#environment-dynamics" title="Link to this heading">¶</a></h2>
<section id="can-you-train-a-low-entropy-forward-dynamics-and-or-reward-model-also-useful-for-offline-rl">
<h3>Can you train a low entropy forward dynamics and/or reward model (also useful for offline RL)?<a class="headerlink" href="#can-you-train-a-low-entropy-forward-dynamics-and-or-reward-model-also-useful-for-offline-rl" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: The next state and rewards are used to generate targets for value learning in RL algorithms. If these are very noisy, then the targets will be noisy and learning may be slow or unstable. Environments may be inherently stochastic (i.e., random spawns of enemies), the formulation of the obs space may have a missing variable (i.e., a POMDP) or the dependence on the previous state may just be very loose to nonexistent.</p></li>
<li><p>Prescription: Depending on the source of the noise, it may be useful to revisit the observation formulation to be sure it includes all necessary information, a network architecture that can process the sequence of previous states rather than just the last state (i.e., LSTM, Transformer) or even use a Distributional RL algorithm to explicitly model the distribution of value (rather than just expected value).</p></li>
</ul>
</section>
</section>
<section id="observation-space">
<h2>Observation Space<a class="headerlink" href="#observation-space" title="Link to this heading">¶</a></h2>
<section id="are-your-observations-normalized-standardized">
<h3>Are your observations normalized/standardized?<a class="headerlink" href="#are-your-observations-normalized-standardized" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: Input and output targets that have the same relative scale tend to be more stable as network weights don’t need to get really large/small to compensate.  For the same reason, learning tends to be faster since network weights are initialized to an appropriate scale and don’t need to get there by gradient descent. Additionally, if there is extreme difference in scale between observation features (e.g., [-1,+1] vs. [-1000, 1000]), the larger may dominate the smaller before weights can compensate.</p></li>
<li><p>Prescription: If you know the minimum/maximum ranges for these values, you can manually normalize to the range of [0,1]. Alternatively, you can use running mean/std instance normalization (e.g., the TorchRL <a class="reference external" href="https://github.com/pytorch/rl/blob/20b6fc92574959b5edd0a7658e3d45ecadaef2eb/torchrl/envs/transforms/transforms.py#L2313">implementation</a> or the Gym <a class="reference external" href="https://github.com/openai/gym/blob/master/gym/wrappers/normalize.py">implementation</a>). The mean and std deviation will change radically at the beginning of training but then slowly converge with more data. One can collect a large buffer before making any updates to compute a starting mean and std if stability is a problem.</p></li>
</ul>
</section>
</section>
<section id="action-space">
<h2>Action Space<a class="headerlink" href="#action-space" title="Link to this heading">¶</a></h2>
<section id="is-the-effect-of-an-action-changing-dramatically-during-an-episode">
<h3>Is the effect of an action changing dramatically during an episode?<a class="headerlink" href="#is-the-effect-of-an-action-changing-dramatically-during-an-episode" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: If an action leads to a failure during the first stages of training, the agent may learn to never perform it and it could prevent it from solving the task entirely (i.e., a ‘submit your work’ action).</p></li>
<li><p>Prescription: It may be that the problem should be formulated hierarchically (i.e. an agent that learns to ‘submit work’). Additionally, sufficient exploration becomes very important in this case.</p></li>
</ul>
</section>
<section id="is-the-action-space-too-high-dimensional">
<h3>Is the action space too high dimensional?<a class="headerlink" href="#is-the-action-space-too-high-dimensional" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: If the action space is extremely large (i.e., recommender systems), it may be the case that adequately exploring the entire action space is infeasible.</p></li>
<li><p>Prescription: To alleviate this, one could manually prune the action space or develop state-dependent heuristics to mask/filter which actions are available to the agent (e.g., masking out the “fire” action in certain Atari games or illegal moves in chess) or combine actions/action sequences (e.g., grasp and release actions in manipulation tasks could be the same action and also sequences of primitives). If this is not possible, alternative methods exist such as <a class="reference external" href="https://arxiv.org/pdf/2210.01241.pdf">top-p</a> sampling wherein you sample from only the top actions with cumulative probability p.</p></li>
</ul>
</section>
<section id="are-your-actions-normalized-standardized">
<h3>Are your actions normalized/standardized?<a class="headerlink" href="#are-your-actions-normalized-standardized" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Reason: Input and output targets that have the same relative scale tend to be more stable as network weights don’t need to get really large/small to compensate. For the same reason, learning tends to be faster since network weights are initialized to an appropriate scale and don’t need to get there by gradient descent. In some algorithms, actions can be input to a Q function and in others gradients can flow directly through the action output into the policy (e.g., reparameterization in Soft Actor-Critic) so it is important for reasonably scaled actions.</p></li>
<li><p>Prescription: It is common to <a class="reference external" href="https://github.com/DLR-RM/stable-baselines3/blob/b702884c23b6aeaa5d2a830b37d6b15fb1bdf983/stable_baselines3/common/policies.py#L354">clip</a> the action outputs of a policy to a reasonable range. Note, this clipped action should not (as opposed to the raw action) be used for training because the clip operation is not part of the computation graph and gradients will be incorrect. This should be thought of as part of the environment and so a policy will learn that actions in the bounded region lead to higher reward. One can also use a squashing function such as tanh. This can be part of the computation graph and to do this efficiently, one should correct the log probs such as is done <a class="reference external" href="https://github.com/Unity-Technologies/ml-agents/blob/develop/ml-agents/mlagents/trainers/torch_entities/distributions.py#L110">here</a>.  Remember to remap actions to the original action space on the environment side if normalized.</p></li>
</ul>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="GYM.html" class="btn btn-neutral float-right" title="Working with gym" accesskey="n" rel="next">Next <img src="../../../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="../../knowledge_base.html" class="btn btn-neutral" title="Knowledge Base" accesskey="p" rel="prev"><img src="../../../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Things to consider when debugging RL</a><ul>
<li><a class="reference internal" href="#general">General</a><ul>
<li><a class="reference internal" href="#have-you-validated-your-algorithm-implementation-on-a-few-small-toy-problems-with-known-optimal-returns-e-g-gridworlds-mountaincar">Have you validated your algorithm implementation on a few small, toy problems with known optimal returns e.g. gridworlds, mountaincar?</a></li>
<li><a class="reference internal" href="#have-you-visualized-your-agents">Have you visualized your agents?</a></li>
<li><a class="reference internal" href="#be-very-careful-with-any-data-augmentation">Be very careful with any data augmentation.</a></li>
</ul>
</li>
<li><a class="reference internal" href="#policy">Policy</a><ul>
<li><a class="reference internal" href="#does-the-entropy-of-your-policy-converge-too-quickly-too-slowly-or-change-drastically">Does the entropy of your policy converge too quickly, too slowly or change drastically?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#rewards-beyond-going-up">Rewards (beyond “going up”)</a><ul>
<li><a class="reference internal" href="#is-the-agent-favoring-a-single-component-of-the-reward-function-i-e-velocity-vs-l2-action-magnitude">Is the agent favoring a single component of the reward function (i.e. velocity vs L2 action magnitude)?</a></li>
<li><a class="reference internal" href="#is-the-task-horizon-extremely-long">Is the task horizon extremely long?</a></li>
<li><a class="reference internal" href="#are-your-rewards-normalized-standardized">Are your rewards normalized/standardized?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#exploration">Exploration</a><ul>
<li><a class="reference internal" href="#is-value-loss-going-up-early-in-training">Is value loss going up early in training?</a></li>
<li><a class="reference internal" href="#are-actions-roughly-uniformly-normally-random-early-in-training">Are actions (roughly) uniformly/normally random early in training?</a></li>
<li><a class="reference internal" href="#are-intrinsic-rewards-decaying-as-learning-progresses-in-a-singleton-task">Are intrinsic rewards decaying as learning progresses in a singleton task?</a></li>
<li><a class="reference internal" href="#are-episodic-intrinsic-rewards-remaining-constant-or-increasing-as-learning-progresses-in-an-episodic-task">Are episodic intrinsic rewards remaining constant or increasing as learning progresses in an episodic task?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#environment-dynamics">Environment Dynamics</a><ul>
<li><a class="reference internal" href="#can-you-train-a-low-entropy-forward-dynamics-and-or-reward-model-also-useful-for-offline-rl">Can you train a low entropy forward dynamics and/or reward model (also useful for offline RL)?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#observation-space">Observation Space</a><ul>
<li><a class="reference internal" href="#are-your-observations-normalized-standardized">Are your observations normalized/standardized?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#action-space">Action Space</a><ul>
<li><a class="reference internal" href="#is-the-effect-of-an-action-changing-dramatically-during-an-episode">Is the effect of an action changing dramatically during an episode?</a></li>
<li><a class="reference internal" href="#is-the-action-space-too-high-dimensional">Is the action space too high dimensional?</a></li>
<li><a class="reference internal" href="#are-your-actions-normalized-standardized">Are your actions normalized/standardized?</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>
  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'main',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../../_static/design-tabs.js"></script>

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Lorem ipsum dolor sit amet, consectetur</p>
          <a class="with-right-arrow" href="https://shiftlab.github.io/pytorch/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://shiftlab.github.io/pytorch/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/">PyTorch</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/get-started">Get Started</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/features">Features</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/ecosystem">Ecosystem</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/blog/">Blog</a></li>
            <li><a href="https://shiftlab.github.io/pytorch/resources">Resources</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://shiftlab.github.io/pytorch/support">Support</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.slack.com" target="_blank">Slack</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Follow Us</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://shiftlab.github.io/pytorch/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="#">Get Started</a>
          </li>

          <li>
            <a href="#">Features</a>
          </li>

          <li>
            <a href="#">Ecosystem</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://shiftlab.github.io/pytorch/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    mobileMenu.bind();
    mobileTOC.bind();
    pytorchAnchors.bind();

    $(window).on("load", function() {
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
    })

    // Add class to links that have code blocks, since we cannot create links in code blocks
    $("article.pytorch-article a span.pre").each(function(e) {
      $(this).closest("a").addClass("has-code");
    });
  </script>
</body>
</html>