


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>LLM Interface &mdash; torchrl main documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="History" href="generated/torchrl.data.llm.History.html" />
    <link rel="prev" title="register_gym_spec_conversion" href="generated/torchrl.envs.register_gym_spec_conversion.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            
    <div class="version">
      <a href="../../versions.html"><span style="font-size:110%">main (0.0.0+unknown) &#x25BC</span></a>
    </div>
    


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/getting-started-0.html">Get started with Environments, TED and transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/getting-started-1.html">Get started with TorchRL’s modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/getting-started-2.html">Getting started with model optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/getting-started-3.html">Get started with data collection and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/getting-started-4.html">Get started with logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/getting-started-5.html">Get started with your own first training loop</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/coding_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torchrl_demo.html">Introduction to TorchRL</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/multiagent_ppo.html">Multi-Agent Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/torchrl_envs.html">TorchRL envs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/pretrained_models.html">Using pretrained models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/dqn_with_rnn.html">Recurrent DQN: Training recurrent policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/rb_tutorial.html">Using Replay Buffers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/export.html">Exporting TorchRL modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/llm_browser.html">TorchRL LLM: Building Tool-Enabled Environments</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/multiagent_competitive_ddpg.html">Competitive Multi-Agent Reinforcement Learning (DDPG) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/multi_task.html">Task-specific policy in multi-task environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/coding_ddpg.html">TorchRL objectives: Coding a DDPG loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/coding_dqn.html">TorchRL trainer: A DQN example</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">API Reference</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="knowledge_base.html">Knowledge Base</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">API Reference</a> &gt;</li>
        
      <li>LLM Interface</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/reference/llms.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
    
    
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="llm-interface">
<h1>LLM Interface<a class="headerlink" href="#llm-interface" title="Permalink to this heading">¶</a></h1>
<p id="ref-llms">TorchRL provides a comprehensive framework for LLM post-training and fine-tuning. The LLM API is built around five core concepts that work
together to create a complete reinforcement learning pipeline for language models:</p>
<ol class="arabic simple">
<li><p><strong>Data Representation</strong> (<a class="reference internal" href="#data-structures">Data Structures</a>): The foundation for handling conversations, text parsing, and LLM
output classes. This includes the <a class="reference internal" href="generated/torchrl.data.llm.History.html#torchrl.data.llm.History" title="torchrl.data.llm.History"><code class="xref py py-class docutils literal notranslate"><span class="pre">History</span></code></a> class for managing conversation context and structured output classes for
tokens, log-probabilities, and text.</p></li>
<li><p><strong>LLM Wrapper API</strong> (<a class="reference internal" href="#modules">Modules</a>): Unified interfaces for different LLM backends, including <a class="reference internal" href="generated/torchrl.modules.llm.TransformersWrapper.html#torchrl.modules.llm.TransformersWrapper" title="torchrl.modules.llm.TransformersWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformersWrapper</span></code></a> for
Hugging Face models, <a class="reference internal" href="generated/torchrl.modules.llm.vLLMWrapper.html#torchrl.modules.llm.vLLMWrapper" title="torchrl.modules.llm.vLLMWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">vLLMWrapper</span></code></a> for vLLM inference, and <a class="reference internal" href="generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM" title="torchrl.modules.llm.AsyncVLLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">AsyncVLLM</span></code></a> for high-performance
distributed vLLM inference (recommended). These wrappers provide consistent input/output formats across different backends and an integrated
interface for loss computation, data storage, grading, weight synchronization, etc.</p></li>
<li><p><strong>Environments</strong> (<a class="reference internal" href="#environments">Environments</a>): The orchestration layer that manages data loading, tool execution, reward computation, and formatting. This includes
<a class="reference internal" href="generated/torchrl.envs.llm.ChatEnv.html#torchrl.envs.llm.ChatEnv" title="torchrl.envs.llm.ChatEnv"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatEnv</span></code></a> for conversation management, dataset environments, and various transforms for tool integration.</p></li>
<li><p><strong>Objectives</strong> (<a class="reference internal" href="#objectives">Objectives</a>): Specialized loss functions for LLM training, including <a class="reference internal" href="generated/torchrl.objectives.llm.GRPOLoss.html#torchrl.objectives.llm.GRPOLoss" title="torchrl.objectives.llm.GRPOLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">GRPOLoss</span></code></a> for Group Relative
Policy Optimization and <a class="reference internal" href="generated/torchrl.objectives.llm.SFTLoss.html#torchrl.objectives.llm.SFTLoss" title="torchrl.objectives.llm.SFTLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">SFTLoss</span></code></a> for supervised fine-tuning.</p></li>
<li><p><strong>Collectors</strong> (<a class="reference internal" href="#id1">Collectors</a>): Collectors are used to collect data from the environment and store it in a format that can be used for training. This includes
<a class="reference internal" href="generated/torchrl.collectors.llm.LLMCollector.html#torchrl.collectors.llm.LLMCollector" title="torchrl.collectors.llm.LLMCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">LLMCollector</span></code></a> for collecting data from the environment and <a class="reference internal" href="generated/torchrl.collectors.llm.RayLLMCollector.html#torchrl.collectors.llm.RayLLMCollector" title="torchrl.collectors.llm.RayLLMCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">RayLLMCollector</span></code></a> for collecting
data in distributed settings using Ray.</p></li>
</ol>
<p>These components work together to create a complete pipeline: environments load and format data, LLM wrappers handle inference, data structures maintain
conversation context, and objectives compute training losses. The modular design allows you to mix and match components based on your specific use case.</p>
<p>A complete example of how to use the LLM API can be found in the <cite>sota-implementations/grpo/</cite> directory. The training orchestration involves three main components:</p>
<ul class="simple">
<li><p>The Data Collector: holds a reference to the environment and the inference model or engine. It collects data, puts it in the buffer, and handles weight updates.</p></li>
<li><p>The Replay Buffer: stores the collected data and executes any pre or post-processing steps. These may include:
- Advantage estimation with Monte-Carlo based method (using the <a class="reference internal" href="generated/torchrl.objectives.llm.MCAdvantage.html#torchrl.objectives.llm.MCAdvantage" title="torchrl.objectives.llm.MCAdvantage"><code class="xref py py-class docutils literal notranslate"><span class="pre">MCAdvantage</span></code></a> transform);
- Grading of the outputs;
- Logging etc.</p></li>
<li><p>The trainer: handles the training loop, including the optimization step, serialization, logging and weight updates initialization.</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The LLM API is still under development and may change in the future. Feedback, issues and PRs are welcome!</p>
</div>
<section id="data-structures">
<h2>Data Structures<a class="headerlink" href="#data-structures" title="Permalink to this heading">¶</a></h2>
<p>The data representation layer provides the foundation for handling conversations and LLM outputs in a structured way.</p>
<section id="history-class">
<h3>History Class<a class="headerlink" href="#history-class" title="Permalink to this heading">¶</a></h3>
<p>The <a class="reference internal" href="generated/torchrl.data.llm.History.html#torchrl.data.llm.History" title="torchrl.data.llm.History"><code class="xref py py-class docutils literal notranslate"><span class="pre">History</span></code></a> class is a TensorClass version of the chat format usually found in transformers
(see <a class="reference external" href="https://huggingface.co/docs/transformers/conversations">Hugging Face chat documentation</a>).
It provides a comprehensive API for managing conversation data with features including:</p>
<ul class="simple">
<li><p><strong>Text parsing and formatting</strong>: Convert between text and structured conversation format using <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_text()</span></code>
and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_chat_template()</span></code></p></li>
<li><p><strong>Dynamic conversation building</strong>: Append and extend conversations with <code class="xref py py-meth docutils literal notranslate"><span class="pre">append()</span></code> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">extend()</span></code> methods</p></li>
<li><p><strong>Multi-model support</strong>: Automatic template detection for various model families (Qwen, DialoGPT, Falcon, DeepSeek, etc.)</p></li>
<li><p><strong>Assistant token masking</strong>: Identify which tokens were generated by the assistant for reinforcement learning applications</p></li>
<li><p><strong>Tool calling support</strong>: Handle function calls and tool responses in conversations</p></li>
<li><p><strong>Batch operations</strong>: Efficient tensor operations for processing multiple conversations simultaneously.</p></li>
</ul>
<table class="autosummary longtable docutils # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.data.llm.History.html#torchrl.data.llm.History" title="torchrl.data.llm.History"><code class="xref py py-obj docutils literal notranslate"><span class="pre">History</span></code></a>(role, content[, is_complete, ...])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.data.llm.ContentBase.html#torchrl.data.llm.ContentBase" title="torchrl.data.llm.ContentBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ContentBase</span></code></a>(type, text, url, data, ...[, ...])</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<section id="supported-model-families">
<h4>Supported Model Families<a class="headerlink" href="#supported-model-families" title="Permalink to this heading">¶</a></h4>
<p>We currently support the following model families for string to History parsing or assistant token masking:</p>
<ul class="simple">
<li><p><strong>Qwen family</strong> (e.g., <cite>Qwen/Qwen2.5-0.5B</cite>): Custom template with full tool calling support</p></li>
<li><p><strong>DialoGPT family</strong> (e.g., <cite>microsoft/DialoGPT-medium</cite>): Custom template for conversation format</p></li>
<li><p><strong>Falcon family</strong> (e.g., <cite>tiiuae/falcon-7b-instruct</cite>): Custom template for instruction format</p></li>
<li><p><strong>DeepSeek family</strong> (e.g., <cite>deepseek-ai/deepseek-coder-6.7b-base</cite>): Custom template with native format</p></li>
</ul>
<p>Other models are supported, but you will need to provide a custom template for them.
LLAMA, Mistral, OPT, GPT, MPT, BLOOM, Pythia, Phi, etc. will use the default <cite>chatml_format</cite> template.</p>
</section>
<section id="usage">
<h4>Usage<a class="headerlink" href="#usage" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.llm.chat</span><span class="w"> </span><span class="kn">import</span> <span class="n">History</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create a conversation history</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">history</span> <span class="o">=</span> <span class="n">History</span><span class="o">.</span><span class="n">from_chats</span><span class="p">([[</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello&quot;</span><span class="p">},</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hi there!&quot;</span><span class="p">},</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;How are you?&quot;</span><span class="p">},</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;I&#39;m doing well, thanks!&quot;</span><span class="p">}</span>
<span class="gp">... </span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Load any supported tokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen2.5-0.5B&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Apply chat template with assistant token masking</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">chat_template_name</span><span class="o">=</span><span class="s2">&quot;qwen&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">return_assistant_tokens_mask</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The result contains an assistant_masks tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">assistant_masks</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;assistant_masks&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Assistant tokens: </span><span class="si">{</span><span class="n">assistant_masks</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="adding-custom-templates">
<h4>Adding Custom Templates<a class="headerlink" href="#adding-custom-templates" title="Permalink to this heading">¶</a></h4>
<p>You can add custom chat templates for new model families using the <a class="reference internal" href="#torchrl.data.llm.add_chat_template" title="torchrl.data.llm.add_chat_template"><code class="xref py py-func docutils literal notranslate"><span class="pre">torchrl.data.llm.add_chat_template()</span></code></a> function.</p>
<dl class="py function">
<dt class="sig sig-object py" id="torchrl.data.llm.add_chat_template">
<span class="sig-prename descclassname"><span class="pre">torchrl.data.llm.</span></span><span class="sig-name descname"><span class="pre">add_chat_template</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">template_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">template</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inverse_parser</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_family_keywords</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../_modules/torchrl/data/llm/history.html#add_chat_template"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torchrl.data.llm.add_chat_template" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a custom chat template to the global template dictionary.</p>
<p>This function allows you to add custom chat templates for new model families
that support assistant token masking via the <cite>{% generation %}</cite> keyword.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>template_name</strong> (<em>str</em>) – The name of the template (e.g., “llama”, “mistral”).
This name will be used in the <cite>chat_template_name</cite> parameter of
<cite>History.apply_chat_template()</cite> and <cite>History.from_text()</cite>.</p></li>
<li><p><strong>template</strong> (<em>str</em>) – The Jinja2 template string. Must include <cite>{% generation %}</cite>
blocks around assistant message content to enable token masking.</p></li>
<li><p><strong>inverse_parser</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function that parses formatted text back
into a History object. Should have signature <cite>(text: str) -&gt; History</cite>.
If None, a basic parser will be used.</p></li>
<li><p><strong>model_family_keywords</strong> (<em>list</em><em>[</em><em>str</em><em>]</em><em>, </em><em>optional</em>) – Keywords to detect this model family
in the auto-detection logic. For example, [“llama”, “meta-llama”] for Llama models.
If provided, the template will be automatically selected for models containing
these keywords in their name.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.llm.chat</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_chat_template</span><span class="p">,</span> <span class="n">History</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Add a custom template for Llama models</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">llama_template</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">% f</span><span class="s1">or message in messages %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">%- i</span><span class="s1">f message[&#39;role&#39;] == &#39;user&#39; %}</span>
<span class="gp">... </span><span class="s1">{{ &#39;&lt;s&gt;[INST] &#39; + message[&#39;content&#39;] + &#39; [/INST]&#39; }}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">%- e</span><span class="s1">lif message[&#39;role&#39;] == &#39;assistant&#39; %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">% g</span><span class="s1">eneration %}{{ message[&#39;content&#39;] + &#39;&lt;/s&gt;&#39; }}{</span><span class="si">% e</span><span class="s1">ndgeneration %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">%- e</span><span class="s1">ndif %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">% e</span><span class="s1">ndfor %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">%- i</span><span class="s1">f add_generation_prompt %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">% g</span><span class="s1">eneration %}{{ &#39; &#39; }}{</span><span class="si">% e</span><span class="s1">ndgeneration %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">%- e</span><span class="s1">ndif %}</span>
<span class="gp">... </span><span class="s1">&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">parse_llama_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">History</span><span class="p">:</span>
<span class="gp">... </span>    <span class="c1"># Custom parser for Llama format</span>
<span class="gp">... </span>    <span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="gp">... </span>    <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;&lt;s&gt;\[INST\]\s*(.*?)\s*\[/INST\]\s*(.*?)&lt;/s&gt;&#39;</span>
<span class="gp">... </span>    <span class="n">matches</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">DOTALL</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">user_content</span><span class="p">,</span> <span class="n">assistant_content</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">:</span>
<span class="gp">... </span>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">History</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="n">user_content</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>
<span class="gp">... </span>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">History</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="n">assistant_content</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">lazy_stack</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Add the template with auto-detection</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add_chat_template</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">template_name</span><span class="o">=</span><span class="s2">&quot;llama&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">template</span><span class="o">=</span><span class="n">llama_template</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">inverse_parser</span><span class="o">=</span><span class="n">parse_llama_text</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">model_family_keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;llama&quot;</span><span class="p">,</span> <span class="s2">&quot;meta-llama&quot;</span><span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now you can use it with auto-detection</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">history</span> <span class="o">=</span> <span class="n">History</span><span class="o">.</span><span class="n">from_chats</span><span class="p">([[</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello&quot;</span><span class="p">},</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hi there!&quot;</span><span class="p">}</span>
<span class="gp">... </span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Auto-detection will use the llama template</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">return_assistant_tokens_mask</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Or use it explicitly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">chat_template_name</span><span class="o">=</span><span class="s2">&quot;llama&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">return_assistant_tokens_mask</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="usage-examples">
<h4>Usage Examples<a class="headerlink" href="#usage-examples" title="Permalink to this heading">¶</a></h4>
<section id="adding-a-llama-template">
<h5>Adding a Llama Template<a class="headerlink" href="#adding-a-llama-template" title="Permalink to this heading">¶</a></h5>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">add_chat_template</span><span class="p">,</span> <span class="n">History</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define the Llama chat template</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">llama_template</span> <span class="o">=</span> <span class="s1">&#39;&#39;&#39;</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">% f</span><span class="s1">or message in messages %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">%- i</span><span class="s1">f message[&#39;role&#39;] == &#39;user&#39; %}</span>
<span class="gp">... </span><span class="s1">{{ &#39;&lt;s&gt;[INST] &#39; + message[&#39;content&#39;] + &#39; [/INST]&#39; }}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">%- e</span><span class="s1">lif message[&#39;role&#39;] == &#39;assistant&#39; %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">% g</span><span class="s1">eneration %}{{ message[&#39;content&#39;] + &#39;&lt;/s&gt;&#39; }}{</span><span class="si">% e</span><span class="s1">ndgeneration %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">%- e</span><span class="s1">ndif %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">% e</span><span class="s1">ndfor %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">%- i</span><span class="s1">f add_generation_prompt %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">% g</span><span class="s1">eneration %}{{ &#39; &#39; }}{</span><span class="si">% e</span><span class="s1">ndgeneration %}</span>
<span class="gp">... </span><span class="s1">{</span><span class="si">%- e</span><span class="s1">ndif %}</span>
<span class="gp">... </span><span class="s1">&#39;&#39;&#39;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define the inverse parser for Llama format</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">parse_llama_text</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">History</span><span class="p">:</span>
<span class="gp">... </span>    <span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="gp">... </span>    <span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;&lt;s&gt;\[INST\]\s*(.*?)\s*\[/INST\]\s*(.*?)&lt;/s&gt;&#39;</span>
<span class="gp">... </span>    <span class="n">matches</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">pattern</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">DOTALL</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">user_content</span><span class="p">,</span> <span class="n">assistant_content</span> <span class="ow">in</span> <span class="n">matches</span><span class="p">:</span>
<span class="gp">... </span>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">History</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="n">user_content</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>
<span class="gp">... </span>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">History</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="n">assistant_content</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">lazy_stack</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Add the template with auto-detection</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add_chat_template</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">template_name</span><span class="o">=</span><span class="s2">&quot;llama&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">template</span><span class="o">=</span><span class="n">llama_template</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">inverse_parser</span><span class="o">=</span><span class="n">parse_llama_text</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">model_family_keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;llama&quot;</span><span class="p">,</span> <span class="s2">&quot;meta-llama&quot;</span><span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now you can use it with auto-detection</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">history</span> <span class="o">=</span> <span class="n">History</span><span class="o">.</span><span class="n">from_chats</span><span class="p">([[</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello&quot;</span><span class="p">},</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hi there!&quot;</span><span class="p">}</span>
<span class="gp">... </span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Auto-detection will use the llama template</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">return_assistant_tokens_mask</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="testing-your-custom-templates">
<h4>Testing Your Custom Templates<a class="headerlink" href="#testing-your-custom-templates" title="Permalink to this heading">¶</a></h4>
<p>When adding custom templates, you should test them to ensure they work correctly. Here are the recommended tests:</p>
<section id="assistant-token-masking-test">
<h5>Assistant Token Masking Test<a class="headerlink" href="#assistant-token-masking-test" title="Permalink to this heading">¶</a></h5>
<p>Test that your template supports assistant token masking:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.llm.chat</span><span class="w"> </span><span class="kn">import</span> <span class="n">History</span><span class="p">,</span> <span class="n">add_chat_template</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_my_model_assistant_masking</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that your model supports assistant token masking.&quot;&quot;&quot;</span>
    <span class="c1"># Add your template first</span>
    <span class="n">add_chat_template</span><span class="p">(</span>
        <span class="n">template_name</span><span class="o">=</span><span class="s2">&quot;my_model&quot;</span><span class="p">,</span>
        <span class="n">template</span><span class="o">=</span><span class="s2">&quot;your_template_here&quot;</span><span class="p">,</span>
        <span class="n">model_family_keywords</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;my_model&quot;</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;your/model/name&quot;</span><span class="p">)</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">History</span><span class="o">.</span><span class="n">from_chats</span><span class="p">([[</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Hello&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Hi there!&#39;</span><span class="p">}</span>
    <span class="p">]])</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">chat_template_name</span><span class="o">=</span><span class="s2">&quot;my_model&quot;</span><span class="p">,</span>
        <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">return_assistant_tokens_mask</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Verify assistant mask is present</span>
    <span class="k">assert</span> <span class="s1">&#39;assistant_masks&#39;</span> <span class="ow">in</span> <span class="n">result</span>
    <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;assistant_masks&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Should have batch dimension of 1&quot;</span>
    <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;assistant_masks&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Should have sequence length &gt; 0&quot;</span>

    <span class="c1"># Verify some assistant tokens are masked</span>
    <span class="n">assistant_token_count</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;assistant_masks&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">assistant_token_count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Should have assistant tokens masked&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;✓ </span><span class="si">{</span><span class="n">assistant_token_count</span><span class="si">}</span><span class="s2"> assistant tokens masked&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="template-equivalence-test">
<h5>Template Equivalence Test<a class="headerlink" href="#template-equivalence-test" title="Permalink to this heading">¶</a></h5>
<p>Test that your custom template produces the same output as the model’s default template (except for masking):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_my_model_template_equivalence</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that your template matches the model&#39;s default template.&quot;&quot;&quot;</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;your/model/name&quot;</span><span class="p">)</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">History</span><span class="o">.</span><span class="n">from_chats</span><span class="p">([[</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Hello&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Hi there!&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;How are you?&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;I</span><span class="se">\&#39;</span><span class="s1">m good, thanks!&#39;</span><span class="p">},</span>
    <span class="p">]])</span>

    <span class="c1"># Get output with model&#39;s default template</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">default_out</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
            <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">chat_template</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">chat_template</span><span class="p">,</span>
            <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">default_out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[WARN] Could not get default template: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Get output with your custom template</span>
    <span class="n">custom_out</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">chat_template_name</span><span class="o">=</span><span class="s2">&quot;my_model&quot;</span><span class="p">,</span>
        <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">default_out</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Normalize whitespace for comparison</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;\s+&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>

        <span class="k">assert</span> <span class="n">norm</span><span class="p">(</span><span class="n">default_out</span><span class="p">)</span> <span class="o">==</span> <span class="n">norm</span><span class="p">(</span><span class="n">custom_out</span><span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Custom template does not match default!</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Default: </span><span class="si">{</span><span class="n">default_out</span><span class="si">}</span><span class="se">\n</span><span class="s2">Custom: </span><span class="si">{</span><span class="n">custom_out</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;✓ Template equivalence verified&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;[INFO] Skipped equivalence check (no default template available)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="inverse-parsing-test">
<h5>Inverse Parsing Test<a class="headerlink" href="#inverse-parsing-test" title="Permalink to this heading">¶</a></h5>
<p>If you provided an inverse parser, test that it works correctly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">test_my_model_inverse_parsing</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that your inverse parser works correctly.&quot;&quot;&quot;</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">History</span><span class="o">.</span><span class="n">from_chats</span><span class="p">([[</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Hello&#39;</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;Hi there!&#39;</span><span class="p">}</span>
    <span class="p">]])</span>

    <span class="c1"># Format using your template</span>
    <span class="n">formatted</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">chat_template_name</span><span class="o">=</span><span class="s2">&quot;my_model&quot;</span><span class="p">,</span>
        <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Parse back using your inverse parser</span>
    <span class="n">parsed</span> <span class="o">=</span> <span class="n">History</span><span class="o">.</span><span class="n">from_text</span><span class="p">(</span><span class="n">formatted</span><span class="p">,</span> <span class="n">chat_template_name</span><span class="o">=</span><span class="s2">&quot;my_model&quot;</span><span class="p">)</span>

    <span class="c1"># Verify the parsing worked</span>
    <span class="k">assert</span> <span class="n">parsed</span><span class="o">.</span><span class="n">role</span> <span class="o">==</span> <span class="n">history</span><span class="o">.</span><span class="n">role</span>
    <span class="k">assert</span> <span class="n">parsed</span><span class="o">.</span><span class="n">content</span> <span class="o">==</span> <span class="n">history</span><span class="o">.</span><span class="n">content</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;✓ Inverse parsing verified&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="llm-wrapper-api">
<h3>LLM Wrapper API<a class="headerlink" href="#llm-wrapper-api" title="Permalink to this heading">¶</a></h3>
<p>The LLM wrapper API provides unified interfaces for different LLM backends, ensuring consistent input/output formats across training and inference pipelines. The main wrappers are <a class="reference internal" href="generated/torchrl.modules.llm.TransformersWrapper.html#torchrl.modules.llm.TransformersWrapper" title="torchrl.modules.llm.TransformersWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformersWrapper</span></code></a> for Hugging Face models and <a class="reference internal" href="generated/torchrl.modules.llm.vLLMWrapper.html#torchrl.modules.llm.vLLMWrapper" title="torchrl.modules.llm.vLLMWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">vLLMWrapper</span></code></a> for vLLM inference.</p>
<p><strong>Data Structure Classes</strong></p>
<p>The wrappers use structured <code class="xref py py-class docutils literal notranslate"><span class="pre">TensorClass</span></code> objects to represent different aspects of LLM data:</p>
<ul class="simple">
<li><p><strong>:class:`~torchrl.modules.llm.policies.Text`</strong>: Contains text data with <cite>prompt</cite>, <cite>response</cite>, and <cite>full</cite> fields</p></li>
<li><p><strong>:class:`~torchrl.modules.llm.policies.ChatHistory`</strong>: Contains <a class="reference internal" href="generated/torchrl.data.llm.History.html#torchrl.data.llm.History" title="torchrl.data.llm.History"><code class="xref py py-class docutils literal notranslate"><span class="pre">History</span></code></a> objects with <cite>prompt</cite>, <cite>response</cite>, and <cite>full</cite> fields</p></li>
<li><p><strong>:class:`~torchrl.modules.llm.policies.Tokens`</strong>: Contains tokenized data with <cite>prompt</cite>, <cite>response</cite>, and <cite>full</cite> fields</p></li>
<li><p><strong>:class:`~torchrl.modules.llm.policies.LogProbs`</strong>: Contains log probabilities with <cite>prompt</cite>, <cite>response</cite>, and <cite>full</cite> fields</p></li>
<li><p><strong>:class:`~torchrl.modules.llm.policies.Masks`</strong>: Contains attention and assistant masks</p></li>
</ul>
<p><strong>API Flow</strong></p>
<p>The wrappers operate in two distinct modes:</p>
<p><strong>Generation Mode (`generate=True`)</strong>:
- <strong>Input</strong>: Reads from <cite>prompt</cite> fields (e.g., <cite>history.prompt</cite>, <cite>text.prompt</cite>, <cite>tokens.prompt</cite>)
- <strong>Output</strong>: Writes to both <cite>response</cite> and <cite>full</cite> fields</p>
<blockquote>
<div><ul class="simple">
<li><p><cite>response</cite>: Contains only the newly generated content</p></li>
<li><p><cite>full</cite>: Contains the complete sequence (prompt + response)</p></li>
</ul>
</div></blockquote>
<p><strong>Log-Probability Mode (`generate=False`)</strong>:
- <strong>Input</strong>: Reads from <cite>full</cite> fields (e.g., <cite>history.full</cite>, <cite>text.full</cite>, <cite>tokens.full</cite>)
- <strong>Output</strong>: Writes log probabilities to the corresponding <cite>full</cite> fields</p>
<p><strong>LLM-Environment Interaction Loop</strong></p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="../_images/llm-env.png"><img alt="LLM-Environment interaction loop" src="../_images/llm-env.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-text">LLM-Environment interaction: the LLM generates a response, the environment updates the conversation, and transforms can inject new messages or tools.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In a typical RL or tool-augmented setting, the LLM and environment interact in a loop:</p>
<ol class="arabic simple">
<li><p><strong>LLM Generation</strong>: The LLM wrapper receives a <cite>prompt</cite> (the current conversation history), generates a <cite>response</cite>, and outputs a <cite>full</cite> field</p></li>
</ol>
<blockquote>
<div><p>containing the concatenation of the prompt and response.</p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>Environment Step</strong>: The environment takes the <cite>full</cite> field and makes it the next <cite>prompt</cite> for the LLM. This ensures that the conversation</p></li>
</ol>
<blockquote>
<div><p>context grows with each turn. See <span class="xref std std-ref">ref_env_llm_step</span> for more details.</p>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p><strong>Transforms</strong>: Before the next LLM step, transforms can modify the conversation—for example, by inserting a new user message, a tool call,</p></li>
</ol>
<blockquote>
<div><p>or a reward annotation.</p>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p><strong>Repeat</strong>: This process repeats for as many turns as needed, enabling multi-turn dialogue, tool use, and RL training.</p></li>
</ol>
<p>This design allows for flexible augmentation of the conversation at each step, supporting advanced RL and tool-use scenarios.</p>
<p>A typical pseudocode loop:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the first prompt out of an initial query</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">TensorDict</span><span class="p">({</span><span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello!&quot;</span><span class="p">},</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
<span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
    <span class="c1"># LLM generates a response given the current prompt</span>
    <span class="n">llm_output</span> <span class="o">=</span> <span class="n">llm</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
    <span class="c1"># Environment steps: creates a (&quot;next&quot;, &quot;history&quot;) field with the new prompt (from the previous `&quot;full&quot;` field)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">llm_output</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Integration with History</strong></p>
<p>When using <cite>input_mode=”history”</cite>, the wrapper integrates seamlessly with the <a class="reference internal" href="generated/torchrl.data.llm.History.html#torchrl.data.llm.History" title="torchrl.data.llm.History"><code class="xref py py-class docutils literal notranslate"><span class="pre">History</span></code></a> class:</p>
<ul class="simple">
<li><p><strong>Input</strong>: Takes a <code class="xref py py-class docutils literal notranslate"><span class="pre">ChatHistory</span></code> object containing a History in the <cite>prompt</cite> field</p></li>
<li><p><strong>Generation</strong>: Applies chat templates to convert History to tokens, generates response, then parses the full text back into a History object</p></li>
<li><p><strong>Output</strong>: Returns a ChatHistory with:
- <cite>prompt</cite>: Original conversation history
- <cite>response</cite>: New History object containing only the assistant’s response
- <cite>full</cite>: Complete conversation history with the new response appended</p></li>
</ul>
<p>This design allows for natural conversation flow where each generation step extends the conversation history, making it ideal for multi-turn
dialogue systems.</p>
<section id="prompt-vs-response-and-padding">
<h4>Prompt vs. Response and padding<a class="headerlink" href="#prompt-vs-response-and-padding" title="Permalink to this heading">¶</a></h4>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/llm-data.svg"><img alt="LLM output data format (Tokens, Masks, Padded vs. Sparse)" src="../_images/llm-data.svg" width="80%" /></a>
<figcaption>
<p><span class="caption-text">Structure of LLM outputs: padded vs. sparse representations for Tokens, LogProbs, and Masks.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The diagram above illustrates the structure of the main output classes used in TorchRL’s LLM API:</p>
<ul class="simple">
<li><p><strong>Tokens</strong> (and by extension, <strong>LogProbs</strong>):
- <em>Padded</em> format: All sequences in a batch are padded to the same length (with a special pad token), making them suitable for tensor operations. The prompt and response are concatenated to form <cite>tokens.full</cite>, and masks indicate valid vs. padded positions.
- <em>Sparse</em> format: Each sequence retains its original length (no padding), represented as lists of tensors. This is more memory-efficient for variable-length data.</p></li>
<li><p><strong>Masks</strong>: Two main masks are shown:
- <cite>mask.attention_mask_all</cite> marks valid (non-pad) tokens.
- <cite>mask.assistant_mask_all</cite> marks which tokens were generated by the assistant (useful for RLHF and SFT training).</p></li>
<li><p><strong>Text</strong>: Not shown in detail, as it is simply the decoded string representation of the prompt, response, or full sequence.</p></li>
</ul>
<p>This format ensures that all LLM outputs (Tokens, LogProbs, Masks, Text) are consistent and easy to manipulate, regardless of whether you use padded or sparse batching.</p>
<p>In general, we recommend working with unpadded data, as it is more memory-efficient and easier to manipulate.
For instance, when collecting multiple padded elements from the buffer, it may be hard to clearly understand how to re-pad them
to combine them in a cohesive batch. Working with unpadded data is more straightforward.</p>
</section>
</section>
</section>
<section id="modules">
<h2>Modules<a class="headerlink" href="#modules" title="Permalink to this heading">¶</a></h2>
<p>The LLM wrapper API provides unified interfaces for different LLM backends, ensuring consistent input/output formats across training and inference pipelines.</p>
<section id="wrappers">
<h3>Wrappers<a class="headerlink" href="#wrappers" title="Permalink to this heading">¶</a></h3>
<p>The main goal of these primitives is to:</p>
<ul class="simple">
<li><p>Unify the input/output data format across training and inference pipelines</p></li>
<li><p>Unify the input/output data format across backends (to be able to use different backends across losses and collectors)</p></li>
<li><p>Provide appropriate tooling to construct these objects in typical RL settings (resource allocation, async execution, weight update, etc.)</p></li>
</ul>
<table class="autosummary longtable docutils # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.LLMWrapperBase.html#torchrl.modules.llm.LLMWrapperBase" title="torchrl.modules.llm.LLMWrapperBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LLMWrapperBase</span></code></a>(*args, **kwargs)</p></td>
<td><p>A LLM wrapper base class.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.TransformersWrapper.html#torchrl.modules.llm.TransformersWrapper" title="torchrl.modules.llm.TransformersWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TransformersWrapper</span></code></a>(*args, **kwargs)</p></td>
<td><p>A wrapper class for Hugging Face Transformers models, providing a consistent interface for text generation and log probability computation.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.vLLMWrapper.html#torchrl.modules.llm.vLLMWrapper" title="torchrl.modules.llm.vLLMWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vLLMWrapper</span></code></a>(*args, **kwargs)</p></td>
<td><p>A wrapper class for vLLM models, providing a consistent interface for text generation and log probability computation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.RemoteTransformersWrapper.html#torchrl.modules.llm.RemoteTransformersWrapper" title="torchrl.modules.llm.RemoteTransformersWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RemoteTransformersWrapper</span></code></a>(model[, ...])</p></td>
<td><p>A remote Ray actor wrapper for TransformersWrapper that provides a simplified interface.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM" title="torchrl.modules.llm.AsyncVLLM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AsyncVLLM</span></code></a>(engine_args[, num_replicas, ...])</p></td>
<td><p>A service that manages multiple async vLLM engine actors for distributed inference.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.ChatHistory.html#torchrl.modules.llm.ChatHistory" title="torchrl.modules.llm.ChatHistory"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ChatHistory</span></code></a>([prompt, response, full, ...])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.Text.html#torchrl.modules.llm.Text" title="torchrl.modules.llm.Text"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Text</span></code></a>([prompt, response, full, device, names])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.LogProbs.html#torchrl.modules.llm.LogProbs" title="torchrl.modules.llm.LogProbs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LogProbs</span></code></a>([prompt, response, full, padded, ...])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.Masks.html#torchrl.modules.llm.Masks" title="torchrl.modules.llm.Masks"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Masks</span></code></a>([all_attention_mask, ...])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.Tokens.html#torchrl.modules.llm.Tokens" title="torchrl.modules.llm.Tokens"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tokens</span></code></a>([prompt, response, full, padded, ...])</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<section id="async-vllm-engine-recommended">
<h4>Async vLLM Engine (Recommended)<a class="headerlink" href="#async-vllm-engine-recommended" title="Permalink to this heading">¶</a></h4>
<p><a class="reference internal" href="generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM" title="torchrl.modules.llm.AsyncVLLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">AsyncVLLM</span></code></a> is the recommended approach for high-performance vLLM inference in TorchRL.
It provides a distributed, async-capable inference service built on Ray that offers superior performance and resource utilization compared to synchronous vLLM engines.</p>
<p><strong>Key Features:</strong></p>
<ul class="simple">
<li><p><strong>Distributed Architecture</strong>: Runs multiple vLLM engine replicas as Ray actors for horizontal scaling</p></li>
<li><p><strong>Load Balancing</strong>: Automatically distributes requests across available replicas</p></li>
<li><p><strong>Native vLLM Batching</strong>: Leverages vLLM’s optimized batching for maximum throughput.
Every thread or actor in your code will be able to make requests to the vLLM engine(s), put the query in
the queue and let the engine handle the batching.</p></li>
<li><p><strong>Resource Management</strong>: Automatic GPU allocation and cleanup through Ray placement groups</p></li>
<li><p><strong>Simple API</strong>: Single-import convenience with <a class="reference internal" href="generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM.from_pretrained" title="torchrl.modules.llm.AsyncVLLM.from_pretrained"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code></a></p></li>
</ul>
<p><strong>Basic Usage:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncVLLM</span><span class="p">,</span> <span class="n">vLLMWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SamplingParams</span>

<span class="c1"># Create async vLLM service (recommended)</span>
<span class="n">async_engine</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;Qwen/Qwen2.5-7B&quot;</span><span class="p">,</span>
    <span class="n">num_devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>       <span class="c1"># Use 2 GPUs per replica (tensor parallel)</span>
    <span class="n">num_replicas</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>      <span class="c1"># Create 2 replicas for higher throughput</span>
    <span class="n">max_model_len</span><span class="o">=</span><span class="mi">4096</span>
<span class="p">)</span>

<span class="c1"># Use with vLLMWrapper for TorchRL integration</span>
<span class="n">wrapper</span> <span class="o">=</span> <span class="n">vLLMWrapper</span><span class="p">(</span><span class="n">async_engine</span><span class="p">,</span> <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;history&quot;</span><span class="p">,</span> <span class="n">generate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Direct generation (also supported)</span>
<span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">async_engine</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Hello, world!&quot;</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>

<span class="c1"># Cleanup when done</span>
<span class="n">async_engine</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</pre></div>
</div>
<p>These objects (AsyncVLLM and vLLMWrapper) can be shared across multiple collectors, environments, or workers efficiently.
They can be directly passed from one worker to another: under the hood, Ray will handle the handler sharing and remote execution.</p>
<p><strong>Performance Benefits:</strong></p>
<ul class="simple">
<li><p><strong>Higher Throughput</strong>: Multiple replicas process requests concurrently</p></li>
<li><p><strong>Better GPU Utilization</strong>: Ray ensures optimal GPU allocation and co-location</p></li>
<li><p><strong>Reduced Latency</strong>: Native batching reduces per-request overhead</p></li>
<li><p><strong>Fault Tolerance</strong>: Ray provides automatic error recovery and resource management</p></li>
</ul>
<p><strong>Resource Sharing:</strong></p>
<p>AsyncVLLM instances can be shared across multiple collectors, environments, or workers efficiently:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncVLLM</span><span class="p">,</span> <span class="n">vLLMWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMCollector</span>

<span class="c1"># Create a shared AsyncVLLM service</span>
<span class="n">shared_async_engine</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;Qwen/Qwen2.5-7B&quot;</span><span class="p">,</span>
    <span class="n">num_devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_replicas</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>  <span class="c1"># High throughput for multiple consumers</span>
    <span class="n">max_model_len</span><span class="o">=</span><span class="mi">4096</span>
<span class="p">)</span>

<span class="c1"># Multiple wrappers can use the same AsyncVLLM service</span>
<span class="n">wrapper1</span> <span class="o">=</span> <span class="n">vLLMWrapper</span><span class="p">(</span><span class="n">shared_async_engine</span><span class="p">,</span> <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;history&quot;</span><span class="p">)</span>
<span class="n">wrapper2</span> <span class="o">=</span> <span class="n">vLLMWrapper</span><span class="p">(</span><span class="n">shared_async_engine</span><span class="p">,</span> <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">)</span>

<span class="c1"># Multiple collectors can share the same service</span>
<span class="n">collector1</span> <span class="o">=</span> <span class="n">LLMCollector</span><span class="p">(</span><span class="n">env1</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">wrapper1</span><span class="p">)</span>
<span class="n">collector2</span> <span class="o">=</span> <span class="n">LLMCollector</span><span class="p">(</span><span class="n">env2</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">wrapper2</span><span class="p">)</span>

<span class="c1"># The AsyncVLLM service automatically load-balances across replicas</span>
<span class="c1"># No additional coordination needed between consumers</span>
</pre></div>
</div>
<p>This approach is more efficient than creating separate vLLM instances for each consumer, as it:</p>
<ul class="simple">
<li><p><strong>Reduces Memory Usage</strong>: Single model loading shared across consumers</p></li>
<li><p><strong>Automatic Load Balancing</strong>: Requests are distributed across replicas</p></li>
<li><p><strong>Better Resource Utilization</strong>: GPUs are used more efficiently</p></li>
<li><p><strong>Simplified Management</strong>: Single service to monitor and manage</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>AsyncVLLM vs. Traditional Actor Sharing</strong></p>
<p>Unlike traditional Ray actor sharing patterns where you manually create named actors and share references,
AsyncVLLM handles the distributed architecture internally. You simply create one AsyncVLLM service and
pass it to multiple consumers. The service automatically:</p>
<ul class="simple">
<li><p>Creates and manages multiple Ray actors (replicas) internally</p></li>
<li><p>Load-balances requests across replicas without manual coordination</p></li>
<li><p>Handles actor lifecycle and resource cleanup</p></li>
</ul>
<p>This eliminates the need for manual actor name management and reference sharing that was required
with the previous <cite>RemotevLLMWrapper</cite> approach.</p>
</div>
</section>
<section id="remote-wrappers">
<h4>Remote Wrappers<a class="headerlink" href="#remote-wrappers" title="Permalink to this heading">¶</a></h4>
<p>TorchRL provides remote wrapper classes that enable distributed execution of LLM wrappers using Ray. These wrappers provide a simplified interface that doesn’t require explicit <cite>remote()</cite> and <cite>get()</cite> calls, making them easy to use in distributed settings.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>For vLLM: Use AsyncVLLM Instead</strong></p>
<p>For vLLM-based inference, we recommend using <a class="reference internal" href="generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM" title="torchrl.modules.llm.AsyncVLLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">AsyncVLLM</span></code></a> directly rather than
remote wrappers. AsyncVLLM provides better performance, resource utilization, and built-in load balancing.
See the <a class="reference internal" href="#async-vllm-engine-recommended">Async vLLM Engine (Recommended)</a> section above for details.</p>
<p>Remote wrappers are primarily intended for Transformers-based models or other use cases where AsyncVLLM
is not applicable.</p>
</div>
<p><strong>Key Features:</strong></p>
<ul class="simple">
<li><p><strong>Simplified Interface</strong>: No need to call <cite>remote()</cite> and <cite>get()</cite> explicitly</p></li>
<li><p><strong>Full API Compatibility</strong>: Exposes all public methods from the base <cite>LLMWrapperBase</cite> class</p></li>
<li><p><strong>Automatic Ray Management</strong>: Handles Ray initialization and remote execution internally</p></li>
<li><p><strong>Property Access</strong>: All properties are accessible through the remote wrapper</p></li>
<li><p><strong>Error Handling</strong>: Proper error propagation from remote actors</p></li>
<li><p><strong>Resource Management</strong>: Context manager support for automatic cleanup</p></li>
</ul>
<p><strong>Model Parameter Requirements:</strong></p>
<ul class="simple">
<li><p><strong>RemoteTransformersWrapper</strong>: Only accepts string model names/paths. Transformers models are not serializable.</p></li>
</ul>
<p><strong>Supported Backends:</strong></p>
<p>Currently, only Transformers-based models are supported through remote wrappers. For vLLM models, use <a class="reference internal" href="generated/torchrl.modules.llm.AsyncVLLM.html#torchrl.modules.llm.AsyncVLLM" title="torchrl.modules.llm.AsyncVLLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">AsyncVLLM</span></code></a> instead.</p>
<p><strong>Usage Examples:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">ray</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules.llm.policies</span><span class="w"> </span><span class="kn">import</span> <span class="n">RemoteTransformersWrapper</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.data.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">History</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules.llm.policies</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatHistory</span><span class="p">,</span> <span class="n">Text</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDict</span>

<span class="c1"># Initialize Ray (if not already done)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">ray</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
    <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="c1"># Transformers wrapper (only string models supported)</span>
<span class="c1"># The remote wrappers implement context managers for proper resource cleanup:</span>
<span class="k">with</span> <span class="n">RemoteTransformersWrapper</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">,</span>
    <span class="n">max_concurrency</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
    <span class="n">generate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">generate_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">}</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">remote_transformers</span><span class="p">:</span>

    <span class="n">text_input</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">({</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">Text</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Hello world&quot;</span><span class="p">)},</span> <span class="n">batch_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">remote_transformers</span><span class="p">(</span><span class="n">text_input</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Performance Considerations:</strong></p>
<ul class="simple">
<li><p><strong>Network Overhead</strong>: Remote execution adds network communication overhead</p></li>
<li><p><strong>Serialization</strong>: Data is serialized when sent to remote actors</p></li>
<li><p><strong>Memory</strong>: Each remote actor maintains its own copy of the model</p></li>
<li><p><strong>Concurrency</strong>: Multiple remote wrappers can run concurrently</p></li>
<li><p><strong>Max Concurrency</strong>: Use the <cite>max_concurrency</cite> parameter to control the number of concurrent calls to each remote actor</p></li>
<li><p><strong>Cleanup</strong>: Always use context managers or call <cite>cleanup_batching()</cite> to prevent hanging due to batching locks</p></li>
</ul>
</section>
<section id="utils">
<h4>Utils<a class="headerlink" href="#utils" title="Permalink to this heading">¶</a></h4>
<table class="autosummary longtable docutils # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.make_async_vllm_engine.html#torchrl.modules.llm.make_async_vllm_engine" title="torchrl.modules.llm.make_async_vllm_engine"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_async_vllm_engine</span></code></a>(*, model_name[, ...])</p></td>
<td><p>Create an async vLLM engine service.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.stateless_init_process_group_async.html#torchrl.modules.llm.stateless_init_process_group_async" title="torchrl.modules.llm.stateless_init_process_group_async"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stateless_init_process_group_async</span></code></a>(...)</p></td>
<td><p>Initializes a stateless process group for distributed communication (async version).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.make_vllm_worker.html#torchrl.modules.llm.make_vllm_worker" title="torchrl.modules.llm.make_vllm_worker"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_vllm_worker</span></code></a>(*, model_name[, devices, ...])</p></td>
<td><p>Creates a vLLM inference engine with tensor parallelism support.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.modules.llm.stateless_init_process_group.html#torchrl.modules.llm.stateless_init_process_group" title="torchrl.modules.llm.stateless_init_process_group"><code class="xref py py-obj docutils literal notranslate"><span class="pre">stateless_init_process_group</span></code></a>(master_address, ...)</p></td>
<td><p>Initializes a stateless process group for distributed communication.</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="collectors">
<h2>Collectors<a class="headerlink" href="#collectors" title="Permalink to this heading">¶</a></h2>
<p id="id1">TorchRL offers specialized collector classes (<a class="reference internal" href="generated/torchrl.collectors.llm.LLMCollector.html#torchrl.collectors.llm.LLMCollector" title="torchrl.collectors.llm.LLMCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">LLMCollector</span></code></a> and <a class="reference internal" href="generated/torchrl.collectors.llm.RayLLMCollector.html#torchrl.collectors.llm.RayLLMCollector" title="torchrl.collectors.llm.RayLLMCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">RayLLMCollector</span></code></a>)
that are tailored for LLM use cases. We also provide weight synchronization schemes for vLLM inference engines.</p>
<p>See <span class="xref std std-ref">ref_collectors</span> for more details on the collector API. In brief, the idea of a collector is to isolate the inference part of the pipeline
in a dedicated class.
A collector usually takes as input a policy and an environment, and alternate between running one and the other.
In “classical” settings, the policy is similar to the policy being trained (with some optional extra-exploration). In the context of LLM fine-tuning,
the policy will usually be a specialized inference engine, such as a vLLM server.
Collectors are defined by the following parameters and features:</p>
<ul class="simple">
<li><p><strong>Sync/Async</strong>: Whether the collector should run in sync or async mode.
In sync mode, the collector will run the inference step in alternate with the optimization/training step.
In async mode, the collector will run the inference step in parallel with the optimization/training step.
A replay buffer can be passed to the collector, in such a way that the collector can directly write to it.
In other cases, the collector can be iterated over to collect data.</p></li>
<li><p><strong>Steps</strong>: A collector is built with a certain number of steps budget, as well as a number of steps to be
included in each batch yield during collection.</p></li>
<li><p><strong>Weight Synchronization Schemes</strong>: Weight sync schemes handle the synchronization of weights between the training model
and the inference engine. The new scheme-based approach provides flexible, high-performance weight updates for vLLM and
other inference backends.</p></li>
</ul>
<section id="vllm-weight-synchronization-schemes">
<h3>vLLM Weight Synchronization Schemes<a class="headerlink" href="#vllm-weight-synchronization-schemes" title="Permalink to this heading">¶</a></h3>
<p>TorchRL provides two weight synchronization schemes for vLLM engines, offering different trade-offs between
performance and simplicity:</p>
<p><strong>1. NCCL-Based Synchronization</strong> (<a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMWeightSyncScheme.html#torchrl.weight_update.llm.VLLMWeightSyncScheme" title="torchrl.weight_update.llm.VLLMWeightSyncScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">VLLMWeightSyncScheme</span></code></a>)</p>
<p>Uses NCCL collectives for high-bandwidth GPU-to-GPU weight transfers. Best for:</p>
<ul class="simple">
<li><p>High-frequency weight updates</p></li>
<li><p>Large models where transfer speed is critical</p></li>
<li><p>Setups with GPU interconnect (NVLink, InfiniBand)</p></li>
</ul>
<p><strong>2. Double-Buffer Synchronization</strong> (<a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMDoubleBufferSyncScheme.html#torchrl.weight_update.llm.VLLMDoubleBufferSyncScheme" title="torchrl.weight_update.llm.VLLMDoubleBufferSyncScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">VLLMDoubleBufferSyncScheme</span></code></a>)</p>
<p>Uses memory-mapped file storage for asynchronous weight transfers. Best for:</p>
<ul class="simple">
<li><p>Simpler setup without NCCL coordination</p></li>
<li><p>Distributed setups with shared filesystems (NFS)</p></li>
<li><p>Cases where update frequency is lower</p></li>
</ul>
<p><strong>Usage Example with NCCL:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">RayLLMCollector</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.weight_update.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">VLLMWeightSyncScheme</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncVLLM</span><span class="p">,</span> <span class="n">vLLMWrapper</span>

<span class="c1"># Create vLLM engine</span>
<span class="n">vllm_engine</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;Qwen/Qwen2.5-7B&quot;</span><span class="p">,</span>
    <span class="n">num_devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_replicas</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">vLLMWrapper</span><span class="p">(</span><span class="n">vllm_engine</span><span class="p">,</span> <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;history&quot;</span><span class="p">)</span>

<span class="c1"># Create NCCL weight sync scheme</span>
<span class="n">weight_sync_scheme</span> <span class="o">=</span> <span class="n">VLLMWeightSyncScheme</span><span class="p">(</span>
    <span class="n">master_address</span><span class="o">=</span><span class="s2">&quot;localhost&quot;</span><span class="p">,</span>
    <span class="n">master_port</span><span class="o">=</span><span class="mi">29500</span><span class="p">,</span>
    <span class="n">gpus_per_replica</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># tp_size × dp_size × pp_size</span>
    <span class="n">num_replicas</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;state_dict&quot;</span>
<span class="p">)</span>

<span class="c1"># Create collector with weight sync scheme</span>
<span class="n">collector</span> <span class="o">=</span> <span class="n">RayLLMCollector</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="n">make_env</span><span class="p">,</span>
    <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
    <span class="n">dialog_turns_per_batch</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">total_dialog_turns</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">weight_sync_schemes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;policy&quot;</span><span class="p">:</span> <span class="n">weight_sync_scheme</span><span class="p">},</span>
    <span class="n">track_policy_version</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># During training, get the sender and update weights</span>
<span class="n">sender</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">_weight_senders</span><span class="p">[</span><span class="s2">&quot;policy&quot;</span><span class="p">]</span>
<span class="n">sender</span><span class="o">.</span><span class="n">register_model</span><span class="p">(</span><span class="n">training_model</span><span class="p">)</span>

<span class="c1"># Initialize collective group (must be called before first update)</span>
<span class="n">metadata</span> <span class="o">=</span> <span class="n">get_model_metadata</span><span class="p">(</span><span class="n">training_model</span><span class="p">)</span>
<span class="n">sender</span><span class="o">.</span><span class="n">init_all_workers_group</span><span class="p">(</span><span class="n">metadata</span><span class="p">,</span> <span class="n">vllm_engine</span><span class="o">=</span><span class="n">vllm_engine</span><span class="p">)</span>

<span class="c1"># Update weights during training</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">collector</span><span class="p">):</span>
    <span class="c1"># ... training step ...</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">sender</span><span class="o">.</span><span class="n">update_weights</span><span class="p">()</span>  <span class="c1"># Broadcasts via NCCL</span>
</pre></div>
</div>
<p><strong>Usage Example with Double-Buffer:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">RayLLMCollector</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.weight_update.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">VLLMDoubleBufferSyncScheme</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.modules.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncVLLM</span><span class="p">,</span> <span class="n">vLLMWrapper</span>

<span class="c1"># Create vLLM engine</span>
<span class="n">vllm_engine</span> <span class="o">=</span> <span class="n">AsyncVLLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;Qwen/Qwen2.5-7B&quot;</span><span class="p">,</span>
    <span class="n">num_devices</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">num_replicas</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">vLLMWrapper</span><span class="p">(</span><span class="n">vllm_engine</span><span class="p">,</span> <span class="n">input_mode</span><span class="o">=</span><span class="s2">&quot;history&quot;</span><span class="p">)</span>

<span class="c1"># Create double-buffer weight sync scheme</span>
<span class="n">weight_sync_scheme</span> <span class="o">=</span> <span class="n">VLLMDoubleBufferSyncScheme</span><span class="p">(</span>
    <span class="n">remote_addr</span><span class="o">=</span><span class="s2">&quot;/tmp/weights&quot;</span><span class="p">,</span>  <span class="c1"># Or &quot;/mnt/shared/weights&quot; for NFS</span>
    <span class="n">num_threads</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;state_dict&quot;</span>
<span class="p">)</span>

<span class="c1"># Create collector with weight sync scheme</span>
<span class="n">collector</span> <span class="o">=</span> <span class="n">RayLLMCollector</span><span class="p">(</span>
    <span class="n">env</span><span class="o">=</span><span class="n">make_env</span><span class="p">,</span>
    <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
    <span class="n">dialog_turns_per_batch</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">total_dialog_turns</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">weight_sync_schemes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;policy&quot;</span><span class="p">:</span> <span class="n">weight_sync_scheme</span><span class="p">},</span>
    <span class="n">track_policy_version</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># During training, get the sender and receiver</span>
<span class="n">sender</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">_weight_senders</span><span class="p">[</span><span class="s2">&quot;policy&quot;</span><span class="p">]</span>
<span class="n">sender</span><span class="o">.</span><span class="n">register_model</span><span class="p">(</span><span class="n">training_model</span><span class="p">)</span>

<span class="c1"># No initialization needed for double-buffer scheme!</span>

<span class="c1"># Update weights during training</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">collector</span><span class="p">):</span>
    <span class="c1"># ... training step ...</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">sender</span><span class="o">.</span><span class="n">update_weights</span><span class="p">()</span>  <span class="c1"># Writes to shared storage</span>
        <span class="c1"># vLLM workers can poll and apply: receiver.poll_and_apply()</span>
</pre></div>
</div>
</section>
<section id="policy-version-tracking">
<h3>Policy Version Tracking<a class="headerlink" href="#policy-version-tracking" title="Permalink to this heading">¶</a></h3>
<p>LLM Collectors also allow to track the version of the policy, which is useful for some use cases.
This is done by adding a <a class="reference internal" href="generated/torchrl.envs.llm.transforms.PolicyVersion.html#torchrl.envs.llm.transforms.PolicyVersion" title="torchrl.envs.llm.transforms.PolicyVersion"><code class="xref py py-class docutils literal notranslate"><span class="pre">PolicyVersion</span></code></a> transform to the environment, which is
then incremented by the collector after each weight update. To do this, one either provides the stateful version of the
transform, or a boolean to the collector constructor.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.llm.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolicyVersion</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.collectors.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMCollector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.weight_update.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">VLLMWeightSyncScheme</span><span class="p">,</span> <span class="n">get_model_metadata</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">()</span> <span class="c1"># place your code here</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">policy</span> <span class="o">=</span> <span class="n">make_policy</span><span class="p">()</span> <span class="c1"># place your code here</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scheme</span> <span class="o">=</span> <span class="n">VLLMWeightSyncScheme</span><span class="p">(</span><span class="n">master_port</span><span class="o">=</span><span class="mi">29500</span><span class="p">,</span> <span class="n">gpus_per_replica</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_replicas</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">collector</span> <span class="o">=</span> <span class="n">LLMCollector</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span> <span class="n">weight_sync_schemes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;policy&quot;</span><span class="p">:</span> <span class="n">scheme</span><span class="p">},</span> <span class="n">track_policy_version</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Get the sender and register model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sender</span> <span class="o">=</span> <span class="n">collector</span><span class="o">.</span><span class="n">_weight_senders</span><span class="p">[</span><span class="s2">&quot;policy&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sender</span><span class="o">.</span><span class="n">register_model</span><span class="p">(</span><span class="n">training_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Initialize the collective group</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metadata</span> <span class="o">=</span> <span class="n">get_model_metadata</span><span class="p">(</span><span class="n">training_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sender</span><span class="o">.</span><span class="n">init_all_workers_group</span><span class="p">(</span><span class="n">metadata</span><span class="p">,</span> <span class="n">vllm_engine</span><span class="o">=</span><span class="n">policy</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Update weights</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sender</span><span class="o">.</span><span class="n">update_weights</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">collector</span><span class="o">.</span><span class="n">policy_version_tracker</span><span class="o">.</span><span class="n">version</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the policy version is written in the data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">collector</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;policy_version&quot;</span><span class="p">])</span>
</pre></div>
</div>
<table class="autosummary longtable docutils # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMWeightSyncScheme.html#torchrl.weight_update.llm.VLLMWeightSyncScheme" title="torchrl.weight_update.llm.VLLMWeightSyncScheme"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VLLMWeightSyncScheme</span></code></a>([master_address, ...])</p></td>
<td><p>Weight synchronization scheme for vLLM engines.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMWeightSender.html#torchrl.weight_update.llm.VLLMWeightSender" title="torchrl.weight_update.llm.VLLMWeightSender"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VLLMWeightSender</span></code></a>(scheme)</p></td>
<td><p>Sends weights to vLLM workers using collective communication.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMWeightReceiver.html#torchrl.weight_update.llm.VLLMWeightReceiver" title="torchrl.weight_update.llm.VLLMWeightReceiver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VLLMWeightReceiver</span></code></a>(scheme, vllm_engine)</p></td>
<td><p>Receives weights in a vLLM worker using collective communication.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMCollectiveTransport.html#torchrl.weight_update.llm.VLLMCollectiveTransport" title="torchrl.weight_update.llm.VLLMCollectiveTransport"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VLLMCollectiveTransport</span></code></a>(master_address, ...)</p></td>
<td><p>Transport for vLLM using collective communication (NCCL).</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMDoubleBufferSyncScheme.html#torchrl.weight_update.llm.VLLMDoubleBufferSyncScheme" title="torchrl.weight_update.llm.VLLMDoubleBufferSyncScheme"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VLLMDoubleBufferSyncScheme</span></code></a>(remote_addr[, ...])</p></td>
<td><p>Weight synchronization scheme for vLLM using double-buffered storage.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMDoubleBufferWeightSender.html#torchrl.weight_update.llm.VLLMDoubleBufferWeightSender" title="torchrl.weight_update.llm.VLLMDoubleBufferWeightSender"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VLLMDoubleBufferWeightSender</span></code></a>(scheme)</p></td>
<td><p>Sends weights to vLLM workers using double-buffered storage.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMDoubleBufferWeightReceiver.html#torchrl.weight_update.llm.VLLMDoubleBufferWeightReceiver" title="torchrl.weight_update.llm.VLLMDoubleBufferWeightReceiver"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VLLMDoubleBufferWeightReceiver</span></code></a>(scheme, ...)</p></td>
<td><p>Receives weights in a vLLM worker using double-buffered storage.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMDoubleBufferTransport.html#torchrl.weight_update.llm.VLLMDoubleBufferTransport" title="torchrl.weight_update.llm.VLLMDoubleBufferTransport"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VLLMDoubleBufferTransport</span></code></a>(remote_addr[, ...])</p></td>
<td><p>Transport for vLLM using double-buffered memory-mapped storage.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.weight_update.llm.get_model_metadata.html#torchrl.weight_update.llm.get_model_metadata" title="torchrl.weight_update.llm.get_model_metadata"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_model_metadata</span></code></a>(model)</p></td>
<td><p>Extract model metadata from a model.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="legacy-weight-updaters-deprecated">
<h3>Legacy Weight Updaters (Deprecated)<a class="headerlink" href="#legacy-weight-updaters-deprecated" title="Permalink to this heading">¶</a></h3>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.11: </span>The <cite>vLLMUpdater</cite> and <cite>vLLMUpdaterV2</cite> classes are deprecated in favor of the new weight synchronization schemes
(<a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMWeightSyncScheme.html#torchrl.weight_update.llm.VLLMWeightSyncScheme" title="torchrl.weight_update.llm.VLLMWeightSyncScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">VLLMWeightSyncScheme</span></code></a> and <a class="reference internal" href="generated/torchrl.weight_update.llm.VLLMDoubleBufferSyncScheme.html#torchrl.weight_update.llm.VLLMDoubleBufferSyncScheme" title="torchrl.weight_update.llm.VLLMDoubleBufferSyncScheme"><code class="xref py py-class docutils literal notranslate"><span class="pre">VLLMDoubleBufferSyncScheme</span></code></a>).
These schemes provide better performance, more flexibility, and cleaner integration with collectors.
The legacy updaters will be removed in a future release.</p>
<p>The legacy weight updaters (<cite>vLLMUpdater</cite> and <cite>vLLMUpdaterV2</cite>) are still available but are no longer recommended.
Please migrate to the new weight synchronization schemes shown above.</p>
</div>
<table class="autosummary longtable docutils # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.collectors.llm.vLLMUpdater.html#torchrl.collectors.llm.vLLMUpdater" title="torchrl.collectors.llm.vLLMUpdater"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vLLMUpdater</span></code></a>(*args[, v2])</p></td>
<td><p>A class that sends weights to vLLM workers.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.collectors.llm.vLLMUpdaterV2.html#torchrl.collectors.llm.vLLMUpdaterV2" title="torchrl.collectors.llm.vLLMUpdaterV2"><code class="xref py py-obj docutils literal notranslate"><span class="pre">vLLMUpdaterV2</span></code></a>(vllm_engine)</p></td>
<td><p>Simplified vLLM weight updater using the RLvLLMEngine interface.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.collectors.llm.LLMCollector.html#torchrl.collectors.llm.LLMCollector" title="torchrl.collectors.llm.LLMCollector"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LLMCollector</span></code></a>(env, *[, policy, ...])</p></td>
<td><p>A simplified version of SyncDataCollector for LLM inference.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.collectors.llm.RayLLMCollector.html#torchrl.collectors.llm.RayLLMCollector" title="torchrl.collectors.llm.RayLLMCollector"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RayLLMCollector</span></code></a>(env, *[, policy, ...])</p></td>
<td><p>A lightweight Ray implementation of the LLM Collector that can be extended and sampled remotely.</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="environments">
<h2>Environments<a class="headerlink" href="#environments" title="Permalink to this heading">¶</a></h2>
<p>The environment layer orchestrates data loading, tool execution, reward computation, and formatting. When fine-tuning an LLM using TorchRL, the environment is a
crucial component of the inference pipeline, alongside the policy and collector.</p>
<section id="chatenv">
<h3>ChatEnv<a class="headerlink" href="#chatenv" title="Permalink to this heading">¶</a></h3>
<p><a class="reference internal" href="generated/torchrl.envs.llm.ChatEnv.html#torchrl.envs.llm.ChatEnv" title="torchrl.envs.llm.ChatEnv"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatEnv</span></code></a> serves as a blank canvas for LLM environments - it’s a basic tool designed to be extended with transforms that add
specific functionality. The base ChatEnv provides the fundamental structure for managing conversation state using the
<a class="reference internal" href="generated/torchrl.data.llm.History.html#torchrl.data.llm.History" title="torchrl.data.llm.History"><code class="xref py py-class docutils literal notranslate"><span class="pre">History</span></code></a> format, but it’s intentionally minimal to allow maximum flexibility.</p>
<section id="core-functionality">
<h4>Core Functionality<a class="headerlink" href="#core-functionality" title="Permalink to this heading">¶</a></h4>
<p>ChatEnv operates in three main modes:
- <strong>History mode</strong>: Uses <a class="reference internal" href="generated/torchrl.data.llm.History.html#torchrl.data.llm.History" title="torchrl.data.llm.History"><code class="xref py py-class docutils literal notranslate"><span class="pre">History</span></code></a> objects for conversation management
- <strong>Text mode</strong>: Uses simple text strings for input/output
- <strong>Tokens mode</strong>: Uses tokenized data for input/output</p>
<p>The environment maintains conversation state by:
- <strong>Reset</strong>: Initializes a new conversation with an optional system prompt
- <strong>Step</strong>: Takes the LLM’s response and updates the conversation history, preparing the next prompt</p>
</section>
<section id="transform-based-architecture">
<h4>Transform-Based Architecture<a class="headerlink" href="#transform-based-architecture" title="Permalink to this heading">¶</a></h4>
<p>Transforms are the main way to extend ChatEnv with specific capabilities:</p>
<ul class="simple">
<li><p><strong>Reward computation</strong>: <a class="reference internal" href="generated/torchrl.envs.llm.transforms.KLRewardTransform.html#torchrl.envs.llm.transforms.KLRewardTransform" title="torchrl.envs.llm.transforms.KLRewardTransform"><code class="xref py py-class docutils literal notranslate"><span class="pre">KLRewardTransform</span></code></a> for KL divergence rewards</p></li>
<li><p><strong>Tool execution</strong>: <a class="reference internal" href="generated/torchrl.envs.llm.transforms.PythonInterpreter.html#torchrl.envs.llm.transforms.PythonInterpreter" title="torchrl.envs.llm.transforms.PythonInterpreter"><code class="xref py py-class docutils literal notranslate"><span class="pre">PythonInterpreter</span></code></a> for Python code
execution, <a class="reference internal" href="generated/torchrl.envs.llm.transforms.MCPToolTransform.html#torchrl.envs.llm.transforms.MCPToolTransform" title="torchrl.envs.llm.transforms.MCPToolTransform"><code class="xref py py-class docutils literal notranslate"><span class="pre">MCPToolTransform</span></code></a> for general tool calling.</p></li>
<li><p><strong>Data loading</strong>: <a class="reference internal" href="generated/torchrl.envs.llm.transforms.DataLoadingPrimer.html#torchrl.envs.llm.transforms.DataLoadingPrimer" title="torchrl.envs.llm.transforms.DataLoadingPrimer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoadingPrimer</span></code></a> for loading prompts from datasets</p></li>
<li><p><strong>Thinking prompts</strong>: <a class="reference internal" href="generated/torchrl.envs.llm.transforms.AddThinkingPrompt.html#torchrl.envs.llm.transforms.AddThinkingPrompt" title="torchrl.envs.llm.transforms.AddThinkingPrompt"><code class="xref py py-class docutils literal notranslate"><span class="pre">AddThinkingPrompt</span></code></a> for chain-of-thought reasoning</p></li>
<li><p><strong>Policy tracking</strong>: <a class="reference internal" href="generated/torchrl.envs.llm.transforms.PolicyVersion.html#torchrl.envs.llm.transforms.PolicyVersion" title="torchrl.envs.llm.transforms.PolicyVersion"><code class="xref py py-class docutils literal notranslate"><span class="pre">PolicyVersion</span></code></a> for version control</p></li>
<li><p><strong>Step counting</strong>: Built-in step tracking and reset management using <a class="reference internal" href="generated/torchrl.envs.transforms.StepCounter.html#torchrl.envs.transforms.StepCounter" title="torchrl.envs.transforms.StepCounter"><code class="xref py py-class docutils literal notranslate"><span class="pre">StepCounter</span></code></a>.</p></li>
</ul>
</section>
<section id="integration-with-llm-wrappers">
<h4>Integration with LLM Wrappers<a class="headerlink" href="#integration-with-llm-wrappers" title="Permalink to this heading">¶</a></h4>
<p id="ref-env-llm-step">ChatEnv is designed to work seamlessly with both <a class="reference internal" href="generated/torchrl.modules.llm.TransformersWrapper.html#torchrl.modules.llm.TransformersWrapper" title="torchrl.modules.llm.TransformersWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">TransformersWrapper</span></code></a> and <a class="reference internal" href="generated/torchrl.modules.llm.vLLMWrapper.html#torchrl.modules.llm.vLLMWrapper" title="torchrl.modules.llm.vLLMWrapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">vLLMWrapper</span></code></a>.
The environment handles the conversation state management while the wrapper handles the actual LLM inference, creating a clean separation of concerns.</p>
<p>On each call to <cite>step</cite>, the environment:</p>
<ul class="simple">
<li><p>Takes the LLM’s output, specifically the <cite>full</cite> field, which contains the entire conversation so far, including the new response (e.g., <cite>history.full</cite>, <cite>text.full</cite>, <cite>tokens.full</cite>).</p></li>
<li><p>Sets this <cite>full</cite> field as the new <cite>prompt</cite> for the next LLM step (e.g., <cite>td[“next”, “history”].prompt</cite>, <cite>td[“next”, “text”].prompt</cite>, <cite>td[“next”, “tokens”].prompt</cite>).</p></li>
<li><p>Optionally, applies transforms to insert new user messages, tool calls, or other modifications to the conversation before the next LLM step to refine the prompt.</p></li>
</ul>
<p>This mechanism enables seamless multi-turn interactions and supports complex workflows such as tool use and reward shaping.</p>
</section>
</section>
<section id="task-specific-environments">
<h3>Task-Specific Environments<a class="headerlink" href="#task-specific-environments" title="Permalink to this heading">¶</a></h3>
<p>We provide a few task-specific environments, such as <a class="reference internal" href="generated/torchrl.envs.llm.GSM8KEnv.html#torchrl.envs.llm.GSM8KEnv" title="torchrl.envs.llm.GSM8KEnv"><code class="xref py py-class docutils literal notranslate"><span class="pre">GSM8KEnv</span></code></a> for the GSM8K dataset,
<a class="reference internal" href="generated/torchrl.envs.llm.IFEvalEnv.html#torchrl.envs.llm.IFEvalEnv" title="torchrl.envs.llm.IFEvalEnv"><code class="xref py py-class docutils literal notranslate"><span class="pre">IFEvalEnv</span></code></a> for the IFEval dataset, and <code class="xref py py-class docutils literal notranslate"><span class="pre">MLGymEnv</span></code> for MLGym integration.</p>
<p>These environments wrap a <a class="reference internal" href="generated/torchrl.envs.llm.ChatEnv.html#torchrl.envs.llm.ChatEnv" title="torchrl.envs.llm.ChatEnv"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatEnv</span></code></a> and add a <a class="reference internal" href="generated/torchrl.envs.llm.transforms.DataLoadingPrimer.html#torchrl.envs.llm.transforms.DataLoadingPrimer" title="torchrl.envs.llm.transforms.DataLoadingPrimer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoadingPrimer</span></code></a> transform
(plus an optional reward parsing transform) in a <code class="xref py py-class docutils literal notranslate"><span class="pre">TransformedEnv</span></code> class.</p>
<table class="autosummary longtable docutils # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.ChatEnv.html#torchrl.envs.llm.ChatEnv" title="torchrl.envs.llm.ChatEnv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ChatEnv</span></code></a>(*args, **kwargs)</p></td>
<td><p>A chat-based environment for LLMs, designed as a blank canvas for conversation and RL.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.DatasetChatEnv.html#torchrl.envs.llm.DatasetChatEnv" title="torchrl.envs.llm.DatasetChatEnv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DatasetChatEnv</span></code></a>(*args, **kwargs)</p></td>
<td><p>Base class for chat environment with queries pulled from a dataset.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.GSM8KEnv.html#torchrl.envs.llm.GSM8KEnv" title="torchrl.envs.llm.GSM8KEnv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GSM8KEnv</span></code></a>(*args, **kwargs)</p></td>
<td><p>GSM8K dataset environment.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.make_gsm8k_env.html#torchrl.envs.llm.make_gsm8k_env" title="torchrl.envs.llm.make_gsm8k_env"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_gsm8k_env</span></code></a>([dataset, num_envs, repeats, ...])</p></td>
<td><p>A builder for an LLMEnv-based GSM8K environment.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.GSM8KPrepareQuestion.html#torchrl.envs.llm.GSM8KPrepareQuestion" title="torchrl.envs.llm.GSM8KPrepareQuestion"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GSM8KPrepareQuestion</span></code></a>([in_keys, out_keys])</p></td>
<td><p>A transform to prepare the prompt when using GSM8k within an LLMEnv.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.IFEvalEnv.html#torchrl.envs.llm.IFEvalEnv" title="torchrl.envs.llm.IFEvalEnv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IFEvalEnv</span></code></a>(*args, **kwargs)</p></td>
<td><p>A chat environment based on the IFEval dataset.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.IfEvalScorer.html#torchrl.envs.llm.IfEvalScorer" title="torchrl.envs.llm.IfEvalScorer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IfEvalScorer</span></code></a>(*[, instruction_ids_key, ...])</p></td>
<td><p>Scorer for the IF-Eval task.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.IFEvalScoreData.html#torchrl.envs.llm.IFEvalScoreData" title="torchrl.envs.llm.IFEvalScoreData"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IFEvalScoreData</span></code></a>(prompt_level_strict_acc, ...)</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.LLMEnv.html#torchrl.envs.llm.LLMEnv" title="torchrl.envs.llm.LLMEnv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LLMEnv</span></code></a>(*args, **kwargs)</p></td>
<td><p>A text generation environment for language models.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.LLMHashingEnv.html#torchrl.envs.llm.LLMHashingEnv" title="torchrl.envs.llm.LLMHashingEnv"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LLMHashingEnv</span></code></a>(*args, **kwargs)</p></td>
<td><p>A text generation environment that uses a hashing module to identify unique observations.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.make_mlgym.html#torchrl.envs.llm.make_mlgym" title="torchrl.envs.llm.make_mlgym"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_mlgym</span></code></a>(*[, task, tasks, tokenizer, ...])</p></td>
<td><p>Wraps an MLGymEnv in a TorchRL Environment.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.MLGymWrapper.html#torchrl.envs.llm.MLGymWrapper" title="torchrl.envs.llm.MLGymWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MLGymWrapper</span></code></a>(*args, **kwargs)</p></td>
<td><p>A thin wrapper for MLGym environments.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.GSM8KRewardParser.html#torchrl.envs.llm.GSM8KRewardParser" title="torchrl.envs.llm.GSM8KRewardParser"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GSM8KRewardParser</span></code></a>(tokenizer[, in_keys, ...])</p></td>
<td><p>Reward parser for GSM8KEnv or make_gsm8k_env.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="transforms">
<h3>Transforms<a class="headerlink" href="#transforms" title="Permalink to this heading">¶</a></h3>
<p>Transforms are used to modify the data before it is passed to the LLM.
Tools are usually implemented as transforms, and appended to a base environment
such as <a class="reference internal" href="generated/torchrl.envs.llm.ChatEnv.html#torchrl.envs.llm.ChatEnv" title="torchrl.envs.llm.ChatEnv"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatEnv</span></code></a>.</p>
<p>An example of a tool transform is the <a class="reference internal" href="generated/torchrl.envs.llm.transforms.PythonInterpreter.html#torchrl.envs.llm.transforms.PythonInterpreter" title="torchrl.envs.llm.transforms.PythonInterpreter"><code class="xref py py-class docutils literal notranslate"><span class="pre">PythonInterpreter</span></code></a> transform, which is used
to execute Python code in the context of the LLM.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.llm.transforms</span><span class="w"> </span><span class="kn">import</span> <span class="n">PythonInterpreter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torchrl.envs.llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatEnv</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">tensordict</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDict</span><span class="p">,</span> <span class="n">set_list_to_stack</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">pprint</span><span class="w"> </span><span class="kn">import</span> <span class="n">pprint</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">set_list_to_stack</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen2.5-7B-Instruct&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">base_env</span> <span class="o">=</span> <span class="n">ChatEnv</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">system_prompt</span><span class="o">=</span><span class="s2">&quot;You are an assistant that can execute Python code. Decorate your code with ```python``` tags.&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">user_role</span><span class="o">=</span><span class="s2">&quot;user&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">system_role</span><span class="o">=</span><span class="s2">&quot;system&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">env</span> <span class="o">=</span> <span class="n">base_env</span><span class="o">.</span><span class="n">append_transform</span><span class="p">(</span><span class="n">PythonInterpreter</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">env</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Pass the reset data - the prompt - to the environment</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reset_data</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">TensorDict</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">text</span><span class="o">=</span><span class="s2">&quot;Let&#39;s write a Python function that returns the square of a number.&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Simulate an action - i.e., a response from the LLM (as if we were an LLM)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">action</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;Here is a block of code to be executed in python:</span>
<span class="gp">... </span><span class="s2">```python</span>
<span class="gp">... </span><span class="s2">def square(x):</span>
<span class="gp">... </span><span class="s2">    return x * x</span>
<span class="gp">... </span><span class="s2">print(&#39;testing the square function with input 2:&#39;, square(2))</span>
<span class="gp">... </span><span class="s2">```</span>
<span class="gp">... </span><span class="s2">&lt;|im_end|&gt;</span>
<span class="gp">... </span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">step_data</span> <span class="o">=</span> <span class="n">reset_data</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;text_response&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">action</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span><span class="p">,</span> <span class="n">s_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step_and_maybe_reset</span><span class="p">(</span><span class="n">reset_data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The history is a stack of chat messages.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#  The python interpreter transform has executed the code in the last message.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pprint</span><span class="p">(</span><span class="n">s_</span><span class="p">[</span><span class="s2">&quot;history&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">))</span>
<span class="go">[&#39;&lt;|im_start|&gt;system\n&#39;</span>
<span class="go"> &#39;You are an assistant that can execute Python code. Decorate your code with &#39;</span>
<span class="go"> &#39;```python``` tags.&lt;|im_end|&gt;\n&#39;</span>
<span class="go"> &#39;&lt;|im_start|&gt;user\n&#39;</span>
<span class="go"> &quot;Let&#39;s write a Python function that returns the square of a &quot;</span>
<span class="go"> &#39;number.&lt;|im_end|&gt;\n&#39;</span>
<span class="go"> &#39;&lt;|im_start|&gt;assistant\n&#39;</span>
<span class="go"> &#39;Here is a block of code to be executed in python:\n&#39;</span>
<span class="go"> &#39;```python\n&#39;</span>
<span class="go"> &#39;def square(x):\n&#39;</span>
<span class="go"> &#39;    return x * x\n&#39;</span>
<span class="go"> &quot;print(&#39;testing the square function with input 2:&#39;, square(2))\n&quot;</span>
<span class="go"> &#39;```&lt;|im_end|&gt;\n&#39;</span>
<span class="go"> &#39;&lt;|im_start|&gt;user\n&#39;</span>
<span class="go"> &#39;&lt;tool_response&gt;\n&#39;</span>
<span class="go"> &#39;Code block 1 executed successfully:\n&#39;</span>
<span class="go"> &#39;testing the square function with input 2: 4\n&#39;</span>
<span class="go"> &#39;\n&#39;</span>
<span class="go"> &#39;&lt;/tool_response&gt;&lt;|im_end|&gt;\n&#39;</span>
<span class="go"> &#39;&lt;|im_start|&gt;assistant\n&#39;]</span>
</pre></div>
</div>
<p>Similarly, environments that load data from a dataset are just special instances of the <a class="reference internal" href="generated/torchrl.envs.llm.ChatEnv.html#torchrl.envs.llm.ChatEnv" title="torchrl.envs.llm.ChatEnv"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatEnv</span></code></a>
augmented with a <a class="reference internal" href="generated/torchrl.envs.llm.transforms.DataLoadingPrimer.html#torchrl.envs.llm.transforms.DataLoadingPrimer" title="torchrl.envs.llm.transforms.DataLoadingPrimer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoadingPrimer</span></code></a> transforms (and some dedicated reward parsing
transforms).</p>
<section id="designing-reward-transforms">
<h4>Designing Reward Transforms<a class="headerlink" href="#designing-reward-transforms" title="Permalink to this heading">¶</a></h4>
<p>When designing reward transforms for LLM environments, several key considerations must be
addressed to ensure proper integration with the training pipeline.
The examples of <a class="reference internal" href="generated/torchrl.envs.llm.GSM8KRewardParser.html#torchrl.envs.llm.GSM8KRewardParser" title="torchrl.envs.llm.GSM8KRewardParser"><code class="xref py py-class docutils literal notranslate"><span class="pre">GSM8KRewardParser</span></code></a> and
<a class="reference internal" href="generated/torchrl.envs.llm.IfEvalScorer.html#torchrl.envs.llm.IfEvalScorer" title="torchrl.envs.llm.IfEvalScorer"><code class="xref py py-class docutils literal notranslate"><span class="pre">IfEvalScorer</span></code></a> provide excellent templates for reward transform design.</p>
<p><strong>Reward Shape Requirements</strong></p>
<p>The reward tensor must have the same number of dimensions as the logits, which is typically
two more dimensions than the environment batch size:</p>
<ul class="simple">
<li><p><strong>Sparse rewards</strong>: Shape <code class="docutils literal notranslate"><span class="pre">(*bsz,</span> <span class="pre">1,</span> <span class="pre">1)</span></code> - single reward per sequence</p></li>
<li><p><strong>Dense rewards</strong>: Shape <code class="docutils literal notranslate"><span class="pre">(*bsz,</span> <span class="pre">num_tokens,</span> <span class="pre">1)</span></code> - per-token rewards</p></li>
</ul>
<p>This shape requirement ensures compatibility with the loss computation pipeline.
For example, in the GSM8K reward parser:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Rewards need to have shape broadcastable to [batch x tokens x 1]</span>
<span class="n">tds</span> <span class="o">=</span> <span class="n">tds</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Done State Management</strong></p>
<p>It is crucial to properly manage the done state to prevent endless generation. Common strategies include:</p>
<ol class="arabic simple">
<li><p><strong>Completion-based termination</strong>: Set done when the response is complete (e.g., <code class="docutils literal notranslate"><span class="pre">History.complete=True</span></code>)</p></li>
<li><p><strong>Content-based termination</strong>: Set done when specific content is detected (e.g., <code class="docutils literal notranslate"><span class="pre">&lt;answer&gt;</span></code> blocks)</p></li>
<li><p><strong>Step-based termination</strong>: Use <a class="reference internal" href="generated/torchrl.envs.transforms.StepCounter.html#torchrl.envs.transforms.StepCounter" title="torchrl.envs.transforms.StepCounter"><code class="xref py py-class docutils literal notranslate"><span class="pre">StepCounter</span></code></a> for predetermined step limits</p></li>
</ol>
<p>Example from IFEvalScorer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_done_if_answer</span> <span class="ow">and</span> <span class="nb">bool</span><span class="p">(</span><span class="n">answer_blocks</span><span class="p">):</span>
    <span class="n">next_tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;done&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="o">...</span><span class="p">))</span>
    <span class="n">next_tensordict</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;terminated&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="o">...</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Input Mode Handling</strong></p>
<p>Reward transforms must handle different input modes correctly:</p>
<ul class="simple">
<li><p><strong>History mode</strong>: Extract text from <code class="docutils literal notranslate"><span class="pre">(&quot;history&quot;,</span> <span class="pre">&quot;full&quot;)</span></code> or <code class="docutils literal notranslate"><span class="pre">(&quot;history&quot;,</span> <span class="pre">&quot;response&quot;)</span></code></p></li>
<li><p><strong>Text mode</strong>: Use text directly from <code class="docutils literal notranslate"><span class="pre">(&quot;text&quot;,</span> <span class="pre">&quot;full&quot;)</span></code> or <code class="docutils literal notranslate"><span class="pre">(&quot;text&quot;,</span> <span class="pre">&quot;response&quot;)</span></code></p></li>
<li><p><strong>Tokens mode</strong>: Decode tokens from <code class="docutils literal notranslate"><span class="pre">(&quot;tokens&quot;,</span> <span class="pre">&quot;full&quot;)</span></code> or <code class="docutils literal notranslate"><span class="pre">(&quot;tokens&quot;,</span> <span class="pre">&quot;response&quot;)</span></code></p></li>
</ul>
<p>The GSM8K reward parser demonstrates this pattern:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">input_mode</span> <span class="o">==</span> <span class="s2">&quot;history&quot;</span><span class="p">:</span>
    <span class="n">responses</span> <span class="o">=</span> <span class="n">lazy_stack</span><span class="p">([</span><span class="n">r</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">responses</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="mi">0</span><span class="p">)])</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">responses</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">):</span>
        <span class="n">text_completion</span> <span class="o">=</span> <span class="n">responses</span><span class="o">.</span><span class="n">content</span>
<span class="k">elif</span> <span class="n">input_mode</span> <span class="o">==</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span>
    <span class="n">text_completion</span> <span class="o">=</span> <span class="n">responses</span>
<span class="k">elif</span> <span class="n">input_mode</span> <span class="o">==</span> <span class="s2">&quot;tokens&quot;</span><span class="p">:</span>
    <span class="n">text_completion</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">responses</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
<p><strong>Specification Management</strong></p>
<p>Accurate specification of reward and observation specs is essential for proper environment initialization. Both GSM8K and IFEval provide good examples:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">transform_reward_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward_spec</span><span class="p">:</span> <span class="n">Composite</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composite</span><span class="p">:</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">reward_spec</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">reward_spec</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="n">Composite</span><span class="p">(</span>
            <span class="n">reward_answer</span><span class="o">=</span><span class="n">Unbounded</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span>
            <span class="n">reward_think</span><span class="o">=</span><span class="n">Unbounded</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span>
            <span class="n">reward_right</span><span class="o">=</span><span class="n">Unbounded</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span>
            <span class="n">reward_contained</span><span class="o">=</span><span class="n">Unbounded</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span>
            <span class="n">reward</span><span class="o">=</span><span class="n">Unbounded</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span>
            <span class="n">success</span><span class="o">=</span><span class="n">Unbounded</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">reward_spec</span>
</pre></div>
</div>
<p><strong>Batch Processing Considerations</strong></p>
<p>For efficient processing, handle batched data appropriately:</p>
<ol class="arabic simple">
<li><p><strong>Flatten batch dimensions</strong>: Use <code class="docutils literal notranslate"><span class="pre">tensordict.view(-1)</span></code> for processing</p></li>
<li><p><strong>Reshape results</strong>: Restore original batch structure after processing</p></li>
<li><p><strong>Handle variable-length sequences</strong>: Use proper padding and masking</p></li>
</ol>
<p><strong>Reward Aggregation Strategies</strong></p>
<p>Consider different reward aggregation approaches:</p>
<ol class="arabic simple">
<li><p><strong>Simple aggregation</strong>: Sum or average multiple reward components</p></li>
<li><p><strong>Weighted aggregation</strong>: Apply different weights to different components</p></li>
<li><p><strong>Conditional rewards</strong>: Base rewards on specific conditions or thresholds</p></li>
</ol>
<p>The IFEvalScorer demonstrates a sophisticated aggregation strategy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">default_reward_aggregator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">score</span><span class="p">:</span> <span class="n">IFEvalScoreData</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
    <span class="c1"># Format score (max 1.0)</span>
    <span class="n">format_score</span> <span class="o">=</span> <span class="p">(</span><span class="n">format_components</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Structure score (max 1.0)</span>
    <span class="n">structure_score</span> <span class="o">=</span> <span class="n">think_score</span> <span class="o">+</span> <span class="n">answer_score</span>

    <span class="c1"># Completion bonus (max 0.2)</span>
    <span class="n">completion_bonus</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">complete</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.2</span>

    <span class="k">return</span> <span class="n">format_score</span> <span class="o">+</span> <span class="n">structure_score</span> <span class="o">+</span> <span class="n">completion_bonus</span>
</pre></div>
</div>
<p><strong>Post-Processing in Replay Buffers</strong></p>
<p>Rewards can also be computed after the fact by appending transforms to the replay buffer. However, done state capture must remain in the environment transform since it needs to occur on-the-fly during data collection.</p>
<p><strong>Error Handling and Robustness</strong></p>
<p>Implement robust error handling for parsing failures:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">cot</span><span class="p">,</span> <span class="n">potential_answer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extract_tags</span><span class="p">(</span><span class="n">compl</span><span class="p">)</span>
<span class="k">except</span> <span class="n">ET</span><span class="o">.</span><span class="n">ParseError</span><span class="p">:</span>
    <span class="n">cot</span><span class="p">,</span> <span class="n">potential_answer</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Performance Considerations</strong></p>
<ol class="arabic simple">
<li><p><strong>Avoid redundant computations</strong>: Cache parsed results when possible</p></li>
<li><p><strong>Use efficient text processing</strong>: Leverage regex or XML parsing as appropriate</p></li>
<li><p><strong>Minimize memory allocations</strong>: Reuse tensors and avoid unnecessary copies</p></li>
</ol>
<p>By following these design principles, reward transforms can be effectively integrated into the LLM training pipeline while maintaining performance and reliability.</p>
<table class="autosummary longtable docutils # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.AddThinkingPrompt.html#torchrl.envs.llm.transforms.AddThinkingPrompt" title="torchrl.envs.llm.transforms.AddThinkingPrompt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AddThinkingPrompt</span></code></a>(cond[, prompt, ...])</p></td>
<td><p>A transform that adds thinking prompts to encourage the LLM to reconsider its response.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.BrowserTransform.html#torchrl.envs.llm.transforms.BrowserTransform" title="torchrl.envs.llm.transforms.BrowserTransform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BrowserTransform</span></code></a>([allowed_domains, ...])</p></td>
<td><p>A transform that enables web browsing capabilities.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.DataLoadingPrimer.html#torchrl.envs.llm.transforms.DataLoadingPrimer" title="torchrl.envs.llm.transforms.DataLoadingPrimer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DataLoadingPrimer</span></code></a>(*args[, use_ray_service])</p></td>
<td><p>A primer that loads data from a dataloader and converts it into a tensordict using <code class="docutils literal notranslate"><span class="pre">stack_method</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.KLComputation.html#torchrl.envs.llm.transforms.KLComputation" title="torchrl.envs.llm.transforms.KLComputation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KLComputation</span></code></a>([gen_log_probs_full_key, ...])</p></td>
<td><p>A transform to compute KL divergence between two log-prob tensors and optionally add it to the reward.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.KLRewardTransform.html#torchrl.envs.llm.transforms.KLRewardTransform" title="torchrl.envs.llm.transforms.KLRewardTransform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KLRewardTransform</span></code></a>(*args[, use_ray_service])</p></td>
<td><p>A legacy transform for computing KL divergence-based rewards.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.MCPToolTransform.html#torchrl.envs.llm.transforms.MCPToolTransform" title="torchrl.envs.llm.transforms.MCPToolTransform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MCPToolTransform</span></code></a>(tools, tool_schemas[, ...])</p></td>
<td><p>A transform that executes MCP-style tools in response to LLM actions.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.PolicyVersion.html#torchrl.envs.llm.transforms.PolicyVersion" title="torchrl.envs.llm.transforms.PolicyVersion"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PolicyVersion</span></code></a>(version_type, ...)</p></td>
<td><p>A transform that keeps track of the version of the policy.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.PythonInterpreter.html#torchrl.envs.llm.transforms.PythonInterpreter" title="torchrl.envs.llm.transforms.PythonInterpreter"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PythonInterpreter</span></code></a>([tokenizer, tool_name, ...])</p></td>
<td><p>A transform that executes Python code in the LLM response.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.RayDataLoadingPrimer.html#torchrl.envs.llm.transforms.RayDataLoadingPrimer" title="torchrl.envs.llm.transforms.RayDataLoadingPrimer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RayDataLoadingPrimer</span></code></a>(*[, dataloader, ...])</p></td>
<td><p>A <a class="reference internal" href="generated/torchrl.envs.llm.transforms.DataLoadingPrimer.html#torchrl.envs.llm.transforms.DataLoadingPrimer" title="torchrl.envs.llm.transforms.dataloading.DataLoadingPrimer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoadingPrimer</span></code></a> that creates a single actor that can be shared by multiple environments.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.RetrieveKL.html#torchrl.envs.llm.transforms.RetrieveKL" title="torchrl.envs.llm.transforms.RetrieveKL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RetrieveKL</span></code></a>(*args[, use_ray_service])</p></td>
<td><p>A transform to retrieve the KL divergence between two models' log-probabilities.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.RetrieveLogProb.html#torchrl.envs.llm.transforms.RetrieveLogProb" title="torchrl.envs.llm.transforms.RetrieveLogProb"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RetrieveLogProb</span></code></a>(model, *[, ...])</p></td>
<td><p>A transform to retrieve log-probabilities from a model for KL divergence computation.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.TemplateTransform.html#torchrl.envs.llm.transforms.TemplateTransform" title="torchrl.envs.llm.transforms.TemplateTransform"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TemplateTransform</span></code></a>(tokenizer[, chat_template])</p></td>
<td><p>A transform that maps applies a chat template to an input string during the forward pass, and parses the strings to the template during backward.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.Tokenizer.html#torchrl.envs.llm.transforms.Tokenizer" title="torchrl.envs.llm.transforms.Tokenizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Tokenizer</span></code></a>([in_keys, out_keys, in_keys_inv, ...])</p></td>
<td><p>Applies a tokenization operation on the specified inputs.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.as_nested_tensor.html#torchrl.envs.llm.transforms.as_nested_tensor" title="torchrl.envs.llm.transforms.as_nested_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_nested_tensor</span></code></a>(list_of_tensordicts)</p></td>
<td><p>Stacks a list of tensordicts into a single tensordict with nested tensors.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.envs.llm.transforms.as_padded_tensor.html#torchrl.envs.llm.transforms.as_padded_tensor" title="torchrl.envs.llm.transforms.as_padded_tensor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">as_padded_tensor</span></code></a>(list_of_tensordicts[, dim, ...])</p></td>
<td><p>Stacks a list of tensordicts into a single tensordict with padded tensors.</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="objectives">
<h2>Objectives<a class="headerlink" href="#objectives" title="Permalink to this heading">¶</a></h2>
<p>LLM post-training requires specialized loss functions that are adapted to the unique characteristics of language models.</p>
<section id="grpo">
<h3>GRPO<a class="headerlink" href="#grpo" title="Permalink to this heading">¶</a></h3>
<p>The <a class="reference internal" href="generated/torchrl.objectives.llm.GRPOLoss.html#torchrl.objectives.llm.GRPOLoss" title="torchrl.objectives.llm.GRPOLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">GRPOLoss</span></code></a> class is a thin wrapper around the <a class="reference internal" href="generated/torchrl.objectives.PPOLoss.html#torchrl.objectives.PPOLoss" title="torchrl.objectives.PPOLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">PPOLoss</span></code></a> class
that codes the LLM-specific functionalities.</p>
<table class="autosummary longtable docutils # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.objectives.llm.GRPOLoss.html#torchrl.objectives.llm.GRPOLoss" title="torchrl.objectives.llm.GRPOLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GRPOLoss</span></code></a>(*args, **kwargs)</p></td>
<td><p>GRPO loss.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.objectives.llm.GRPOLossOutput.html#torchrl.objectives.llm.GRPOLossOutput" title="torchrl.objectives.llm.GRPOLossOutput"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GRPOLossOutput</span></code></a>(loss_objective, ...[, ...])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.objectives.llm.MCAdvantage.html#torchrl.objectives.llm.MCAdvantage" title="torchrl.objectives.llm.MCAdvantage"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MCAdvantage</span></code></a>(grpo_size[, prompt_key, ...])</p></td>
<td><p>Monte-Carlo advantage computation engine.</p></td>
</tr>
</tbody>
</table>
<section id="sft">
<h4>SFT<a class="headerlink" href="#sft" title="Permalink to this heading">¶</a></h4>
<table class="autosummary longtable docutils # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.objectives.llm.SFTLoss.html#torchrl.objectives.llm.SFTLoss" title="torchrl.objectives.llm.SFTLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SFTLoss</span></code></a>(*args, **kwargs)</p></td>
<td><p>Supervised fine-tuning loss.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="generated/torchrl.objectives.llm.SFTLossOutput.html#torchrl.objectives.llm.SFTLossOutput" title="torchrl.objectives.llm.SFTLossOutput"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SFTLossOutput</span></code></a>(loss_sft[, loss_kl_to_ref, ...])</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<table class="autosummary longtable docutils # Necessary for the table generated by autosummary to look decent align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="generated/torchrl.data.llm.TopKRewardSelector.html#torchrl.data.llm.TopKRewardSelector" title="torchrl.data.llm.TopKRewardSelector"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TopKRewardSelector</span></code></a>(total_dialog_turns, topk_size)</p></td>
<td><p>A replay-buffer transform that selects the top-k rewards for each prompt.</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="generated/torchrl.data.llm.History.html" class="btn btn-neutral float-right" title="History" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="generated/torchrl.envs.register_gym_spec_conversion.html" class="btn btn-neutral" title="register_gym_spec_conversion" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, Meta.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>


        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">LLM Interface</a><ul>
<li><a class="reference internal" href="#data-structures">Data Structures</a><ul>
<li><a class="reference internal" href="#history-class">History Class</a><ul>
<li><a class="reference internal" href="#supported-model-families">Supported Model Families</a></li>
<li><a class="reference internal" href="#usage">Usage</a></li>
<li><a class="reference internal" href="#adding-custom-templates">Adding Custom Templates</a></li>
<li><a class="reference internal" href="#usage-examples">Usage Examples</a><ul>
<li><a class="reference internal" href="#adding-a-llama-template">Adding a Llama Template</a></li>
</ul>
</li>
<li><a class="reference internal" href="#testing-your-custom-templates">Testing Your Custom Templates</a><ul>
<li><a class="reference internal" href="#assistant-token-masking-test">Assistant Token Masking Test</a></li>
<li><a class="reference internal" href="#template-equivalence-test">Template Equivalence Test</a></li>
<li><a class="reference internal" href="#inverse-parsing-test">Inverse Parsing Test</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#llm-wrapper-api">LLM Wrapper API</a><ul>
<li><a class="reference internal" href="#prompt-vs-response-and-padding">Prompt vs. Response and padding</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#modules">Modules</a><ul>
<li><a class="reference internal" href="#wrappers">Wrappers</a><ul>
<li><a class="reference internal" href="#async-vllm-engine-recommended">Async vLLM Engine (Recommended)</a></li>
<li><a class="reference internal" href="#remote-wrappers">Remote Wrappers</a></li>
<li><a class="reference internal" href="#utils">Utils</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#collectors">Collectors</a><ul>
<li><a class="reference internal" href="#vllm-weight-synchronization-schemes">vLLM Weight Synchronization Schemes</a></li>
<li><a class="reference internal" href="#policy-version-tracking">Policy Version Tracking</a></li>
<li><a class="reference internal" href="#legacy-weight-updaters-deprecated">Legacy Weight Updaters (Deprecated)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#environments">Environments</a><ul>
<li><a class="reference internal" href="#chatenv">ChatEnv</a><ul>
<li><a class="reference internal" href="#core-functionality">Core Functionality</a></li>
<li><a class="reference internal" href="#transform-based-architecture">Transform-Based Architecture</a></li>
<li><a class="reference internal" href="#integration-with-llm-wrappers">Integration with LLM Wrappers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#task-specific-environments">Task-Specific Environments</a></li>
<li><a class="reference internal" href="#transforms">Transforms</a><ul>
<li><a class="reference internal" href="#designing-reward-transforms">Designing Reward Transforms</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#objectives">Objectives</a><ul>
<li><a class="reference internal" href="#grpo">GRPO</a><ul>
<li><a class="reference internal" href="#sft">SFT</a><ul>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
     
    <script type="text/javascript">
      $(document).ready(function() {
	  var downloadNote = $(".sphx-glr-download-link-note.admonition.note");
	  if (downloadNote.length >= 1) {
	      var tutorialUrl = $("#tutorial-type").text();
	      var githubLink = "https://github.com/pytorch/rl/blob/main/tutorials/sphinx-"  + tutorialUrl + ".py",
		  notebookLink = $(".sphx-glr-download-jupyter").find(".download.reference")[0].href,
		  notebookDownloadPath = notebookLink.split('_downloads')[1],
		  colabLink = "https://colab.research.google.com/github/pytorch/rl/blob/gh-pages/main/_downloads" + notebookDownloadPath;

	      $(".pytorch-call-to-action-links a[data-response='Run in Google Colab']").attr("href", colabLink);
	      $(".pytorch-call-to-action-links a[data-response='View on Github']").attr("href", githubLink);
	  }

          var overwrite = function(_) {
              if ($(this).length > 0) {
                  $(this)[0].href = "https://github.com/pytorch/rl"
              }
          }
          // PC
          $(".main-menu a:contains('GitHub')").each(overwrite);
          // Mobile
          $(".main-menu a:contains('Github')").each(overwrite);
      });

      
       $(window).ready(function() {
           var original = window.sideMenus.bind;
           var startup = true;
           window.sideMenus.bind = function() {
               original();
               if (startup) {
                   $("#pytorch-right-menu a.reference.internal").each(function(i) {
                       if (this.classList.contains("not-expanded")) {
                           this.nextElementSibling.style.display = "block";
                           this.classList.remove("not-expanded");
                           this.classList.add("expanded");
                       }
                   });
                   startup = false;
               }
           };
       });
    </script>

    


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>