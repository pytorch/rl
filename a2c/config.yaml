# Environment
env_library: gym # env_library used for the simulated environment.
env_name: HalfCheetah-v4   name of the environment to be created. Default=Humanoid-v2
env_task: run # task (if any) for the environment.
from_pixels: False # whether the environment output should be state vector(s) (default) or the pixels.
frame_skip: 1 # frame_skip for the environment.
reward_scaling: 1.0 # scale of the reward.
reward_loc: 0.0 # location of the reward.
init_env_steps: 1000 # number of random steps to compute normalizing constants
vecnorm: True # Normalizes the environment observation and reward outputs with the running statistics obtained across processes.
norm_rewards: False # If True, rewards will be normalized on the fly.
norm_stats: True # Deactivates the normalization based on random collection of data.
noops:  0 # number of random steps to do after reset. Default is 0
catframes: 0 # Number of frames to concatenate through time. Default is 0 (do not use CatFrames).
center_crop: False # center crop size.
grayscale: True # Disables grayscale transform.
max_frames_per_traj: 1000 # Number of steps before a reset of the environment is called (if it has not been flagged as done before).
batch_transform: False # if True, the transforms will be applied to the parallel env, and not to each individual env.\
image_size: 84 # if True and environment has discrete action space, then it is encoded as categorical values rather than one-hot.
categorical_action_encoding: False

# Logger
logger: wandb # recorder type to be used. One of 'tensorboard', 'wandb' or 'csv'
record_video: False # whether a video of the task should be rendered during logging.
no_video: True # whether a video of the task should be rendered during logging.
exp_name: "" # experiment name. Used for logging directory.
record_interval: 1000 # number of batch collections in between two collections of validation rollouts. Default=1000.
record_frames: 1000 # number of steps in validation rollouts. " "Default=1000.
recorder_log_keys: ["reward"] # Keys to log in the recorder
offline_logging: True # If True, Wandb will do the logging offline

# Collector
collector_devices: ["cpu"] # device on which the data collector should store the trajectories to be passed to this script.
pin_memory: False # if True, the data collector will call pin_memory before dispatching tensordicts onto the passing device
init_with_lag: False # if True, the first trajectory will be truncated earlier at a random step.
frames_per_batch: 128 # Number of steps executed in the environment per collection.
total_frames: 1_000_000 # total number of frames collected for training. Does account for frame_skip.
num_workers: 4 # Number of workers used for data collection.
env_per_collector: 4 # Number of environments per collector. If the env_per_collector is in the range:
# 1 < env_per_collector <= num_workers, then the collector runs
# ceil(num_workers/env_per_collector) in parallel and executes the policy steps synchronously
# for each of these parallel wrappers. If env_per_collector=num_workers, no parallel wrapper is created
seed: 42 # seed used for the environment, pytorch and numpy.
exploration_mode: "" # exploration mode of the data collector.
async_collection: False # Whether data collection should be done asynchrously.

# Model
gSDE: True # if True, exploration is achieved using the gSDE technique.
tanh_loc: False # if True, uses a Tanh-Normal transform for the policy location of the form
# upscale * tanh(loc/upscale) (only available with TanhTransform and TruncatedGaussian distributions)
default_policy_scale: 1.0 # Default policy scale parameter
distribution: tanh_normal # if True, uses a Tanh-Normal-Tanh distribution for the policy
lstm: False # if True, uses an LSTM for the policy.
shared_mapping: False # if True, the first layers of the actor-critic are shared.

# Objective
entropy_coef: 0.0 # Entropy factor for the A2C loss
critic_coef: 0.4 # Critic factor for the A2C loss
critic_loss_function: smooth_l1 # loss function for the value network. Either one of l1, l2 or smooth_l1 (default).
advantage_in_loss: False # if True, the advantage is computed on the sub-batch

# Trainer
optim_steps_per_batch: 1 # Number of optimization steps in between two collection of data.
optimizer: "adam" # Optimizer to be used.
lr_scheduler: "cosine" # LR scheduler.
selected_keys: null # a list of strings that indicate the data that should be kept from the data collector.
batch_size: 128 # batch size of the TensorDict retrieved from the replay buffer. Default=256.
log_interval: 1 # logging interval, in terms of optimization steps. Default=10000.
lr: 5e-4 # Learning rate used for the optimizer. Default=3e-4.
weight_decay: 0.0 # Weight-decay to be used with the optimizer. Default=0.0.
clip_norm: 1000.0 # value at which the total gradient norm / single derivative should be clipped. Default=1000.0
clip_grad_norm: False # if called, the gradient will be clipped based on its L2 norm. Otherwise, single gradient values will be clipped to the desired threshold.
normalize_rewards_online: False # Computes the running statistics of the rewards and normalizes them before they are passed to the loss module.
normalize_rewards_online_scale: 1.0 # Final scale of the normalized rewards.
normalize_rewards_online_decay: 0.9999 # Decay of the reward moving averaging
sub_traj_len: -1 # length of the trajectories that sub-samples must have in online settings.