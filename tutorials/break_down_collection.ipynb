{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69828b7-c34c-4a66-99e8-e5803624f7c5",
   "metadata": {},
   "source": [
    "# Rollout collection walktrhough with TensorDicts\n",
    "\n",
    "This notebook aims to demonstrate how TensorDicts facilitate communication between actors and environments, irrespective of their required inputs and outputs, while still allowing for a transparent data workflow. We showcase this capability through an example of a data collection loop. Importantly, we want to emphasize that this feature does not compromise the readability of the process. Throughout the demonstration, we will illustrate how information about the data workflow remains transparent and easily accessible to the user, streamlining the entire process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccf990-1696-4528-a35b-a340218b3b54",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5e42cbf1-d858-443b-bcf9-972146ef0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tensordict import TensorDict\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.modules import OneHotCategorical, ProbabilisticActor\n",
    "from torchrl.envs.utils import step_mdp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9bc47-4054-49c9-a88d-edaf64cb42aa",
   "metadata": {},
   "source": [
    "### Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e8c94fff-13d6-4434-9590-457b26f59dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GymEnv(\"CartPole-v1\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d19b2-99fc-4d74-a75c-b9731d275219",
   "metadata": {},
   "source": [
    "### Visualise the environment spaces specs\n",
    "\n",
    "We can obtain comprehensive information about all environment inputs and outputs, including their shapes, data types, and devices, by examining the environment specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9135cbef-4c0a-4c0a-b42f-c49b2603f687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeSpec(\n",
      "    output_spec: CompositeSpec(\n",
      "        _observation_spec: CompositeSpec(\n",
      "            observation: BoundedTensorSpec(\n",
      "                shape=torch.Size([4]),\n",
      "                space=ContinuousBox(\n",
      "                    minimum=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                    maximum=Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous), device=cpu, shape=torch.Size([])),\n",
      "        _reward_spec: CompositeSpec(\n",
      "            reward: UnboundedContinuousTensorSpec(\n",
      "                shape=torch.Size([1]),\n",
      "                space=None,\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous), device=cpu, shape=torch.Size([])),\n",
      "        _done_spec: CompositeSpec(\n",
      "            done: DiscreteTensorSpec(\n",
      "                shape=torch.Size([1]),\n",
      "                space=DiscreteBox(n=2),\n",
      "                device=cpu,\n",
      "                dtype=torch.bool,\n",
      "                domain=discrete), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([])),\n",
      "    input_spec: CompositeSpec(\n",
      "        _state_spec: None,\n",
      "        _action_spec: CompositeSpec(\n",
      "            action: OneHotDiscreteTensorSpec(\n",
      "                shape=torch.Size([2]),\n",
      "                space=DiscreteBox(n=2),\n",
      "                device=cpu,\n",
      "                dtype=torch.int64,\n",
      "                domain=discrete), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([])), device=cpu, shape=torch.Size([]))\n"
     ]
    }
   ],
   "source": [
    "print(env.specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a59eb-6d9e-46ea-b42d-5117f1ab94dc",
   "metadata": {},
   "source": [
    "### Create the actor and visualising its inputs and outputs\n",
    "\n",
    "We can retrieve a list of the expected tensor inputs and outputs of the actor using the 'in_keys' and 'out_keys' attributes. In this case, the actor expect to receive a TensorDict object with an 'observation' tensor in it, and will populate the TensorDict with 2 additional tensors, the 'logits' and the 'action'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5114b40f-1b86-4c33-ba23-3028bf29eabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor inputs:  ['observation']\n",
      "actor outputs:  ['logits', 'action']\n"
     ]
    }
   ],
   "source": [
    "actor = ProbabilisticActor(\n",
    "    module=TensorDictModule(nn.Linear(4, 2), in_keys=[\"observation\"], out_keys=[\"logits\"]),\n",
    "    in_keys=[\"logits\"],\n",
    "    out_keys=[\"action\"],\n",
    "    distribution_class=OneHotCategorical)\n",
    "\n",
    "print(\"actor inputs: \", actor.in_keys)\n",
    "print(\"actor outputs: \", actor.out_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdad2d-53dd-484d-91fc-08048278a77f",
   "metadata": {},
   "source": [
    "### Create a target TensorDict to store the rollouts\n",
    "\n",
    "In this example, we will collect T consecutive steps of the environment. To do so, we create an empty TensorDict with batch size T. We can print it and see that no tensors are contained it in.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9ec0aa53-70b4-485d-a089-ab1f268afb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "    },\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "out = TensorDict({}, batch_size=[T], device=\"cpu\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2cf1e3-fefa-4c52-bad3-a3b8832b737f",
   "metadata": {},
   "source": [
    "### Single step-by-step data collection loop\n",
    "\n",
    "Following, we will do the first run through the data collection loop step-by-step, showing how the data TensorDict is filled up with a transition data.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "df223e62-7e46-4cc1-9f02-06fb576b0041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "data = env.reset()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a2182-6b30-4a47-bf86-d7eb7ad084a1",
   "metadata": {},
   "source": [
    "<br>\n",
    "As expected, the actor adds action an logits to data.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "29ac25a2-9fc6-4ba9-af8b-853c2979e733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        logits: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "data = actor(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20589fe4-a23f-4ad4-862a-df7ffc45a289",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now that we have the action in our TensorDict, we can take a step in the environment. As in any RL environment, our environment will return the next observation, the reward and a done flag in response the selected action.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "697d0bbc-b032-4b5a-a527-ecc586dd2960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        logits: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "data = env.step(data)\n",
    "out[i] = data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b0347-1189-415f-90f9-bce2a7f0f5c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "Finally, we use the 'step_mdp()' function to update the TensorDict by one step, shifting the 'done' and 'observation' tensors from the 'next' state to the current state, thereby preparing the TensorDict for another iteration through the loop.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f3e774df-309e-4e6c-9b18-36d6311fb5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        logits: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "data = step_mdp(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9722403e-b126-4c0d-8479-f24609a9af0d",
   "metadata": {},
   "source": [
    "<br>\n",
    "Because TensorDicts enable the transfer of any tensors between actors and environments, the code below will function effectively for any actor and environment. While the tensors within the TensorDict may vary depending on the specific case, they will always remain accessible to the user as demonstrated in the notebook, facilitating understanding of the workflow.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0e2ad073-5747-45c9-83c2-43d72460c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, T):\n",
    "    data = actor(data)\n",
    "    data = env.step(data)\n",
    "    out[i] = data\n",
    "    data = step_mdp(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876fe00f-fa8e-4d46-b292-81f73739149c",
   "metadata": {},
   "source": [
    "<br>\n",
    "The out TensorDict contains now the collected data.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "01773eda-67e9-4f75-82f4-be005cfb81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        logits: Tensor(shape=torch.Size([10, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([10]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        observation: Tensor(shape=torch.Size([10, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
