{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "308f4833-c7ec-4040-b724-df01d437ce44",
      "metadata": {},
      "source": [
        "[<img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/facebookresearch/rl/blob/main/tutorials/demo.ipynb)\n",
        "\n",
        "# TorchRL Demo\n",
        "\n",
        "___\n",
        "This demo was presented at ICML 2022 on the industry demo day.\n",
        "\n",
        "It gives a good overview of TorchRL functionalities.\n",
        "\n",
        "Feel free to reach out to vmoens@fb.com or submit issues if you have questions or comments about it.\n",
        "___\n",
        "\n",
        "TorchRL is an open-source Reinforcement Learning (RL) library for PyTorch.\n",
        "\n",
        "https://github.com/facebookresearch/rl\n",
        "\n",
        "The PyTorch ecosystem team (Meta) has decided to invest in that library to provide a leading platform to develop RL solutions in research settings.\n",
        "\n",
        "It provides pytorch and **python-first**, low and high level **abstractions** for RL that are intended to be efficient, documented and properly tested. The code is aimed at supporting research in RL. Most of it is written in python in a highly modular way, such that researchers can easily swap components, transform them or write new ones with little effort.\n",
        "\n",
        "This repo attempts to align with the existing pytorch ecosystem libraries in that it has a dataset pillar (torchrl/envs), transforms, models, data utilities (e.g. collectors and containers), etc. TorchRL aims at having as few dependencies as possible (python standard library, numpy and pytorch). Common environment libraries (e.g. OpenAI gym) are only optional.\n",
        "\n",
        "Content:\n",
        "\n",
        "```\n",
        "torchrl\n",
        "│\n",
        "└───collectors\n",
        "│       collectors.py\n",
        "│   \n",
        "└───data\n",
        "│   │   tensor_specs.py\n",
        "│   └───postprocs\n",
        "│   │       postprocs.py\n",
        "│   └───replay_buffers\n",
        "│   │       replay_buffers.py\n",
        "│   │       storages.py\n",
        "│   └───tensordict\n",
        "│           memmap.py\n",
        "│           metatensor.py\n",
        "│           tensordict.py\n",
        "└───envs\n",
        "│   │   common.py\n",
        "│   │   env_creator.py\n",
        "│   │   gym_like.py\n",
        "│   │   vec_env.py\n",
        "│   └───libs\n",
        "│   │       dm_control.py\n",
        "│   │       gym.py\n",
        "│   └───transforms\n",
        "│           functional.py\n",
        "│           transforms.py\n",
        "└───modules\n",
        "│   └───distributions\n",
        "│   │       continuous.py\n",
        "│   │       discrete.py\n",
        "│   └───models\n",
        "│   │       models.py\n",
        "│   │       exploration.py\n",
        "│   └───tensordict_module\n",
        "│           actors.py\n",
        "│           common.py\n",
        "│           exploration.py\n",
        "│           probabilistic.py\n",
        "│           sequence.py\n",
        "└───obsjectives\n",
        "│   └───costs\n",
        "│   │       common.py\n",
        "│   │       ddpg.py\n",
        "│   │       dqn.py\n",
        "│   │       functional.py\n",
        "│   │       ppo.py\n",
        "│   │       redq.py\n",
        "│   │       reinforce.py\n",
        "│   │       sac.py\n",
        "│   │       utils.py\n",
        "│   └───returns\n",
        "│           advantages.py\n",
        "│           functional.py\n",
        "│           returns.py\n",
        "└───record\n",
        "└───trainers\n",
        "    │   loggers.py\n",
        "    │   trainers.py\n",
        "    └───helpers\n",
        "            collectors.py\n",
        "            envs.py\n",
        "            losses.py\n",
        "            models.py\n",
        "            recorder.py\n",
        "            replay_buffer.py\n",
        "            trainers.py\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d38be66d-a807-4a53-956f-d53eb99cc801",
      "metadata": {},
      "source": [
        "Unlike other domains, RL is less about media than _algorithms_. As such, it is harder to make truly independent components.\n",
        "\n",
        "What TorchRL is not:\n",
        "- a collection of algorithms: we do not intend to provide SOTA implementations of RL algorithms, but we provide these algorithms only as examples of how to use the library.\n",
        "- a research framework\n",
        "\n",
        "TorchRL has very few core dependencies, mostly PyTorch and functorch. All other dependencies (gym, torchvision, wandb / tensorboard) are optional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce9584b",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install functorch\n",
        "!pip install \"gym[classic_control]\"\n",
        "!pip install torchrl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af6372c-de8b-4435-92b4-53f95f4e5db5",
      "metadata": {
        "tags": []
      },
      "source": [
        "## Data\n",
        "### TensorDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8a66580d-4ab5-4c64-a3c0-7af171603bbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchrl.data import TensorDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "99bb0b39-fc81-4fcf-8de4-b115b1a49724",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        key 1: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
            "        key 2: Tensor(torch.Size([5, 5, 6]), dtype=torch.bool)},\n",
            "    batch_size=torch.Size([5]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "# Creating a TensorDict\n",
        "batch_size = 5\n",
        "tensordict = TensorDict(source={\n",
        "    \"key 1\": torch.zeros(batch_size, 3),\n",
        "    \"key 2\": torch.zeros(batch_size, 5, 6, dtype=torch.bool)\n",
        "}, batch_size = [batch_size])\n",
        "print(tensordict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8ef9a4f1-4682-41cb-b024-37770740d389",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        key 1: Tensor(torch.Size([3]), dtype=torch.float32),\n",
              "        key 2: Tensor(torch.Size([5, 6]), dtype=torch.bool)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# indexing\n",
        "tensordict[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7e0c8f4a-29c8-4194-acf8-fce33eb21d3e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# querying keys\n",
        "tensordict[\"key 1\"] is tensordict.get(\"key 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "40aab986-6949-4b21-990c-1afa56eea914",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([2, 5]),\n",
              " tensor([[[0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.],\n",
              "          [0.]],\n",
              " \n",
              "         [[1.],\n",
              "          [1.],\n",
              "          [1.],\n",
              "          [1.],\n",
              "          [1.]]]))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Stacking tensordicts\n",
        "\n",
        "tensordict1 = TensorDict(source={\n",
        "    \"key 1\": torch.zeros(batch_size, 1),\n",
        "    \"key 2\": torch.zeros(batch_size, 5, 6, dtype=torch.bool)\n",
        "}, batch_size = [batch_size])\n",
        "\n",
        "tensordict2 = TensorDict(source={\n",
        "    \"key 1\": torch.ones(batch_size, 1),\n",
        "    \"key 2\": torch.ones(batch_size, 5, 6, dtype=torch.bool)\n",
        "}, batch_size = [batch_size])\n",
        "\n",
        "tensordict = torch.stack([tensordict1, tensordict2], 0)\n",
        "tensordict.batch_size, tensordict[\"key 1\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "117ad9cb-9eec-4e50-9f26-03fe1f58ebf2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "view(-1):  torch.Size([10]) torch.Size([10, 1])\n",
            "to device:  LazyStackedTensorDict(\n",
            "    fields={\n",
            "        key 1: Tensor(torch.Size([2, 5, 1]), dtype=torch.float32),\n",
            "        key 2: Tensor(torch.Size([2, 5, 5, 6]), dtype=torch.bool)},\n",
            "    batch_size=torch.Size([2, 5]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "share memory:  LazyStackedTensorDict(\n",
            "    fields={\n",
            "        key 1: Tensor(torch.Size([2, 5, 1]), dtype=torch.float32),\n",
            "        key 2: Tensor(torch.Size([2, 5, 5, 6]), dtype=torch.bool)},\n",
            "    batch_size=torch.Size([2, 5]),\n",
            "    device=cpu,\n",
            "    is_shared=True)\n",
            "permute(1, 0):  torch.Size([5, 2]) torch.Size([5, 2, 1])\n",
            "expand:  torch.Size([3, 2, 5]) torch.Size([3, 2, 5, 1])\n"
          ]
        }
      ],
      "source": [
        "# Other functionalities\n",
        "print(\"view(-1): \", tensordict.view(-1).batch_size, tensordict.view(-1).get(\"key 1\").shape)\n",
        "\n",
        "print(\"to device: \", tensordict.to(\"cpu\"))\n",
        "\n",
        "# print(\"pin_memory: \", tensordict.pin_memory())\n",
        "\n",
        "print(\"share memory: \", tensordict.share_memory_())\n",
        "\n",
        "print(\"permute(1, 0): \", \n",
        "      tensordict.permute(1, 0).batch_size, \n",
        "      tensordict.permute(1, 0).get(\"key 1\").shape)\n",
        "\n",
        "print(\"expand: \", \n",
        "      tensordict.expand(3, *tensordict.batch_size).batch_size, \n",
        "      tensordict.expand(3, *tensordict.batch_size).get(\"key 1\").shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb26d8d-399f-498f-af7a-cf16597636d1",
      "metadata": {},
      "source": [
        "#### Nested tensordict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ac43369a-5bb7-45e8-afc0-8df76be1450e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        key 1: Tensor(torch.Size([5, 3]), dtype=torch.float32),\n",
              "        key 2: TensorDict(\n",
              "            fields={\n",
              "                sub-key 1: Tensor(torch.Size([5, 2, 1]), dtype=torch.float32)},\n",
              "            batch_size=torch.Size([5, 2]),\n",
              "            device=cpu,\n",
              "            is_shared=False)},\n",
              "    batch_size=torch.Size([5]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensordict = TensorDict(source={\n",
        "    \"key 1\": torch.zeros(batch_size, 3),\n",
        "    \"key 2\": TensorDict(source={\n",
        "        \"sub-key 1\": torch.zeros(batch_size, 2, 1)\n",
        "    }, batch_size=[batch_size, 2])\n",
        "}, batch_size = [batch_size])\n",
        "tensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e94f6a8d-2429-45c8-9d50-abebef682836",
      "metadata": {},
      "source": [
        "### Replay buffers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f123c7f1-fa92-491d-97c8-c5d82f556003",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.data import ReplayBuffer, PrioritizedReplayBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ddbdd51d-a36f-4b07-b3c4-6699ca813835",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rb = ReplayBuffer(100, collate_fn=lambda x: x)\n",
        "rb.add(1)\n",
        "rb.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d5254dd2-a2be-42cb-8304-a14324a51a2c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2, 1, 2]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rb.extend([2, 3])\n",
        "rb.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e607eb52-b55e-4416-93fc-a4aa400c47cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "rb = PrioritizedReplayBuffer(100, alpha=0.7, beta=1.1, collate_fn=lambda x: x)\n",
        "rb.add(1)\n",
        "rb.sample(1)\n",
        "rb.update_priority(1, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f43a07-d7bb-4bfc-8c11-c5eb4ae0caf7",
      "metadata": {},
      "source": [
        "#### working with tensordicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8ec9461d-afa4-4d35-ab04-6f66ac0e4036",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "collate_fn = torch.stack\n",
        "rb = ReplayBuffer(100, collate_fn=collate_fn)\n",
        "rb.add(TensorDict({\"a\": torch.randn(3)}, batch_size=[]))\n",
        "len(rb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "68d94666-3724-41a4-b888-e8d4bd75f853",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rb.extend(TensorDict({\"a\": torch.randn(2, 3)}, batch_size=[2]))\n",
        "len(rb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6a808673-0682-48f0-bf27-749c2bff753c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LazyStackedTensorDict(\n",
              "    fields={\n",
              "        a: Tensor(torch.Size([10, 3]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([10]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rb.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e79c254c-44d4-451b-ac76-733b38dd095e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        a: Tensor(torch.Size([2, 3]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([2]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rb.sample(2).contiguous()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "eb833a0f-01b9-4222-8239-cc8a7698b66d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        a: Tensor(torch.Size([2, 3]), dtype=torch.float32),\n",
              "        index: Tensor(torch.Size([2, 1]), dtype=torch.int32)},\n",
              "    batch_size=torch.Size([2]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "from torchrl.data import TensorDictPrioritizedReplayBuffer\n",
        "rb = TensorDictPrioritizedReplayBuffer(100, alpha=0.7, beta=1.1, priority_key=\"td_error\")\n",
        "rb.extend(TensorDict({\"a\": torch.randn(2, 3)}, batch_size=[2]))\n",
        "tensordict_sample = rb.sample(2).contiguous()\n",
        "tensordict_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1e029dc1-e5d0-4b7f-99fe-7f416725bdf2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [0]], dtype=torch.int32)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensordict_sample[\"index\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "70017242-5396-4319-905b-40e483b0f96f",
      "metadata": {},
      "outputs": [],
      "source": [
        "tensordict_sample[\"td_error\"] = torch.rand(2)\n",
        "rb.update_priority(tensordict_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6e64849c-f1c7-42d1-8d31-6fc7bece6e30",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0.28791671991348267\n",
            "1 0.06984968483448029\n",
            "2 0.0\n"
          ]
        }
      ],
      "source": [
        "for i, val in enumerate(rb._sum_tree):\n",
        "    print(i, val)\n",
        "    if i == len(rb):\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a6d60d-3de1-43f9-a498-337abb98de1d",
      "metadata": {},
      "source": [
        "## Envs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2bde26db-1880-4fcb-bd1d-f90420317b3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n"
          ]
        }
      ],
      "source": [
        "from torchrl.envs.libs.gym import GymWrapper, GymEnv\n",
        "import gym\n",
        "\n",
        "gym_env = gym.make(\"Pendulum-v1\")\n",
        "env = GymWrapper(gym_env)\n",
        "env = GymEnv(\"Pendulum-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "92441513-0e7e-424c-bd92-348febfb6875",
      "metadata": {},
      "outputs": [],
      "source": [
        "tensordict = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "ba2264a4-0cad-4f68-93bb-841365e468f1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([1]), dtype=torch.float32),\n",
              "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
              "        next_observation: Tensor(torch.Size([3]), dtype=torch.float32),\n",
              "        observation: Tensor(torch.Size([3]), dtype=torch.float32),\n",
              "        reward: Tensor(torch.Size([1]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.rand_step(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b01aebe-3517-41bf-9247-bc5f7bc44b7a",
      "metadata": {},
      "source": [
        "### changing environments config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "8296f4f9-9947-4053-a681-064eca21c2d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
              "        pixels: Tensor(torch.Size([500, 500, 3]), dtype=torch.uint8),\n",
              "        state: Tensor(torch.Size([3]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "bb91ba2f-158e-4bcd-bc37-c8601da92384",
      "metadata": {},
      "outputs": [],
      "source": [
        "env.close()\n",
        "del env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "05d14d70-3307-40df-a190-3c48b767feca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n"
          ]
        }
      ],
      "source": [
        "from torchrl.envs import Compose, ObservationNorm, ToTensorImage, NoopResetEnv, TransformedEnv\n",
        "base_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\n",
        "env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\n",
        "env.append_transform(ObservationNorm(keys_in=[\"next_pixels\"], loc=2, scale=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9509c09-b6a9-426a-a799-9766a195156b",
      "metadata": {},
      "source": [
        "### Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "fb96959d-c587-4f72-af17-86171d1ad952",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.envs import Compose, ObservationNorm, ToTensorImage, NoopResetEnv, TransformedEnv\n",
        "base_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\n",
        "env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\n",
        "env.append_transform(ObservationNorm(keys_in=[\"next_pixels\"], loc=2, scale=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f50a7a63-d156-4eac-94db-69395f799865",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
              "        pixels: Tensor(torch.Size([3, 500, 500]), dtype=torch.float32),\n",
              "        state: Tensor(torch.Size([3]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "883a438f-1f78-4759-a47d-77025cc24920",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env:  TransformedEnv(env=GymEnv(env=Pendulum-v1, batch_size=torch.Size([]), device=cpu), transform=Compose(\n",
            "        NoopResetEnv(noops=3, random=True),\n",
            "        ToTensorImage(keys=['next_pixels']),\n",
            "        ObservationNorm(loc=2.0000, scale=1.0000, keys=['next_pixels'])))\n",
            "last transform parent:  TransformedEnv(env=GymEnv(env=Pendulum-v1, batch_size=torch.Size([]), device=cpu), transform=Compose(\n",
            "        NoopResetEnv(noops=3, random=True),\n",
            "        ToTensorImage(keys=['next_pixels'])))\n"
          ]
        }
      ],
      "source": [
        "print(\"env: \", env)\n",
        "print(\"last transform parent: \", env.transform[2].parent)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f3160c-34e9-40a8-8ace-141a050111a2",
      "metadata": {},
      "source": [
        "### Vectorized environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "64137b0a-0267-4e6f-b08b-4b947ad98823",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        done: Tensor(torch.Size([4, 1]), dtype=torch.bool),\n",
              "        pixels: Tensor(torch.Size([4, 3, 500, 500]), dtype=torch.float32),\n",
              "        state: Tensor(torch.Size([4, 3]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([4]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchrl.envs import ParallelEnv\n",
        "base_env = ParallelEnv(4, lambda: GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False))\n",
        "env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))  # applies transforms on batch of envs\n",
        "env.append_transform(ObservationNorm(keys_in=[\"next_pixels\"], loc=2, scale=1))\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "93fc6dd6-1ed5-412f-8f4b-4099b30b969d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NdBoundedTensorSpec(\n",
              "     shape=torch.Size([1]), space=ContinuousBox(minimum=tensor([-2.]), maximum=tensor([2.])), device=cpu, dtype=torch.float32, domain=continuous)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.action_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b72279-24e4-486a-beca-e7c130164ed6",
      "metadata": {},
      "source": [
        "## Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6cb397-cbd6-4caf-bc1e-2f5388cd4c64",
      "metadata": {},
      "source": [
        "### Models\n",
        "#### MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "cb115026-b9ca-49c0-afd4-99795563e86b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (0): LazyLinear(in_features=0, out_features=32, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/vmoens/venv/rl/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "from torchrl.modules import MLP, ConvNet\n",
        "from torchrl.modules.models.utils import SquashDims\n",
        "from torch import nn\n",
        "net = MLP(num_cells=[32, 64], out_features=4, activation_class=nn.ELU)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "f6007bbe-df30-4762-97f5-26676647a40e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 4])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net(torch.randn(10, 3)).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a444a92-7b7d-42fc-a3d3-2bad7bc32880",
      "metadata": {},
      "source": [
        "#### CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "9facfc1d-f207-43b2-8fd5-92fa6b32fb18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ConvNet(\n",
            "  (0): LazyConv2d(0, 32, kernel_size=(8, 8), stride=(2, 2))\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1))\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): SquashDims()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "cnn = ConvNet(num_cells=[32, 64], kernel_sizes=[8, 4], strides=[2, 1], aggregator_class=SquashDims)\n",
        "print(cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "cd9d9eb1-ec44-4113-a3a1-5291f388e274",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10, 6400])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn(torch.randn(10, 3, 32, 32)).shape  # last tensor is squashed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab751e1c-3fbc-4209-9339-a0dea73664e5",
      "metadata": {},
      "source": [
        "### TensorDictModules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "0144b74a-2230-4d78-9b07-ad29a4f39402",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        key 1: Tensor(torch.Size([10, 3]), dtype=torch.float32),\n",
            "        key 2: Tensor(torch.Size([10, 4]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([10]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "from torchrl.modules import TensorDictModule\n",
        "tensordict = TensorDict({\"key 1\": torch.randn(10, 3)}, batch_size=[10])\n",
        "module = nn.Linear(3, 4)\n",
        "td_module = TensorDictModule(module, in_keys=[\"key 1\"], out_keys=[\"key 2\"])\n",
        "td_module(tensordict)\n",
        "print(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "819ab5df-d29c-44f4-8cc8-c15a2d553285",
      "metadata": {},
      "source": [
        "### Sequences of modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "e0e3e3ff-074a-4984-8e10-85fdd9d2328f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDictSequential(\n",
            "    module=ModuleList(\n",
            "      (0): TensorDictModule(\n",
            "          module=Linear(in_features=5, out_features=3, bias=True), \n",
            "          device=cpu, \n",
            "          in_keys=['observation'], \n",
            "          out_keys=['hidden'])\n",
            "      (1): TensorDictModule(\n",
            "          module=Linear(in_features=3, out_features=4, bias=True), \n",
            "          device=cpu, \n",
            "          in_keys=['hidden'], \n",
            "          out_keys=['action'])\n",
            "      (2): TensorDictModule(\n",
            "          module=MLP(\n",
            "            (0): LazyLinear(in_features=0, out_features=4, bias=True)\n",
            "            (1): Tanh()\n",
            "            (2): Linear(in_features=4, out_features=5, bias=True)\n",
            "            (3): Tanh()\n",
            "            (4): Linear(in_features=5, out_features=1, bias=True)\n",
            "          ), \n",
            "          device=cpu, \n",
            "          in_keys=['hidden', 'action'], \n",
            "          out_keys=['value'])\n",
            "    ), \n",
            "    device=cpu, \n",
            "    in_keys=['observation'], \n",
            "    out_keys=['hidden', 'action', 'value'])\n"
          ]
        }
      ],
      "source": [
        "from torchrl.modules import TensorDictSequential\n",
        "backbone_module = nn.Linear(5, 3)\n",
        "backbone = TensorDictModule(backbone_module, in_keys=[\"observation\"], out_keys=[\"hidden\"])\n",
        "actor_module = nn.Linear(3, 4)\n",
        "actor = TensorDictModule(actor_module, in_keys=[\"hidden\"], out_keys=[\"action\"])\n",
        "value_module = MLP(out_features=1, num_cells=[4, 5])\n",
        "value = TensorDictModule(value_module, in_keys=[\"hidden\", \"action\"], out_keys=[\"value\"])\n",
        "\n",
        "sequence = TensorDictSequential(backbone, actor, value)\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6e7980e8-7286-403e-82f7-8386021a6c85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['observation'] ['hidden', 'action', 'value']\n"
          ]
        }
      ],
      "source": [
        "print(sequence.in_keys, sequence.out_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "b9be6bff-d543-45bc-975e-d820b9db6ce8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
              "        hidden: Tensor(torch.Size([3, 3]), dtype=torch.float32),\n",
              "        observation: Tensor(torch.Size([3, 5]), dtype=torch.float32),\n",
              "        value: Tensor(torch.Size([3, 1]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([3]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensordict = TensorDict(\n",
        "    {\"observation\": torch.randn(3, 5)}, [3],\n",
        ")\n",
        "backbone(tensordict)\n",
        "actor(tensordict)\n",
        "value(tensordict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "315d7d77-314e-4ffc-b9aa-e3651937dc98",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
            "        hidden: Tensor(torch.Size([3, 3]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([3, 5]), dtype=torch.float32),\n",
            "        value: Tensor(torch.Size([3, 1]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([3]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "tensordict = TensorDict(\n",
        "    {\"observation\": torch.randn(3, 5)}, [3],\n",
        ")\n",
        "sequence(tensordict)\n",
        "print(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a62cd71d-a33c-41dd-aa75-eb4cefef8c50",
      "metadata": {},
      "source": [
        "### Functional programming (ensembling / meta-RL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "f3496472-b697-4c78-9b77-972b74573884",
      "metadata": {},
      "outputs": [],
      "source": [
        "fsequence, (params, buffers) = sequence.make_functional_with_buffers()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "1577590f-5156-439f-a2f1-f8cba1fa3e78",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(list(fsequence.parameters()))  # functional modules have no parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "971618a2-9c4c-4af6-b170-082cdea4a756",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([3, 4]), dtype=torch.float32),\n",
              "        hidden: Tensor(torch.Size([3, 3]), dtype=torch.float32),\n",
              "        observation: Tensor(torch.Size([3, 5]), dtype=torch.float32),\n",
              "        value: Tensor(torch.Size([3, 1]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([3]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fsequence(tensordict, params=params, buffers=buffers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "ad98c6dc-918e-450a-9f3c-feb738e36d35",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),\n",
            "        hidden: Tensor(torch.Size([4, 3, 3]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([4, 3, 5]), dtype=torch.float32),\n",
            "        value: Tensor(torch.Size([4, 3, 1]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([4, 3]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "params_expand = [p.expand(4, *p.shape) for p in params]\n",
        "buffers_expand = [b.expand(4, *b.shape) for b in buffers]\n",
        "tensordict_exp = fsequence(tensordict, params=params_expand, buffers=buffers, vmap=(0, 0, None))\n",
        "print(tensordict_exp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14084eb3-36e6-4729-8383-7ef4471fea5f",
      "metadata": {},
      "source": [
        "### Specialized classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "9c3f6d96-f213-4ef5-b700-133f40bf52f9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.0137,  0.1524, -0.0641], grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "from torchrl.data import NdBoundedTensorSpec\n",
        "spec = NdBoundedTensorSpec(-torch.ones(3), torch.ones(3))\n",
        "base_module = nn.Linear(5, 3)\n",
        "module = TensorDictModule(module=base_module, spec=spec, in_keys=[\"obs\"], out_keys=[\"action\"], safe=True)\n",
        "tensordict = TensorDict({\"obs\": torch.randn(5)}, batch_size=[])\n",
        "module(tensordict)[\"action\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "441a1de4-e5e5-4ccf-a4a4-c7bb10e3ccc0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-1.,  1., -1.], grad_fn=<IndexPutBackward0>)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensordict = TensorDict({\"obs\": torch.randn(5)*100}, batch_size=[])\n",
        "module(tensordict)[\"action\"]  # safe=True projects the result within the set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "9ca25cc1-56bc-4e77-9feb-9298435042b9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([3]), dtype=torch.float32),\n",
              "        obs: Tensor(torch.Size([5]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchrl.modules import Actor\n",
        "base_module = nn.Linear(5, 3)\n",
        "actor = Actor(base_module, in_keys=[\"obs\"])\n",
        "tensordict = TensorDict({\"obs\": torch.randn(5)}, batch_size=[])\n",
        "actor(tensordict)  # action is the default value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "0ba0507a-ff43-42d5-bd4f-c25fd006c00f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([3, 2]), dtype=torch.float32),\n",
            "        input: Tensor(torch.Size([3, 5]), dtype=torch.float32),\n",
            "        loc: Tensor(torch.Size([3, 2]), dtype=torch.float32),\n",
            "        scale: Tensor(torch.Size([3, 2]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([3]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "# Probabilistic modules\n",
        "from torchrl.modules import ProbabilisticTensorDictModule\n",
        "from torchrl.data import TensorDict\n",
        "from torchrl.modules import  TanhNormal, NormalParamWrapper\n",
        "td = TensorDict({\"input\": torch.randn(3, 5)}, [3,])\n",
        "net = NormalParamWrapper(nn.Linear(5, 4))  # splits the output in loc and scale\n",
        "module = TensorDictModule(net, in_keys=[\"input\"], out_keys=[\"loc\", \"scale\"])\n",
        "td_module = ProbabilisticTensorDictModule(\n",
        "   module=module,\n",
        "   dist_param_keys=[\"loc\", \"scale\"],\n",
        "   out_key_sample=[\"action\"],\n",
        "   distribution_class=TanhNormal,\n",
        "   return_log_prob=False,\n",
        ")\n",
        "td_module(td)\n",
        "print(td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "a0a6dc50-a11c-408f-ae06-7c83795a8353",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([3, 2]), dtype=torch.float32),\n",
            "        input: Tensor(torch.Size([3, 5]), dtype=torch.float32),\n",
            "        loc: Tensor(torch.Size([3, 2]), dtype=torch.float32),\n",
            "        sample_log_prob: Tensor(torch.Size([3, 1]), dtype=torch.float32),\n",
            "        scale: Tensor(torch.Size([3, 2]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([3]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "# returning the log-probability\n",
        "td = TensorDict({\"input\": torch.randn(3, 5)}, [3,])\n",
        "td_module = ProbabilisticTensorDictModule(\n",
        "   module=module,\n",
        "   dist_param_keys=[\"loc\", \"scale\"],\n",
        "   out_key_sample=[\"action\"],\n",
        "   distribution_class=TanhNormal,\n",
        "   return_log_prob=True,\n",
        ")\n",
        "td_module(td)\n",
        "print(td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "a84857c9-8a00-4526-92e4-8b6a05646bd5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "random: tensor([[ 0.8728, -0.1335],\n",
            "        [-0.9833,  0.3497],\n",
            "        [-0.6889, -0.6433]], grad_fn=<ClampBackward1>)\n",
            "mode: tensor([[-0.1131,  0.1761],\n",
            "        [-0.3425, -0.2665],\n",
            "        [ 0.2915,  0.6207]], grad_fn=<ClampBackward1>)\n",
            "mean: tensor([[-0.1131,  0.1441],\n",
            "        [-0.2375, -0.1242],\n",
            "        [ 0.1372,  0.3810]], grad_fn=<MeanBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# Sampling vs mode / mean\n",
        "from torchrl.envs.utils import set_exploration_mode\n",
        "td = TensorDict({\"input\": torch.randn(3, 5)}, [3,])\n",
        "\n",
        "torch.manual_seed(0)\n",
        "with set_exploration_mode(\"random\"):\n",
        "    td_module(td)\n",
        "    print(\"random:\", td[\"action\"])\n",
        "    \n",
        "with set_exploration_mode(\"mode\"):\n",
        "    td_module(td)\n",
        "    print(\"mode:\", td[\"action\"])\n",
        "\n",
        "with set_exploration_mode(\"mean\"):\n",
        "    td_module(td)\n",
        "    print(\"mean:\", td[\"action\"])\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "141a232e-1472-4b7e-9d88-dfd0a19b8adf",
      "metadata": {},
      "source": [
        "## Using environments and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "384a8372-3096-4897-b03d-af638b17e452",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "total steps: 99\n",
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([100, 1]), dtype=torch.float32),\n",
            "        done: Tensor(torch.Size([100, 1]), dtype=torch.bool),\n",
            "        next_observation: Tensor(torch.Size([100, 3]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([100, 3]), dtype=torch.float32),\n",
            "        reward: Tensor(torch.Size([100, 1]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([100]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "from torchrl.envs.utils import step_tensordict\n",
        "env = GymEnv(\"Pendulum-v1\")\n",
        "\n",
        "action_spec = env.action_spec\n",
        "actor_module = nn.Linear(3, 1)\n",
        "actor = TensorDictModule(actor_module, spec=action_spec, in_keys=[\"observation\"], out_keys=[\"action\"])\n",
        "\n",
        "torch.manual_seed(0)\n",
        "env.set_seed(0)\n",
        "\n",
        "max_steps = 100\n",
        "tensordict = env.reset()\n",
        "tensordicts = TensorDict({}, [max_steps])\n",
        "for i in range(max_steps):\n",
        "    actor(tensordict)\n",
        "    tensordicts[i] = env.step(tensordict)\n",
        "    tensordict = step_tensordict(tensordict)  # roughly equivalent to obs = next_obs\n",
        "    if env.is_done:\n",
        "        break\n",
        "\n",
        "tensordicts_prealloc = tensordicts.clone()\n",
        "print(\"total steps:\", i)\n",
        "print(tensordicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "71a2f7e7-815d-4e1c-bd8c-4b4942f3de7d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total steps: 99\n",
            "LazyStackedTensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([100, 1]), dtype=torch.float32),\n",
            "        done: Tensor(torch.Size([100, 1]), dtype=torch.bool),\n",
            "        next_observation: Tensor(torch.Size([100, 3]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([100, 3]), dtype=torch.float32),\n",
            "        reward: Tensor(torch.Size([100, 1]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([100]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "# equivalent\n",
        "torch.manual_seed(0)\n",
        "env.set_seed(0)\n",
        "\n",
        "max_steps = 100\n",
        "tensordict = env.reset()\n",
        "tensordicts = []\n",
        "for i in range(max_steps):\n",
        "    actor(tensordict)\n",
        "    tensordicts.append(env.step(tensordict))\n",
        "    tensordict = step_tensordict(tensordict)  # roughly equivalent to obs = next_obs\n",
        "    if env.is_done:\n",
        "        break\n",
        "tensordicts_stack = torch.stack(tensordicts, 0)\n",
        "print(\"total steps:\", i)\n",
        "print(tensordicts_stack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "5380a357-dcb9-43a8-8a2e-f4be939db91f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(tensordicts_stack == tensordicts_prealloc).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "ac59466c-1e39-4ecd-a840-72d5ec204b2d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([100, 1]), dtype=torch.float32),\n",
              "        done: Tensor(torch.Size([100, 1]), dtype=torch.bool),\n",
              "        next_observation: Tensor(torch.Size([100, 3]), dtype=torch.float32),\n",
              "        observation: Tensor(torch.Size([100, 3]), dtype=torch.float32),\n",
              "        reward: Tensor(torch.Size([100, 1]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([100]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# helper\n",
        "torch.manual_seed(0)\n",
        "env.set_seed(0)\n",
        "tensordict_rollout = env.rollout(policy=actor, max_steps=max_steps)\n",
        "tensordict_rollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "7c7d8600-ecbb-4a55-b266-ed929f5d38c8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(tensordict_rollout == tensordicts_prealloc).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f8ef53-4c35-44fe-8763-792d9c237440",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a0ae640d-777a-4aed-9c1d-0638d933afc9",
      "metadata": {},
      "source": [
        "## Collectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "02cfd1d3-150b-4430-8392-f8a629beb42d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n"
          ]
        }
      ],
      "source": [
        "from torchrl.envs import ParallelEnv, EnvCreator\n",
        "from torchrl.envs.libs.gym import GymEnv\n",
        "from torchrl.modules import TensorDictModule\n",
        "from torchrl.collectors import MultiSyncDataCollector, MultiaSyncDataCollector\n",
        "from torch import nn\n",
        "\n",
        "# EnvCreator makes sure that we can send a lambda function from process to process\n",
        "parallel_env = ParallelEnv(3, EnvCreator(lambda: GymEnv(\"Pendulum-v1\")))\n",
        "create_env_fn=[parallel_env, parallel_env]\n",
        "\n",
        "actor_module = nn.Linear(3, 1)\n",
        "actor = TensorDictModule(actor_module, in_keys=[\"observation\"], out_keys=[\"action\"])\n",
        "\n",
        "# Sync data collector\n",
        "devices = [\"cpu\", \"cpu\"]\n",
        "\n",
        "collector = MultiSyncDataCollector(\n",
        "    create_env_fn=create_env_fn,  # either a list of functions or a ParallelEnv\n",
        "    policy=actor,\n",
        "    total_frames=240,\n",
        "    max_frames_per_traj=-1,  # envs are terminating, we don't need to stop them early \n",
        "    frames_per_batch=60,  # we want 60 frames at a time (we have 3 envs per sub-collector)\n",
        "    passing_devices=devices,  # len must match len of env created\n",
        "    devices=devices,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "fe6091f2-2b33-4834-b437-fb8860b166f8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([6, 10, 1]), dtype=torch.float32),\n",
            "        done: Tensor(torch.Size([6, 10, 1]), dtype=torch.bool),\n",
            "        mask: Tensor(torch.Size([6, 10, 1]), dtype=torch.bool),\n",
            "        next_observation: Tensor(torch.Size([6, 10, 3]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([6, 10, 3]), dtype=torch.float32),\n",
            "        reward: Tensor(torch.Size([6, 10, 1]), dtype=torch.float32),\n",
            "        step_count: Tensor(torch.Size([6, 10, 1]), dtype=torch.int32),\n",
            "        traj_ids: Tensor(torch.Size([6, 10, 1]), dtype=torch.int64)},\n",
            "    batch_size=torch.Size([6, 10]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "for i, d in enumerate(collector):\n",
        "    if i == 0:\n",
        "        print(d)  # trajectories are split automatically in [6 workers x 10 steps]\n",
        "    collector.update_policy_weights_()  # make sure that our policies have the latest weights if working on multiple devices\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "b6a2e699-0d13-406e-84a4-62caf236f4ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([3, 20, 1]), dtype=torch.float32),\n",
            "        done: Tensor(torch.Size([3, 20, 1]), dtype=torch.bool),\n",
            "        mask: Tensor(torch.Size([3, 20, 1]), dtype=torch.bool),\n",
            "        next_observation: Tensor(torch.Size([3, 20, 3]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([3, 20, 3]), dtype=torch.float32),\n",
            "        reward: Tensor(torch.Size([3, 20, 1]), dtype=torch.float32),\n",
            "        step_count: Tensor(torch.Size([3, 20, 1]), dtype=torch.int32),\n",
            "        traj_ids: Tensor(torch.Size([3, 20, 1]), dtype=torch.int64)},\n",
            "    batch_size=torch.Size([3, 20]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "3\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n",
            "Discarding frameskip arg. This will be taken care of by TorchRL env wrapper.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# async data collector: keeps working while you update your model\n",
        "collector = MultiaSyncDataCollector(\n",
        "    create_env_fn=create_env_fn,  # either a list of functions or a ParallelEnv\n",
        "    policy=actor,\n",
        "    total_frames=240,\n",
        "    max_frames_per_traj=-1,  # envs are terminating, we don't need to stop them early \n",
        "    frames_per_batch=60,  # we want 60 frames at a time (we have 3 envs per sub-collector)\n",
        "    passing_devices=devices,  # len must match len of env created\n",
        "    devices=devices,\n",
        ")\n",
        "\n",
        "for i, d in enumerate(collector):\n",
        "    if i == 0:\n",
        "        print(d)  # trajectories are split automatically in [6 workers x 10 steps]\n",
        "    collector.update_policy_weights_()  # make sure that our policies have the latest weights if working on multiple devices\n",
        "print(i)\n",
        "del collector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cb3140a-d335-4a03-835a-0feba8b2581c",
      "metadata": {},
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "39794b46-f82d-4cd0-9121-d8fee34d352d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TorchRL delivers meta-RL compatible loss functions\n",
        "# Disclaimer: This APi may change in the future\n",
        "\n",
        "from torchrl.objectives import DDPGLoss\n",
        "from torchrl.data import TensorDict\n",
        "from torchrl.modules import TensorDictModule\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "actor_module = nn.Linear(3, 1)\n",
        "actor = TensorDictModule(actor_module, in_keys=[\"observation\"], out_keys=[\"action\"])\n",
        "\n",
        "class ConcatModule(nn.Linear):\n",
        "    def forward(self, obs, action):\n",
        "        return super().forward(torch.cat([obs, action], -1))\n",
        "\n",
        "value_module = ConcatModule(4, 1)\n",
        "value = TensorDictModule(value_module, in_keys=[\"observation\", \"action\"], out_keys=[\"state_action_value\"])\n",
        "\n",
        "loss_fn = DDPGLoss(actor, value, gamma=0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "78b5e1ea-eed0-48d7-b979-4e88efc4ff67",
      "metadata": {},
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\n",
        "    \"observation\": torch.randn(10, 3), \n",
        "    \"next_observation\": torch.randn(10, 3),\n",
        "    \"reward\": torch.randn(10, 1),\n",
        "    \"action\": torch.randn(10, 1),\n",
        "    \"done\": torch.zeros(10, 1, dtype=torch.bool),\n",
        "}, batch_size=[10])\n",
        "loss_td = loss_fn(tensordict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "ca1ac32e-f948-432b-a40a-ff5927758377",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        loss_actor: Tensor(torch.Size([1]), dtype=torch.float32),\n",
              "        loss_value: Tensor(torch.Size([1]), dtype=torch.float32),\n",
              "        pred_value: Tensor(torch.Size([1]), dtype=torch.float32),\n",
              "        pred_value_max: Tensor(torch.Size([1]), dtype=torch.float32),\n",
              "        target_value: Tensor(torch.Size([1]), dtype=torch.float32),\n",
              "        target_value_max: Tensor(torch.Size([1]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_td"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "eeacc666-42ca-4cee-9f23-c3082280dc47",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([10, 1]), dtype=torch.float32),\n",
              "        done: Tensor(torch.Size([10, 1]), dtype=torch.bool),\n",
              "        next_observation: Tensor(torch.Size([10, 3]), dtype=torch.float32),\n",
              "        observation: Tensor(torch.Size([10, 3]), dtype=torch.float32),\n",
              "        reward: Tensor(torch.Size([10, 1]), dtype=torch.float32),\n",
              "        td_error: Tensor(torch.Size([10, 1]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([10]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensordict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3342145c-5a99-42b5-9564-f80f9cd14d41",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "500c3fbc-c6e6-448f-ba1a-8cb7916a12a0",
      "metadata": {},
      "source": [
        "## State of the library\n",
        "\n",
        "TorchRL is currently an **alpha-release**: there may be bugs and there is no guarantee about BC-breaking changes.\n",
        "We should be able to move to a beta-release by the end of the year. Our roadmap to get there comprises:\n",
        "- Distributed solutions\n",
        "- Offline RL\n",
        "- Greater support for meta-RL\n",
        "- Multi-task and hierarchical RL\n",
        "\n",
        "## Contributing:\n",
        "We are actively looking for contributors and early users. If you're working in RL (or just curious), try it! Give us feedback: what will make the success of TorchRL is how well it covers researchers needs. To do that, we need their input! Since the library is nascent, it is a great time for you to shape it the way you want!\n",
        "\n",
        "## Installing the library\n",
        "The library is on PyPI: \n",
        "```\n",
        "pip install torchrl\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
