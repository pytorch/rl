{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0e971c71-dc14-46db-a3a0-a71423ebece0",
      "metadata": {},
      "source": [
        "[<img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/pytorch/rl/blob/main/tutorials/multi_task.ipynb)\n",
        "\n",
        "# Task-specific policy in multi-task environments\n",
        "\n",
        "This tutorial details how multi-task policies and batched environments can be used.\n",
        "\n",
        "At the end of this tutorial, you will be capable of writing policies that can compute actions in diverse settings using a distinct set of weights.\n",
        "You will also be able to execute diverse environments in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f081673",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install functorch\n",
        "!pip install dm_control\n",
        "!pip install torchrl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b2c1c6c2-0ce0-42de-805a-b37fbccaa9dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.envs import TransformedEnv, CatTensors, Compose, DoubleToFloat, ParallelEnv\n",
        "from torchrl.envs.libs.dm_control import DMControlEnv\n",
        "from torchrl.modules import TensorDictModule, TensorDictSequential, MLP\n",
        "from torch import nn\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1619f578-870c-40e7-9d3e-5f8eee9460a0",
      "metadata": {},
      "source": [
        "We design two environments, one humanoid that must complete the stand task and another that must learn to walk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2d9cb67c-8c01-4d13-9590-cb9947c89221",
      "metadata": {},
      "outputs": [],
      "source": [
        "env1 = DMControlEnv(\"humanoid\", \"stand\")\n",
        "env1_obs_keys = list(env1.observation_spec.keys())\n",
        "env1 = TransformedEnv(\n",
        "    env1, \n",
        "    Compose(\n",
        "        CatTensors(env1_obs_keys, \"next_observation_stand\", del_keys=False),\n",
        "        CatTensors(env1_obs_keys, \"next_observation\"),\n",
        "        DoubleToFloat(keys_in=[\"next_observation_stand\", \"next_observation\"], keys_inv_in=[\"action\"]),\n",
        "    )\n",
        ")\n",
        "env2 = DMControlEnv(\"humanoid\", \"walk\")\n",
        "env2_obs_keys = list(env2.observation_spec.keys())\n",
        "env2 = TransformedEnv(\n",
        "    env2, \n",
        "    Compose(\n",
        "        CatTensors(env2_obs_keys, \"next_observation_walk\", del_keys=False),\n",
        "        CatTensors(env2_obs_keys, \"next_observation\"),\n",
        "        DoubleToFloat(keys_in=[\"next_observation_walk\", \"next_observation\"], keys_inv_in=[\"action\"]),\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a1327fde-306b-4cd0-9954-caf88bfa2e82",
      "metadata": {},
      "outputs": [],
      "source": [
        "tdreset1 = env1.reset()\n",
        "tdreset2 = env2.reset()\n",
        "\n",
        "# In TorchRL, stacking is done in a lazy manner: the original tensordicts can still be recovered by indexing the main tensordict\n",
        "tdreset = torch.stack([tdreset1, tdreset2], 0)\n",
        "assert tdreset[0] is tdreset1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f9551947-7b2a-4fb7-8fd5-814fefa6c081",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
            "        observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation_stand: Tensor(torch.Size([67]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "print(tdreset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47eb28e-851a-4a33-82cc-d222197df4f4",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7b0ffc6d-3cdb-4761-a527-d0974399895d",
      "metadata": {},
      "source": [
        "## Policy\n",
        "We will design a policy where a backbone reads the \"observation\" key. Then specific sub-components will ready the \"observation_stand\" and \"observation_walk\" keys of the stacked tensordicts, if they are present, and pass them through the dedicated sub-network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5ea2c09a-3420-4323-9d60-50ed05fe778e",
      "metadata": {},
      "outputs": [],
      "source": [
        "action_dim = env1.action_spec.shape[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "09a55836-a473-45ed-86c7-feae2d95771a",
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_common = TensorDictModule(nn.Linear(67, 64), in_keys=[\"observation\"], out_keys=[\"hidden\"])\n",
        "policy_stand = TensorDictModule(MLP(67 + 64, action_dim, depth=2), in_keys=[\"observation_stand\", \"hidden\"], out_keys=[\"action\"])\n",
        "policy_walk = TensorDictModule(MLP(67 + 64, action_dim, depth=2), in_keys=[\"observation_walk\", \"hidden\"], out_keys=[\"action\"])\n",
        "seq = TensorDictSequential(policy_common, policy_stand, policy_walk, partial_tolerant=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a2650046-86bb-4dda-a041-7de52ac133a4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([21]), dtype=torch.float32),\n",
              "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
              "        hidden: Tensor(torch.Size([64]), dtype=torch.float32),\n",
              "        observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
              "        observation_stand: Tensor(torch.Size([67]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's check that our sequence outputs actions for a single env (stand)\n",
        "seq(env1.reset())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "87006ff8-69b5-43db-938f-dd54a08ca027",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([21]), dtype=torch.float32),\n",
              "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
              "        hidden: Tensor(torch.Size([64]), dtype=torch.float32),\n",
              "        observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
              "        observation_walk: Tensor(torch.Size([67]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's check that our sequence outputs actions for a single env (walk)\n",
        "seq(env2.reset())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "563281b8-8aa8-429c-b3d3-10183bbb4c6c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LazyStackedTensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([2, 21]), dtype=torch.float32),\n",
              "        done: Tensor(torch.Size([2, 1]), dtype=torch.bool),\n",
              "        hidden: Tensor(torch.Size([2, 64]), dtype=torch.float32),\n",
              "        observation: Tensor(torch.Size([2, 67]), dtype=torch.float32)},\n",
              "    batch_size=torch.Size([2]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# also works with the stack: now the stand and walk keys have disappeared (because they're not shared by all tensordicts). But the TensorDictSequential still performed the operations.\n",
        "# Note that the backbone was executed in a vectorized way (not in a loop) which is more efficient.\n",
        "seq(tdreset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97faa452-b867-410a-9186-084ed5b86316",
      "metadata": {},
      "source": [
        "## Executing diverse tasks in parallel\n",
        "\n",
        "We can parallelize the operations if the common keys-value pairs share the same specs (in particular their shape and dtype must match: you can't do the following if the observation shapes are different but are pointed to by the same key).\n",
        "\n",
        "If ParallelEnv receives a single env making function, it will assume that a single task has to be performed. If a list of functions is provided, then it will assume that we are in a multi-task setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d642ecb3-69a5-4bbc-a3d7-33456cc96a7a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LazyStackedTensorDict(\n",
            "    fields={\n",
            "        done: Tensor(torch.Size([2, 1]), dtype=torch.bool),\n",
            "        observation: Tensor(torch.Size([2, 67]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([2]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "TensorDict(\n",
            "    fields={\n",
            "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
            "        next_observation_stand: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation_stand: Tensor(torch.Size([67]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "TensorDict(\n",
            "    fields={\n",
            "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
            "        next_observation_walk: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation_walk: Tensor(torch.Size([67]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "env1_maker = lambda: TransformedEnv(\n",
        "    DMControlEnv(\"humanoid\", \"stand\"), \n",
        "    Compose(\n",
        "        CatTensors(env1_obs_keys, \"next_observation_stand\", del_keys=False),\n",
        "        CatTensors(env1_obs_keys, \"next_observation\"),\n",
        "        DoubleToFloat(keys_in=[\"next_observation_stand\", \"next_observation\"], keys_inv_in=[\"action\"]),\n",
        "    )\n",
        ")\n",
        "env2_maker = lambda: TransformedEnv(\n",
        "    DMControlEnv(\"humanoid\", \"walk\"), \n",
        "    Compose(\n",
        "        CatTensors(env2_obs_keys, \"next_observation_walk\", del_keys=False),\n",
        "        CatTensors(env2_obs_keys, \"next_observation\"),\n",
        "        DoubleToFloat(keys_in=[\"next_observation_walk\", \"next_observation\"], keys_inv_in=[\"action\"]),\n",
        "    )\n",
        ")\n",
        "env = ParallelEnv(2, [env1_maker, env2_maker])\n",
        "assert not env._single_task\n",
        "\n",
        "tdreset = env.reset()\n",
        "print(tdreset)\n",
        "print(tdreset[0])\n",
        "print(tdreset[1])  # should be different\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f95b625-d571-473a-ae1e-5e7cb4cbcd3b",
      "metadata": {},
      "source": [
        "Let's pass the output through our network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "31758554-623f-4df0-99e3-ee78411ee92f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LazyStackedTensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([2, 21]), dtype=torch.float32),\n",
            "        done: Tensor(torch.Size([2, 1]), dtype=torch.bool),\n",
            "        hidden: Tensor(torch.Size([2, 64]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([2, 67]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([2]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([21]), dtype=torch.float32),\n",
            "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
            "        hidden: Tensor(torch.Size([64]), dtype=torch.float32),\n",
            "        next_observation_stand: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation_stand: Tensor(torch.Size([67]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([21]), dtype=torch.float32),\n",
            "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
            "        hidden: Tensor(torch.Size([64]), dtype=torch.float32),\n",
            "        next_observation_walk: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation_walk: Tensor(torch.Size([67]), dtype=torch.float32)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "tdreset = seq(tdreset)\n",
        "print(tdreset)\n",
        "print(tdreset[0])\n",
        "print(tdreset[1])  # should be different but all have an \"action\" key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "054f3be1-7474-43f1-9460-665e72cff965",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LazyStackedTensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([2, 21]), dtype=torch.float32),\n",
            "        done: Tensor(torch.Size([2, 1]), dtype=torch.bool),\n",
            "        hidden: Tensor(torch.Size([2, 64]), dtype=torch.float32),\n",
            "        next_observation: Tensor(torch.Size([2, 67]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([2, 67]), dtype=torch.float32),\n",
            "        reward: Tensor(torch.Size([2, 1]), dtype=torch.float64)},\n",
            "    batch_size=torch.Size([2]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([21]), dtype=torch.float32),\n",
            "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
            "        hidden: Tensor(torch.Size([64]), dtype=torch.float32),\n",
            "        next_observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        next_observation_stand: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation_stand: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        reward: Tensor(torch.Size([1]), dtype=torch.float64)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n",
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(torch.Size([21]), dtype=torch.float32),\n",
            "        done: Tensor(torch.Size([1]), dtype=torch.bool),\n",
            "        hidden: Tensor(torch.Size([64]), dtype=torch.float32),\n",
            "        next_observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        next_observation_walk: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        observation_walk: Tensor(torch.Size([67]), dtype=torch.float32),\n",
            "        reward: Tensor(torch.Size([1]), dtype=torch.float64)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=cpu,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "env.step(tdreset)  # computes actions and execute steps in parallel\n",
        "print(tdreset)\n",
        "print(tdreset[0])\n",
        "print(tdreset[1])  # next_observation has now been written"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1295b976-369b-433b-9e5b-fb66d6d9e884",
      "metadata": {},
      "source": [
        "## Rollout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d036a3f4-3bc7-4aa2-918f-636c302d1147",
      "metadata": {},
      "outputs": [],
      "source": [
        "td_rollout = env.rollout(100, policy=seq, return_contiguous=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a940db50-689a-4112-9916-b3438fff50c4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LazyStackedTensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([2, 21]), dtype=torch.float32),\n",
              "        done: Tensor(torch.Size([2, 1]), dtype=torch.bool),\n",
              "        hidden: Tensor(torch.Size([2, 64]), dtype=torch.float32),\n",
              "        next_observation: Tensor(torch.Size([2, 67]), dtype=torch.float32),\n",
              "        observation: Tensor(torch.Size([2, 67]), dtype=torch.float32),\n",
              "        reward: Tensor(torch.Size([2, 1]), dtype=torch.float64)},\n",
              "    batch_size=torch.Size([2]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "td_rollout[:, 0] # tensordict of the first step: only the common keys are shown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "466a7cf6-e20f-430a-84cc-03686e4bb6e8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LazyStackedTensorDict(\n",
              "    fields={\n",
              "        action: Tensor(torch.Size([100, 21]), dtype=torch.float32),\n",
              "        done: Tensor(torch.Size([100, 1]), dtype=torch.bool),\n",
              "        hidden: Tensor(torch.Size([100, 64]), dtype=torch.float32),\n",
              "        next_observation: Tensor(torch.Size([100, 67]), dtype=torch.float32),\n",
              "        next_observation_stand: Tensor(torch.Size([100, 67]), dtype=torch.float32),\n",
              "        observation: Tensor(torch.Size([100, 67]), dtype=torch.float32),\n",
              "        observation_stand: Tensor(torch.Size([100, 67]), dtype=torch.float32),\n",
              "        reward: Tensor(torch.Size([100, 1]), dtype=torch.float64)},\n",
              "    batch_size=torch.Size([100]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "td_rollout[0] # tensordict of the first env: the stand obs is present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d677f2a5-f9f7-4764-b596-ef0087ffb4a0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
