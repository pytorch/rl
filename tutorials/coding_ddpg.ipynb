{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Coding DDPG using TorchRL\n",
    "\n",
    "This tutorial will guide you through the steps to code DDPG from scratch.\n",
    "DDPG ([Deep Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971)) is a simple continuous control algorithm. It essentially consists in learning a parametric value function for an action-observation pair, and then learning a policy that outputs actions that maximise this value function given a certain observation.\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "- how to build an environment in TorchRL, including transforms (e.g. data normalization) and parallel execution;\n",
    "- how to design a policy and value network;\n",
    "- how to collect data from your environment efficiently and store them in a replay buffer;\n",
    "- how to store trajectories (and not transitions) in your replay buffer);\n",
    "- and finally how to evaluate your model.\n",
    "\n",
    "This tutorial assumes the reader is familiar with some of TorchRL primitives, such as `TensorDict` and `TensorDictModules`, although it should be sufficiently transparent to be understood without a deep understanding of these classes.\n",
    "\n",
    "We do not aim at giving a SOTA implementation of the algorithm, but rather to provide a high-level illustration of TorchRL features in the context of this algorithm."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make all the necessary imports for training\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.cuda\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from torchrl.collectors import MultiaSyncDataCollector\n",
    "from torchrl.data import CompositeSpec\n",
    "from torchrl.data import (\n",
    "    TensorDictPrioritizedReplayBuffer,\n",
    "    TensorDictReplayBuffer,\n",
    ")\n",
    "from torchrl.data.postprocs import MultiStep\n",
    "from torchrl.data.replay_buffers.storages import LazyMemmapStorage\n",
    "from torchrl.envs import (\n",
    "    ParallelEnv,\n",
    "    EnvCreator,\n",
    "    DMControlEnv,\n",
    "    GymEnv,\n",
    "    CatTensors,\n",
    "    ObservationNorm,\n",
    "    DoubleToFloat,\n",
    ")\n",
    "from torchrl.envs.transforms import RewardScaling, TransformedEnv\n",
    "from torchrl.envs.utils import set_exploration_mode, step_tensordict\n",
    "from torchrl.modules import (\n",
    "    OrnsteinUhlenbeckProcessWrapper,\n",
    "    MLP,\n",
    "    TensorDictModule,\n",
    "    ProbabilisticActor,\n",
    "    ValueOperator,\n",
    ")\n",
    "from torchrl.modules.distributions.continuous import TanhDelta\n",
    "from torchrl.objectives.costs.utils import hold_out_net\n",
    "from torchrl.trainers import Recorder\n",
    "from torchrl.trainers.helpers.envs import (\n",
    "    get_stats_random_rollout,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Environment\n",
    "\n",
    "Let us start by building the environment.\n",
    "\n",
    "For this example, we will be using the cheetah task. The goal is to make a half-cheetah run as fast as possible.\n",
    "\n",
    "In TorchRL, one can create such a task by relying on dm_control or gym:\n",
    "\n",
    "```python\n",
    "env = GymEnv(\"HalfCheetah-v4\")\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```python\n",
    "env = DMControlEnv(\"cheetah\", \"run\")\n",
    "```\n",
    "\n",
    "We only consider the state-based environment, but if one wishes to use a pixel-based environment, this can be done via the keyword argument `from_pixels=True` which is passed when calling `GymEnv` or `DMControlEnv`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"\n",
    "    Create a base env\n",
    "    \"\"\"\n",
    "    global env_library\n",
    "    global env_name\n",
    "\n",
    "    if backend == \"dm_control\":\n",
    "        env_name = \"cheetah\"\n",
    "        env_task = \"run\"\n",
    "        env_args = (env_name, env_task)\n",
    "        env_library = DMControlEnv\n",
    "    elif backend == \"gym\":\n",
    "        env_name = \"HalfCheetah-v4\"\n",
    "        env_args = (env_name, )\n",
    "        env_library = GymEnv\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "    env_kwargs = {\n",
    "        \"device\": device,\n",
    "        \"frame_skip\": frame_skip,\n",
    "        \"from_pixels\": from_pixels,\n",
    "        \"pixels_only\": from_pixels,\n",
    "    }\n",
    "    env = env_library(*env_args, **env_kwargs)\n",
    "    return env\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transforms\n",
    "\n",
    "Now that we have a base environment, we may want to modify its representation to make it more policy-friendly.\n",
    "\n",
    "It is common in DDPG to rescale the reward using some heuristic value. We will multiply the reward by 5 in this example.\n",
    "\n",
    "If we are using dm_control, it is important also to transform the actions to double precision numbers as this is the dtype expected by the library.\n",
    "\n",
    "We also leave the possibility to normalize the states: we will take care of computing the normalizing constants later on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def make_transformed_env(\n",
    "    env, stats=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply transforms to the env (such as reward scaling and state normalization)\n",
    "    \"\"\"\n",
    "\n",
    "    env = TransformedEnv(env)\n",
    "    \n",
    "    #Â we append transforms one by one, although we might as well create the transformed environment using the `env = TransformedEnv(base_env, transforms)` syntax.\n",
    "    env.append_transform(RewardScaling(loc=0.0, scale=reward_scaling))\n",
    "\n",
    "    double_to_float_list = []\n",
    "    double_to_float_inv_list = []\n",
    "    if env_library is DMControlEnv:\n",
    "        # DMControl requires double-precision\n",
    "        double_to_float_list += [\n",
    "            \"reward\",\n",
    "        ]\n",
    "        double_to_float_inv_list += [\"action\"]\n",
    "    \n",
    "    \n",
    "    # We concatenate all states into a single \"next_observation_vector\"\n",
    "    # even if there is a single tensor, it'll be renamed in \"next_observation_vector\". \n",
    "    # This facilitates the downstream operations as we know the name of the output tensor.\n",
    "    # In some environments (not half-cheetah), there may be more than one observation vector: in this case this code snippet will concatenate them all.\n",
    "    selected_keys = list(env.observation_spec.keys())\n",
    "    out_key = \"next_observation_vector\"\n",
    "    env.append_transform(CatTensors(keys_in=selected_keys, out_key=out_key))\n",
    "\n",
    "    #  we normalize the states\n",
    "    if stats is None:\n",
    "        _stats = {\"loc\": 0.0, \"scale\": 1.0}\n",
    "    else:\n",
    "        _stats = stats\n",
    "    env.append_transform(\n",
    "        ObservationNorm(**_stats, keys_in=[out_key], standard_normal=True)\n",
    "    )\n",
    "\n",
    "    double_to_float_list.append(out_key)\n",
    "    env.append_transform(\n",
    "        DoubleToFloat(\n",
    "            keys_in=double_to_float_list, keys_inv_in=double_to_float_inv_list\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "    return env\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parallel execution\n",
    "\n",
    "The following helper function allows us to run environments in parallel. One can choose between running each base env in a separate process and execute the transform in the main process, or execute the transforms in parallel.\n",
    "To leverage the vectorization capabilities of PyTorch, we adopt the first method:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parallel_env_constructor(\n",
    "    stats,\n",
    "    **env_kwargs,\n",
    "):\n",
    "    if env_per_collector == 1:\n",
    "        env_creator = EnvCreator(\n",
    "            lambda: make_transformed_env(make_env(), stats, **env_kwargs)\n",
    "        )\n",
    "        return env_creator\n",
    "\n",
    "    parallel_env = ParallelEnv(\n",
    "        num_workers=env_per_collector,\n",
    "        create_env_fn=EnvCreator(lambda: make_env()),\n",
    "        create_env_kwargs=None,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    env = make_transformed_env(parallel_env, stats, **env_kwargs)\n",
    "    return env\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Normalization of the observations\n",
    "\n",
    "To compute the normalizing statistics, we run an arbitrary number of random steps in the environment and compute the mean and standard deviation of the collected observations:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_stats_random_rollout(\n",
    "    proof_environment, key: Optional[str] = None\n",
    "):\n",
    "    print(\"computing state stats\")\n",
    "    n = 0\n",
    "    td_stats = []\n",
    "    while n < init_env_steps:\n",
    "        _td_stats = proof_environment.rollout(max_steps=init_env_steps)\n",
    "        n += _td_stats.numel()\n",
    "        _td_stats_select = _td_stats.to_tensordict().select(key).cpu()\n",
    "        if not len(list(_td_stats_select.keys())):\n",
    "            raise RuntimeError(\n",
    "                f\"key {key} not found in tensordict with keys {list(_td_stats.keys())}\"\n",
    "            )\n",
    "        td_stats.append(_td_stats_select)\n",
    "        del _td_stats, _td_stats_select\n",
    "    td_stats = torch.cat(td_stats, 0)\n",
    "\n",
    "    if key is None:\n",
    "        keyset_seedlist(proof_environment.observation_spec.keys())\n",
    "        key = keys.pop()\n",
    "        if len(keys):\n",
    "            raise RuntimeError(\n",
    "                f\"More than one key exists in the observation_specs: {[key] + keys} were found, \"\n",
    "                \"thus get_stats_random_rollout cannot infer which to compute the stats of.\"\n",
    "            )\n",
    "\n",
    "    m = td_stats.get(key).mean(dim=0)\n",
    "    s = td_stats.get(key).std(dim=0)\n",
    "    m[s == 0] = 0.0\n",
    "    s[s == 0] = 1.0\n",
    "\n",
    "    print(\n",
    "        f\"stats computed for {td_stats.numel()} steps. Got: \\n\"\n",
    "        f\"loc = {m}, \\n\"\n",
    "        f\"scale: {s}\"\n",
    "    )\n",
    "    if not torch.isfinite(m).all():\n",
    "        raise RuntimeError(\"non-finite values found in mean\")\n",
    "    if not torch.isfinite(s).all():\n",
    "        raise RuntimeError(\"non-finite values found in sd\")\n",
    "    stats = {\"loc\": m, \"scale\": s}\n",
    "    return stats\n",
    "\n",
    "\n",
    "def get_env_stats():\n",
    "    \"\"\"\n",
    "    Gets the stats of an environment\n",
    "    \"\"\"\n",
    "    proof_env = make_transformed_env(make_env(), None)\n",
    "    proof_env.set_seed(seed)\n",
    "    stats = get_stats_random_rollout(\n",
    "        proof_env, key=\"next_observation_vector\",\n",
    "    )\n",
    "    # make sure proof_env is closed\n",
    "    proof_env.close()\n",
    "    return stats\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building the model\n",
    "\n",
    "Let us now build the DDPG actor and QValue network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_ddpg_actor(\n",
    "    stats,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    proof_environment = make_transformed_env(make_env(), stats)\n",
    "\n",
    "\n",
    "    env_specs = proof_environment.specs\n",
    "    out_features = env_specs[\"action_spec\"].shape[0]\n",
    "\n",
    "    actor_net = MLP(\n",
    "        num_cells=[num_cells] * num_layers,\n",
    "        activation_class=nn.Tanh,\n",
    "        out_features=out_features,\n",
    "    )\n",
    "    in_keys = [\"observation_vector\"]\n",
    "    out_keys = [\"param\"]\n",
    "\n",
    "    actor_module = TensorDictModule(actor_net, in_keys=in_keys, out_keys=out_keys)\n",
    "\n",
    "    # We use a ProbabilisticActor to make sure that we map the network output\n",
    "    # to the right space using a TanhDelta distribution.\n",
    "    actor = ProbabilisticActor(\n",
    "        module=actor_module,\n",
    "        dist_param_keys=[\"param\"],\n",
    "        spec=CompositeSpec(action=env_specs[\"action_spec\"]),\n",
    "        safe=True,\n",
    "        distribution_class=TanhDelta,\n",
    "        distribution_kwargs={\n",
    "            \"min\": env_specs[\"action_spec\"].space.minimum,\n",
    "            \"max\": env_specs[\"action_spec\"].space.maximum,\n",
    "        },\n",
    "    ).to(device)\n",
    "\n",
    "    q_net = MLP(\n",
    "        num_cells=[num_cells] * num_layers,\n",
    "        activation_class=nn.Tanh,\n",
    "        out_features=1,\n",
    "    )\n",
    "\n",
    "    in_keys = in_keys + [\"action\"]\n",
    "    qnet = ValueOperator(\n",
    "        in_keys=in_keys,\n",
    "        module=q_net,\n",
    "    ).to(device)\n",
    "\n",
    "    # init: since we have lazy layers, we should run the network once to initialize them\n",
    "    with torch.no_grad(), set_exploration_mode(\"random\"):\n",
    "        td = proof_environment.rollout(max_steps=1000)\n",
    "        td = td.to(device)\n",
    "        actor(td)\n",
    "        qnet(td)\n",
    "\n",
    "    return actor, qnet\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluator: building your recorder object\n",
    "\n",
    "As the training data is obtained using some exploration strategy, the true performance of our algorithm needs to be assessed in deterministic mode. We do this using a dedicated class, `Recorder`, which executes the policy in the environment at a given frequency and returns some statistics obtained from these simulations.\n",
    "The following helper function builds this object:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_recorder(actor_model_explore, stats):\n",
    "    base_env = make_env()\n",
    "    recorder = make_transformed_env(base_env, stats)\n",
    "    \n",
    "    recorder_obj = Recorder(\n",
    "        record_frames=1000,\n",
    "        frame_skip=frame_skip,\n",
    "        policy_exploration=actor_model_explore,\n",
    "        recorder=recorder,\n",
    "        exploration_mode=\"mean\",\n",
    "        record_interval=record_interval,\n",
    "    )\n",
    "    return recorder_obj\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Replay buffer\n",
    "\n",
    "Replay buffers come in two flavours: prioritized (where some error signal is used to give a higher likelihood of sampling to some items than others) and regular, circular experience replay.\n",
    "\n",
    "We also provide a special storage, names LazyMemmapStorage, that will store tensors on physical memory using a memory-mapped array. The following function takes care of creating the replay buffer with the desired hyperparameters:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_replay_buffer(make_replay_buffer=3):\n",
    "    if prb:\n",
    "        replay_buffer = TensorDictPrioritizedReplayBuffer(\n",
    "                buffer_size,\n",
    "                alpha=0.7,\n",
    "                beta=0.5,\n",
    "                collate_fn=lambda x: x,\n",
    "                pin_memory=False,\n",
    "                prefetch=make_replay_buffer,\n",
    "                storage=LazyMemmapStorage(\n",
    "                    buffer_size,\n",
    "                    scratch_dir=buffer_scratch_dir,\n",
    "                    device=device,\n",
    "                ),\n",
    "            )\n",
    "    else:\n",
    "        replay_buffer = TensorDictReplayBuffer(\n",
    "                buffer_size,\n",
    "                collate_fn=lambda x: x,\n",
    "                pin_memory=False,\n",
    "                prefetch=make_replay_buffer,\n",
    "                storage=LazyMemmapStorage(\n",
    "                    buffer_size,\n",
    "                    scratch_dir=buffer_scratch_dir,\n",
    "                    device=device,\n",
    "                ),\n",
    "            )\n",
    "    return replay_buffer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters\n",
    "After having written all our helper functions, it is now time to set the experiment hyperparameters:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "backend = \"dm_control\"  # or \"gym\" \n",
    "frame_skip = 2  # if this value is changed, the number of frames collected etc. need to be adjusted\n",
    "from_pixels = False\n",
    "reward_scaling = 5.0\n",
    "\n",
    "# execute on cuda if available\n",
    "device = (\n",
    "    torch.device(\"cpu\")\n",
    "    if torch.cuda.device_count() == 0\n",
    "    else torch.device(\"cuda:0\")\n",
    ")\n",
    "\n",
    "init_env_steps = 1000  # number of random steps used as for stats computation\n",
    "env_per_collector = 2  # number of environments in each data collector\n",
    "\n",
    "env_library = None  # overwritten because global in env maker\n",
    "env_name = None  # overwritten because global in env maker\n",
    "\n",
    "exp_name = \"cheetah\"\n",
    "annealing_frames = 1000000 // frame_skip  # Number of frames before OU noise becomes null\n",
    "lr=5e-4\n",
    "weight_decay = 0.0\n",
    "total_frames = 1000000 // frame_skip\n",
    "init_random_frames = 5000 // frame_skip   # Number of random frames used as warm-up\n",
    "optim_steps_per_batch = 32  # Number of iterations of the inner loop\n",
    "batch_size = 128\n",
    "frames_per_batch = 1000 // frame_skip    # Number of frames returned by the collector at each iteration of the outer loop\n",
    "gamma = 0.99\n",
    "tau = 0.005    # Decay factor for the target network\n",
    "prb = True    # If True, a Prioritized replay buffer will be used\n",
    "buffer_size = 1000000 // frame_skip    # Number of frames stored in the buffer\n",
    "buffer_scratch_dir = \"/tmp/\"\n",
    "n_steps_forward = 3\n",
    "\n",
    "record_interval = 10  # record every 10 batch collected\n",
    "\n",
    "# Network specs\n",
    "num_cells = 64\n",
    "num_layers = 2\n",
    "\n",
    "seed = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization\n",
    "To initialize the experiment, we first acquire the observation statistics, then build the networks, wrap them in an exploration wrapper (following the seminal DDPG paper, we used an Ornstein-Uhlenbeck process to add noise to the sampled actions)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# get stats for normalization\n",
    "stats = get_env_stats()\n",
    "\n",
    "# Actor and qnet instantiation\n",
    "actor, qnet = make_ddpg_actor(\n",
    "    stats=stats,\n",
    "    device=device,\n",
    ")\n",
    "if device == torch.device(\"cpu\"):\n",
    "    actor.share_memory()\n",
    "# Target network\n",
    "qnet_target = deepcopy(qnet).requires_grad_(False)\n",
    "\n",
    "# Exploration wrappers:\n",
    "actor_model_explore = OrnsteinUhlenbeckProcessWrapper(\n",
    "    actor,\n",
    "    annealing_num_steps=annealing_frames,\n",
    ").to(device)\n",
    "if device == torch.device(\"cpu\"):\n",
    "    actor_model_explore.share_memory()\n",
    "\n",
    "# Environment setting:\n",
    "create_env_fn = parallel_env_constructor(\n",
    "    stats=stats,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Â Data collector\n",
    "\n",
    "Creating the data collector is a crucial step in an RL experiment.\n",
    "TorchRL provides a couple of classes to collect data in parallel. Here we will use `MultiaSyncDataCollector`, a data collector that will be executed in an async manner (i.e. data will be collected while the policy is being optimized).\n",
    "\n",
    "The parameters to specify are: the list of environment creation functions, the policy, the total number of frames before the collector is considered empty, the maximum number of frames per trajectory (useful for non-terminating environments, like dm_control ones).\n",
    "One should also pass the number of frames in each batch collected, the number of random steps executed independently from the policy, the devices used for policy execution and data transmission.\n",
    "\n",
    "The `MultiStep` object passed as postproc makes it so that the rewards of the n upcoming steps are added (with some discount factor) and the next observation is changed to be the n-step forward observation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Batch collector:\n",
    "collector = MultiaSyncDataCollector(\n",
    "    create_env_fn=[create_env_fn, create_env_fn],\n",
    "    policy = actor_model_explore,\n",
    "    total_frames = total_frames,\n",
    "    max_frames_per_traj = 1000,\n",
    "    frames_per_batch = frames_per_batch,\n",
    "    init_random_frames = init_random_frames,\n",
    "    reset_at_each_iter = False,\n",
    "    postproc = MultiStep(n_steps_max=n_steps_forward, gamma=gamma) if n_steps_forward > 0 else None,\n",
    "    split_trajs = True,\n",
    "    devices = [device, device],  # device for execution\n",
    "    passing_devices = [device, device],  # device where data will be stored and passed\n",
    "    seed = None,\n",
    "    pin_memory = False,\n",
    "    update_at_each_batch = False,\n",
    "    exploration_mode = \"random\",\n",
    ")\n",
    "collector.set_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replay buffer:\n",
    "replay_buffer = make_replay_buffer()\n",
    "\n",
    "# trajectory recorder\n",
    "recorder = make_recorder(actor_model_explore, stats)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we will use the Adam optimizer for the policy and value network, with the same learning rate for both."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_actor = optim.Adam(\n",
    "    actor.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "optimizer_qnet = optim.Adam(\n",
    "    qnet.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "total_collection_steps = total_frames // frames_per_batch\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_actor, T_max=total_collection_steps)\n",
    "scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_qnet, T_max=total_collection_steps)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Time to train the policy!\n",
    "\n",
    "Some notes about the following cell:\n",
    "- `hold_out_net` is a TorchRL context manager that temporarily sets requires_grad to False for a set of network parameters. This is used to prevent `backward` to write gradients on parameters that need not to be differentiated given the loss at hand.\n",
    "- The value network is designed using the `ValueOperator` TensorDictModule subclass. This class will write a `\"state_action_value\"` if one of its `in_keys` is named \"action\", otherwise it will assume that only the state-value is returned and the output key will simply be `\"state_value\"`. In the case of DDPG, the value if of the state-action pair, hence the first name is used.\n",
    "- The `step_tensordict` helper function returns a new TensorDict that essentially does the `obs = next_obs`. In other words, it will return a new tensordict where the values that are related to the next state (next observations of various type) are selected and written as if they were current. This makes it possible to pass this new tensordict to the policy or value network (which expects an `\"observation_vector\"` key, not `\"next_observation_vector\"`.\n",
    "- When using prioritized replay buffer, a priority key is added to the sampled tensordict (named `\"td_error\"` by default). Then, this TensorDict will be fed back to the replay buffer using the `update_priority` method. Under the hood, this method will read the index present in the TensorDict as well as the priority value, and update its list of priorities at these indices.\n",
    "- TorchRL provides optimized versions of the loss functions (such as this one) where one only needs to pass a sampled tensordict and obtains a dictionary of losses and metadata in return (see `torchrl.objectives.costs` for more context). Here we write the full loss function in the optimization loop for transparency. Similarly, the target network updates are written explicitely but TorchRL provides a couple of dedicated classes for this (see `torchrl.objectives.SoftUpdate` and `torchrl.objectives.HardUpdate`).\n",
    "- After each collection of data, we call `collector.update_policy_weights_()`, which will update the policy network weights on the data collector. If the code is executed on cpu or with a single cuda device, this part can be ommited. If the collector is executed on another device, then its weights must be synced with those on the main, training process and this method should be incorporated in the training loop (ideally early in the loop in async settings, and at the end of it in sync settings)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rewards = []\n",
    "rewards_eval = []\n",
    "\n",
    "# Main loop\n",
    "norm_factor_training = sum(gamma**i for i in range(n_steps_forward)) if n_steps_forward else 1\n",
    "\n",
    "collected_frames = 0\n",
    "pbar = tqdm.tqdm(total=total_frames)\n",
    "r0 = None\n",
    "for i, tensordict in enumerate(collector):\n",
    "\n",
    "    # update weights of the inference policy\n",
    "    collector.update_policy_weights_()\n",
    "    \n",
    "    if r0 is None:\n",
    "        r0 = tensordict[\"reward\"].mean().item()\n",
    "    pbar.update(tensordict.numel())\n",
    "    \n",
    "    # extend the replay buffer with the new data\n",
    "    if \"mask\" in tensordict.keys():\n",
    "        # if multi-step, a mask is present to help filter padded values\n",
    "        current_frames = tensordict[\"mask\"].sum()\n",
    "        tensordict = tensordict[tensordict.get(\"mask\").squeeze(-1)]\n",
    "    else:\n",
    "        tensordict = tensordict.view(-1)\n",
    "        current_frames = tensordict.numel()\n",
    "    collected_frames += current_frames\n",
    "    replay_buffer.extend(tensordict.cpu())\n",
    "\n",
    "    # optimization steps\n",
    "    if collected_frames >= init_random_frames:\n",
    "        for j in range(optim_steps_per_batch):\n",
    "            # sample from replay buffer\n",
    "            sampled_tensordict = replay_buffer.sample(batch_size)\n",
    "\n",
    "            # compute loss for qnet and backprop\n",
    "            with hold_out_net(actor):\n",
    "                # get next state value\n",
    "                next_tensordict = step_tensordict(sampled_tensordict)\n",
    "                qnet_target(actor(next_tensordict))\n",
    "                next_value = next_tensordict[\"state_action_value\"]\n",
    "                assert not next_value.requires_grad\n",
    "            value_est = (\n",
    "                sampled_tensordict[\"reward\"]\n",
    "                + gamma * (1 - sampled_tensordict[\"done\"].float()) * next_value\n",
    "            )\n",
    "            value = qnet(sampled_tensordict)[\"state_action_value\"]\n",
    "            value_loss = (value - value_est).pow(2).mean()\n",
    "            # we write the td_error in the sampled_tensordict for priority update\n",
    "            # because the indices of the samples is tracked in sampled_tensordict\n",
    "            # and the replay buffer will know which priorities to update.\n",
    "            sampled_tensordict[\"td_error\"] = (value - value_est).pow(2).detach()\n",
    "            value_loss.backward()\n",
    "            \n",
    "            optimizer_qnet.step()\n",
    "            optimizer_qnet.zero_grad()\n",
    "\n",
    "            # compute loss for actor and backprop: the actor must maximise the state-action value, hence the loss is the neg value of this.\n",
    "            sampled_tensordict_actor = sampled_tensordict.select(*actor.in_keys)\n",
    "            with hold_out_net(qnet):\n",
    "                qnet(actor(sampled_tensordict_actor))\n",
    "            actor_loss = -sampled_tensordict_actor[\"state_action_value\"]\n",
    "            actor_loss.mean().backward()\n",
    "\n",
    "            optimizer_actor.step()\n",
    "            optimizer_actor.zero_grad()\n",
    "\n",
    "            # update qnet_target params\n",
    "            for (p_in, p_dest) in zip(qnet.parameters(), qnet_target.parameters()):\n",
    "                p_dest.data.copy_(tau * p_in.data + (1 - tau) * p_dest.data)\n",
    "            for (b_in, b_dest) in zip(qnet.buffers(), qnet_target.buffers()):\n",
    "                b_dest.data.copy_(tau * b_in.data + (1 - tau) * b_dest.data)\n",
    "\n",
    "            # update priority\n",
    "            if prb:\n",
    "                replay_buffer.update_priority(sampled_tensordict)\n",
    "\n",
    "    rewards.append((i, tensordict['reward'].mean().item() / norm_factor_training / frame_skip))\n",
    "    td_record = recorder(None)\n",
    "    if td_record is not None:\n",
    "        rewards_eval.append((i, td_record[\"r_evaluation\"]))\n",
    "    if len(rewards_eval):\n",
    "        pbar.set_description(f\"reward: {rewards[-1][1]: 4.4f} (r0 = {r0: 4.4f}), reward eval: reward: {rewards_eval[-1][1]: 4.4f}\")\n",
    "\n",
    "    # update the exploration strategy\n",
    "    actor_model_explore.step(current_frames)\n",
    "    if collected_frames >= init_random_frames:\n",
    "        scheduler1.step()\n",
    "        scheduler2.step()\n",
    "\n",
    "collector.shutdown()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Â Experiment results\n",
    "We make a simple plot of the average rewards during training. We can observe that our policy learned quite well to solve the task."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(*zip(*rewards), label=\"training\")\n",
    "plt.plot(*zip(*rewards_eval), label=\"eval\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"iter\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sampling trajectories and using TD(lambda)\n",
    "TD(lambda) is known to be less biased than the regular TD-error we used in the previous example.\n",
    "To use it, however, we need to sample trajectories and not single transitions.\n",
    "\n",
    "We modify the previous example to make this possible.\n",
    "\n",
    "The first modification consists in building a replay buffer that stores trajectories (and not transitions).\n",
    "We'll collect trajectories of (at most) 250 steps (note that the total trajectory length is actually 1000, but we collect batches of 500 transitions obtained over 2 environments running in parallel, hence only 250 steps per trajectory are collected at any given time). Hence, we'll devide our replay buffer size by 250:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "buffer_size = 100000 // frame_skip // 250\n",
    "print(\"the new buffer size is\", buffer_size)\n",
    "batch_size_traj = max(4, batch_size // 250)\n",
    "print(\"the new batch size for trajectories is\", batch_size_traj)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_steps_forward = 0  # disable multi-step for simplicity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code is identical to the initialization we made earlier:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# get stats for normalization\n",
    "stats = get_env_stats()\n",
    "\n",
    "# Actor and qnet instantiation\n",
    "actor, qnet = make_ddpg_actor(\n",
    "    stats=stats,\n",
    "    device=device,\n",
    ")\n",
    "if device == torch.device(\"cpu\"):\n",
    "    actor.share_memory()\n",
    "# Target network\n",
    "qnet_target = deepcopy(qnet).requires_grad_(False)\n",
    "\n",
    "# Exploration wrappers:\n",
    "actor_model_explore = OrnsteinUhlenbeckProcessWrapper(\n",
    "    actor,\n",
    "    annealing_num_steps=annealing_frames,\n",
    ").to(device)\n",
    "if device == torch.device(\"cpu\"):\n",
    "    actor_model_explore.share_memory()\n",
    "\n",
    "# Environment setting:\n",
    "create_env_fn = parallel_env_constructor(\n",
    "    stats=stats,\n",
    ")\n",
    "# Batch collector:\n",
    "collector = MultiaSyncDataCollector(\n",
    "    create_env_fn=[create_env_fn, create_env_fn],\n",
    "    policy = actor_model_explore,\n",
    "    total_frames = total_frames,\n",
    "    max_frames_per_traj = 1000,\n",
    "    frames_per_batch = frames_per_batch,\n",
    "    init_random_frames = init_random_frames,\n",
    "    reset_at_each_iter = False,\n",
    "    postproc = MultiStep(n_steps_max=n_steps_forward, gamma=gamma) if n_steps_forward > 0 else None,\n",
    "    split_trajs = True,\n",
    "    devices = [device, device],  # device for execution\n",
    "    passing_devices = [device, device],  # device where data will be stored and passed\n",
    "    seed = None,\n",
    "    pin_memory = False,\n",
    "    update_at_each_batch = False,\n",
    "    exploration_mode = \"random\",\n",
    ")\n",
    "collector.set_seed(seed)\n",
    "\n",
    "# Replay buffer:\n",
    "replay_buffer = make_replay_buffer(0)\n",
    "\n",
    "# trajectory recorder\n",
    "recorder = make_recorder(actor_model_explore, stats)\n",
    "\n",
    "\n",
    "# Optimizers\n",
    "optimizer_actor = optim.Adam(\n",
    "    actor.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "optimizer_qnet = optim.Adam(\n",
    "    qnet.parameters(), lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "total_collection_steps = total_frames // frames_per_batch\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_actor, T_max=total_collection_steps)\n",
    "scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_qnet, T_max=total_collection_steps)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training loop needs to be modified.\n",
    "First, whereas before extending the replay buffer we used to flatten the collected data, this won't be the case anymore.\n",
    "To understand why, let's check the output shape of the data collector:\n",
    "\n",
    "```python\n",
    "for data in collector:\n",
    "    print(data.shape)\n",
    "    break\n",
    "```\n",
    "```\n",
    "torch.Size([2, 250])\n",
    "```\n",
    "\n",
    "We see that our data has shape `[2, 250]` as expected: 2 envs, each returning 250 frames.\n",
    "\n",
    "Let's import the td_lambda function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchrl.objectives.returns.functional import vec_td_lambda_advantage_estimate\n",
    "lmbda = 0.95"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training loop is roughly the same as before, with the exception that we don't flatten the collected data.\n",
    "Also, the sampling from the replay buffer is slightly different:\n",
    "We will collect at minimum four trajectories, compute the returns (TD(lambda)), then sample from these the values we'll be using to compute gradients. This ensures that do not have batches that are 'too big' but still compute an accurate return.\n",
    "\n",
    "Note that when storing tensordicts the replay buffer, we must change their batch size: indeed, we will be storing an \"index\" (and possibly an priority) key in the stored tensordicts that will not have a time dimension. Because of this, when sampling from the replay buffer, we remove the keys that do not have a time dimension, change the batch size to `torch.Size([batch, time])`, compute our loss and then revert the batch size to `torch.Size([batch])`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rewards = []\n",
    "rewards_eval = []\n",
    "\n",
    "# Main loop\n",
    "norm_factor_training = sum(gamma**i for i in range(n_steps_forward)) if n_steps_forward else 1\n",
    "\n",
    "collected_frames = 0\n",
    "pbar = tqdm.tqdm(total=total_frames)\n",
    "r0 = None\n",
    "for i, tensordict in enumerate(collector):\n",
    "\n",
    "    # update weights of the inference policy\n",
    "    collector.update_policy_weights_()\n",
    "    \n",
    "    if r0 is None:\n",
    "        r0 = tensordict[\"reward\"].mean().item()\n",
    "    pbar.update(tensordict.numel())\n",
    "    \n",
    "    # extend the replay buffer with the new data\n",
    "    tensordict.batch_size = tensordict.batch_size[:1]  #Â this is necessary for prioritized replay buffers: we will assign one priority value to each element, hence the batch_size must comply with the number of priority values\n",
    "    current_frames = tensordict.numel()\n",
    "    collected_frames += tensordict[\"mask\"].sum()\n",
    "    replay_buffer.extend(tensordict.cpu())\n",
    "\n",
    "    # optimization steps\n",
    "    if collected_frames >= init_random_frames:\n",
    "        for j in range(optim_steps_per_batch):\n",
    "            # sample from replay buffer\n",
    "            sampled_tensordict = replay_buffer.sample(batch_size_traj)\n",
    "            # reset the batch size temporarily, and exclude index whose shape is incompatible with the new size\n",
    "            index = sampled_tensordict.get(\"index\")\n",
    "            sampled_tensordict.exclude(\"index\", inplace=True)\n",
    "            sampled_tensordict.batch_size = [batch_size_traj, 250]\n",
    "\n",
    "            # compute loss for qnet and backprop\n",
    "            with hold_out_net(actor):\n",
    "                # get next state value\n",
    "                next_tensordict = step_tensordict(sampled_tensordict)\n",
    "                qnet_target(actor(next_tensordict.view(-1))).view(sampled_tensordict.shape)\n",
    "                next_value = next_tensordict[\"state_action_value\"]\n",
    "                assert not next_value.requires_grad\n",
    "                \n",
    "            # This is the crucial bit: we'll compute the TD(lambda) instead of a simple single step estimate\n",
    "            done = sampled_tensordict[\"done\"]\n",
    "            reward = sampled_tensordict[\"reward\"]\n",
    "            value = qnet(sampled_tensordict.view(-1)).view(sampled_tensordict.shape)[\"state_action_value\"]\n",
    "            advantage = vec_td_lambda_advantage_estimate(gamma, lmbda, value, next_value, reward, done)\n",
    "            # we sample from the values we have computed\n",
    "            rand_idx = torch.randint(0, advantage.numel(), (batch_size,))\n",
    "            value_loss = advantage.view(-1)[rand_idx].pow(2).mean()\n",
    "            \n",
    "            # we write the td_error in the sampled_tensordict for priority update\n",
    "            # because the indices of the samples is tracked in sampled_tensordict\n",
    "            # and the replay buffer will know which priorities to update.\n",
    "            value_loss.backward()\n",
    "            \n",
    "            optimizer_qnet.step()\n",
    "            optimizer_qnet.zero_grad()\n",
    "\n",
    "            # compute loss for actor and backprop: the actor must maximise the state-action value, hence the loss is the neg value of this.\n",
    "            sampled_tensordict_actor = sampled_tensordict.select(*actor.in_keys)\n",
    "            with hold_out_net(qnet):\n",
    "                qnet(actor(sampled_tensordict_actor.view(-1))).view(sampled_tensordict.shape)\n",
    "            actor_loss = -sampled_tensordict_actor[\"state_action_value\"]\n",
    "            actor_loss.view(-1)[rand_idx].mean().backward()\n",
    "\n",
    "            optimizer_actor.step()\n",
    "            optimizer_actor.zero_grad()\n",
    "\n",
    "            # update qnet_target params\n",
    "            for (p_in, p_dest) in zip(qnet.parameters(), qnet_target.parameters()):\n",
    "                p_dest.data.copy_(tau * p_in.data + (1 - tau) * p_dest.data)\n",
    "            for (b_in, b_dest) in zip(qnet.buffers(), qnet_target.buffers()):\n",
    "                b_dest.data.copy_(tau * b_in.data + (1 - tau) * b_dest.data)\n",
    "\n",
    "            # update priority\n",
    "            sampled_tensordict.batch_size = [batch_size_traj]\n",
    "            sampled_tensordict[\"td_error\"] = advantage.detach().pow(2).mean(1)\n",
    "            sampled_tensordict[\"index\"] = index\n",
    "            if prb:\n",
    "                replay_buffer.update_priority(sampled_tensordict)\n",
    "\n",
    "    rewards.append((i, tensordict['reward'].mean().item() / norm_factor_training / frame_skip))\n",
    "    td_record = recorder(None)\n",
    "    if td_record is not None:\n",
    "        rewards_eval.append((i, td_record[\"r_evaluation\"]))\n",
    "    if len(rewards_eval):\n",
    "        pbar.set_description(f\"reward: {rewards[-1][1]: 4.4f} (r0 = {r0: 4.4f}), reward eval: reward: {rewards_eval[-1][1]: 4.4f}\")\n",
    "\n",
    "    # update the exploration strategy\n",
    "    actor_model_explore.step(current_frames)\n",
    "    if collected_frames >= init_random_frames:\n",
    "        scheduler1.step()\n",
    "        scheduler2.step()\n",
    "\n",
    "collector.shutdown()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can observe that using TD(lambda) made our results considerably more stable for a similar training speed:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(*zip(*rewards), label=\"training\")\n",
    "plt.plot(*zip(*rewards_eval), label=\"eval\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"iter\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.tight_layout()\n",
    "plt.title(\"TD-labmda DDPG results\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
